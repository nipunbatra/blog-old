{
  
    
        "post0": {
            "title": "My iPad Setup",
            "content": "Caveats and Story . My laptop is broken. I am away from office. I have an iPad Pro 2020. I got my office desktop’s magic keyboard and trackpad. In this post I am discussing if iPad can help given that I do not have (physical) access to my main computers. Would I recommend this over a main computer - No! But, can you do some things on the iPad reasonably well enough given keyboard and trackpad - Yes! . The Hardware . The magic keyboard and the magic trackpad greatly increase the iPad experience and make it programmer friendly. . . Setting up an iPad for programming . Setting up the terminal app (a-Shell) . Configuration . First, after installing a-Shell, I like to set the font size, terminal background and foreground color. Here is how the a-shell app looks like . . config -b black -f white -s 20 . Text editing and bookmarks . Sometimes I like using vim for editing documents and interfacing with WorkingCopy. a-Shell provides vim! . I like to setup a bookmark to the WorkingCopy folder so that I can direcly edit files in that location. . I do so by: writing pickFolder in a-Shell and setting it to WorkingCopy folder. Now I can set a bookmark to this location. I do so by: bookmark git in the current location (set by pickFolder) . . Git . Interestingly, the latest testflight version of a-shell also provides a “Git-like” interface called libgit2. Configuring it requires specific steps that I’m writing below. Some of these steps are borrowed from this nice tutorial and some are specific to a-shell that I was able to get working courtesy a Twitter discussion with the creator of a-shell. . . Now, the steps. . First, we need to create a new ssh key. . We do so by . ssh-keygen -t rsa -b 4096 -C &quot;email@domain.com&quot; . While configuring I did not setup the passphrase. . The private and public keys are stored in .ssh with the name id_rsa . $ ls -lah .ssh|grep &quot;id&quot; -rw- 1 mobile mobile 3.3K Jun 14 15:43 id_rsa -rw- 1 mobile mobile 747B Jun 14 15:43 id_rsa.pub . Next, I copied the public key in a newly generated ssh key in Github and gave it a name. . Next, I modified .gitconfig as follows . $ cat .gitconfig [user] email = MY EMAIL ID name = MY NAME identityFile = id_rsa . Now, I was almost done! . I was able to push and pull from some old Github repositories but the same did not work with the newer repositories. Again, after a discussion with the creator of a-shell, I figured, this was due to the fact that Github changed the name of the default branch as “main” instead of the earlier “master” whereas libgit2 implementation was expecting “master”. . As a quickfix I renamed the branch on my Github repo as “master” and for now set the default branch to be named “master”. . Finally, I am able to pull and push to the repositories. The next image infact is showing commits and pushes made to the repository generating the blog post you are reading. . . Some other amazing tools . I like the “view” utility in a-shell a lot. It can quickly help you preview various filetypes. . . Also, as a quick tip, one can use Command + W to quickly exit the preview and use the back and forward keys to cycle through the files. This is very useful! . I also like the fact that the convert tool now comes in inbuilt. It can convert between a variety of formats easily. . pbcopy and pbpaste are very convenient utilities to copy to and from the clipboard. Here is how I copied the content of factorial.py into the clipboard. . pbcopy &lt; factorial.py . Shortcuts . a-Shell interfaces nicely with Shortcuts. The following gif shows an interface where I take an input from Shortcuts app -&gt; Pass it to a Python script and execute it inside a-shell -&gt; Store the results in a text file -&gt; View the content of the text file in Shortcuts. . . The link to this shortcut is here . The following is the simple Python script I used called factorial.py . import math import sys num = int(sys.argv[1]) print(f&quot;The factorial of {num} is {math.factorial(num)}&quot;) . The following is an image of the shortcut. . . Based on the suggestion here, I used pbcopy to copy the content to the clipboard and use it directly. It reduces the number of lines! . . Using the WorkingCopy App . WorkingCopy is a very nicely made Git app on the iPad. It is one of the best made apps I have used! . I’ll let the pictures do the talking. . . . . Editors . I use one of the following for editing: . Koder App | WorkingCopy App | vim (in a-Shell) | Textastic | . Setting Linux/Mac machine and iPad for Python programming (via Jupyter notebooks) . On the Linux/Mac (remote server) machines I set up Jupyter notebooks as follows: . First, I create the configuration file via: jupyter notebook --generate-config . Next, In ~/.jupyter/jupyter_notebook_config.py change . c.NotebookApp.ip = &#39;0.0.0.0&#39; c.NotebookApp.allow_origin = &#39;*&#39; c.NotebookApp.port = SET YOUR PORT c.NotebookApp.open_browser = False . Set password using: . jupyter notebook password . Run notebook as: . nohup jupyter notebook&amp; . On the client end (iPad in this case), I use one of the following two ways to access remote Jupyter instances: . Using the Juno Connect app Juno connectly nicely saves all the configuration and can do the necessary port forwarding . | Using Safari and opening the remote Jupyter URL Just open the remote URL with the set port number. I use my workplace recommended VPN client to get into my workplace network and access non-public IP based servers. . | . VNC connection . I tried a few VNC software and found AnyDesk to work fairly well. I did a bunch of small settings tweak. . I setup a password on my remote machine for uninterruped access | I put AnyDesk in the login items on my remote machine so that it starts when the system boots | . Email client . I like the Spark app. . The interface is pretty clean. | It has a bunch of keyboard shortcuts making it nice to use with an external keyboard. | It is trivial to interface with various cloud providers. | It is trivial to save an email as a PDF. | It is trivial to create a “link” to an email for sharing. | . PDF management . The iPad Files app is a fairly well made app. Perhaps not as functional as the Mac Preview app, but, does some things well. . For instance, merging multiple PDFs is easier (for me) than doing the same on Mac. One just needs to select multiple files and then “right” click to “Create PDF” . . I use the following ffmpeg command (inside a-Shell) to create this GIF courtesy this excellent tutorial from GIPHY folks . ffmpeg -i Video.MP4 -filter_complex &quot;[0:v] fps=12,scale=720:-1,split [a][b];[a] palettegen [p];[b][p] palette use&quot; CombinePDF.gif . Another alternative is to use the following shortcut to convert the screen recorded video into a GIF . Some Safari shortcuts . Command + W for closing a tab | Command + L for going to the address bar (sometimes on tabs I am not automatically on the address bar) | . Conclusions (and rants) . For the long run, ofcourse, I would not recommend this kind of a setup. There is some learning and fun in getting a “resticted” device to function well, but as a main machine, probably no, at least not for the next few months. A comparably priced laptop (say M1 Macbook Air) would offer a lot more. . There are many things I’m missing: . Projection to a projector and screen recording do not work at the same time. | No proper dev environment like VSCode | A lot of screen size is “wasted” due to the OS | A lot of the apps are not as good as I’d like. For instance, perhaps a minor thing, but the Youtube app doesn’t have a keyboard shortcut for full screen. One can ofcourse do the same on Safari. iMovie in Mac has a remove background noise feature that works very well. The iPad version does not have the same. | Safari though much improved over the years is not as functional as Safari on the Mac | While quicklook/Files is good, the functionality of something like Preview is hard to beat | The files app can not show hidden files and folders | Powerpoint does not have features like audio recording (though can play) | Keynote, Pages, Numbers require additional clicks than their Mac counterparts | When I connect to an external display, I get thick black bars and moreover, if the monitor does not give me an HDMI out, I can not hear audio as the audio is also routed. As good as the iPad’s display is, it is not huge and strains my eyes after a while. | Copy pasting using the keyboard is a hit and miss. For instance, it fails several times for me when copying from a pdf. Right clicking and copy seems to always work, but, it is more steps than Command C, Command V. | . Some of the above may be resolved over the next updates and the major iPadOS15 updates. Though, I do not expect most of them to be resolved anytime soon. .",
            "url": "https://nipunbatra.github.io/blog/setup/2021/06/14/setup-ipad.html",
            "relUrl": "/setup/2021/06/14/setup-ipad.html",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "My Mac Setup",
            "content": "Setting up a new Mac . Here is a screenshot. . . I will now discuss how I setup a new Mac. I use homebrew. It makes it very easy to maintain all the packages up to date. . Install homebrew and some pertinent packages . /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot; . Install fish shell . brew install fish . Make fish default . sudo bash -c &#39;echo /usr/local/bin/fish &gt;&gt; /etc/shells&#39; chsh -s /usr/local/bin/fish . Install git . brew install git . Install powerline fonts . git clone https://github.com/powerline/fonts cd fonts ./install.sh . Use powerline fonts in shell . Go to Terminal -&gt; Preferences Change font to powerline I am using Roboto Mono Light Powerline 18pt . git clone git://github.com/stephenway/monokai.terminal.git open monokai.terminal (set this theme as default) . Install fish theme . omf install bobthefish . Install Zoom . brew install --cask zoom . Install Firefox . brew install --cask firefox . Install VSCode . brew install --cask visual-studio-code . Install OBSStudio . brew install --cask obs . VLC . brew install --cask vlc . wget . brew install wget . anaconda . brew install --cask anaconda . anaconda path setup in fish . ./usr/local/anaconda3/condabin/conda init fish . Install MacTex (slow!) . brew install --cask mactex . Installing TexStudio brew install --cask texstudio . FFMPeg brew install ffmpeg . Imagemagick brew install imagemagick . Ghostscript brew install ghostscript . Install pandoc . brew install pandoc . Viewing installed packages . brew leaves &gt; brew.txt . The content of brew.txt is: . boost cmake ffmpeg fish git graphviz ilmbase imagemagick pandoc r rtmpdump swig vim wget . brew list --cask &gt; casks.txt . The content of casks.txt is: . anaconda anydesk arduino audacity firefox google-chrome inkscape keycastr mactex notion obs pdf-expert pycharm rstudio simplenote texstudio visual-studio-code vlc zoom .",
            "url": "https://nipunbatra.github.io/blog/setup/2021/06/12/setup-mac.html",
            "relUrl": "/setup/2021/06/12/setup-mac.html",
            "date": " • Jun 12, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "A programming introduction to GANs",
            "content": "Introduction . This is a post about Generative Adversarial Networks (GANs). This post is very heavily influenced and borrows code from: . Video from Luis Serrano | Heavily borrowed code from this article on machine learning mastery | . These folks deserve all the credit! I am writing this post mostly for my learning. . I&#39;d highly recommend reading the above two mentioned resources. . Goal . The goal of GANs is to generate realistic data, i.e. data with similar statistics as the training data. . See below a &quot;generated&quot; face on https://thispersondoesnotexist.com . Refresh this page to get a new face each time! . These are people that do not exist but their faces have been generated using GANs. . from IPython.display import HTML, IFrame IFrame(&quot;https://thispersondoesnotexist.com&quot;, 400, 400) . Overall Block Diagram . Conceptually, GANs are simple.They have two main components: . A discriminator: that tries to accurately tell generated and real data (from training data) apart | A generator: that generates data given some random numbers | . The goal of GANs is to use the generator to create realistic data such that the discriminator thinks it is real (coming from the training dataset) . . The two components discriminator and generator are &quot;fighting&quot; where: . the goal of the discriminator is to tell apart fake (generated) data from true data (from training set) even when the generator is fairly good | the goal of the generator is to generate realistics data such that the discriminator thinks it is real data | . . Creating &quot;true&quot; distribution . Let us now create some data from the true/known distribution. We will be essentially creating a 2x2 matrix (image) as explained in Luis Serrano&#39;s tutorial. The (0, 0) and (1, 1) position will be a high number between 0.8 and 1 whereas the other two positions (0, 1) and (1, 0) have values between 0 and 0.1 . import numpy as np import matplotlib.pyplot as plt import mediapy as media %matplotlib inline np.random.seed(40) import warnings warnings.filterwarnings(&#39;ignore&#39;) import logging import os os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;3&#39; # FATAL logging.getLogger(&#39;tensorflow&#39;).setLevel(logging.FATAL) import tensorflow as tf tf.get_logger().setLevel(&#39;ERROR&#39;) tf.random.set_seed(42) . SIZE = 5000 faces = np.vstack((np.random.uniform(0.8, 1, SIZE), np.random.uniform(0., 0.1, SIZE), np.random.uniform(0., 0.1, SIZE), np.random.uniform(0.8, 1, SIZE))).T faces.shape . (5000, 4) . def plot_face(f): f_reshape = f.reshape(2, 2) plt.imshow(f_reshape, cmap=&quot;Greys&quot;) . def plot_faces(faces, subset=1): images = { f&#39;Image={im}&#39;: faces[im].reshape(2, 2) for im in range(len(faces))[::subset] } media.show_images(images, border=True, columns=8, height=80, cmap=&#39;Greys&#39;) plot_faces(faces, subset=700) . Image=0 | Image=700 | Image=1400 | Image=2100 | Image=2800 | Image=3500 | Image=4200 | Image=4900 | . The above shows some samples drawn from the true distibution. Let us also now create some random/noisy samples. These samples do not have any relationship between the 4 positions. . # Examples of noisy images noise = np.random.randn(40, 4) noise = np.abs(noise) noise = noise/noise.max() . plot_faces(noise) . Image=0 | Image=1 | Image=2 | Image=3 | Image=4 | Image=5 | Image=6 | Image=7 | . Image=8 | Image=9 | Image=10 | Image=11 | Image=12 | Image=13 | Image=14 | Image=15 | . Image=16 | Image=17 | Image=18 | Image=19 | Image=20 | Image=21 | Image=22 | Image=23 | . Image=24 | Image=25 | Image=26 | Image=27 | Image=28 | Image=29 | Image=30 | Image=31 | . Image=32 | Image=33 | Image=34 | Image=35 | Image=36 | Image=37 | Image=38 | Image=39 | . Creating the discriminator . Our discriminator is simple. . It accepts as input a 4 dimensional input (the 2x2 image) | It outputs a single number with sigmoid activation denoting the probability of: image being fake or generated by generator or belonging to class 0 | image being real or sampled from training dataset or belonging to class 1 | . | We use the binary cross entropy loss | . To make the above crystal clear, I&#39;ll use the following gist to draw this NN . from draw_nn import draw_neural_net . fig = plt.figure(figsize=(4, 4)) ax = fig.gca() ax.axis(&#39;off&#39;) draw_neural_net(ax, .1, 0.9, .1, .6, [4, 1]) plt.tight_layout() plt.title(&quot;Discriminator NN&quot;); . from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential, load_model from tensorflow.keras.optimizers import Adam discriminator = Sequential([ Dense(1,activation=&#39;sigmoid&#39;, input_shape=(4, )), ]) discriminator._name = &quot;Discriminator&quot; discriminator.compile( optimizer=Adam(0.001), loss=&#39;binary_crossentropy&#39; ) . discriminator.summary() . Model: &#34;Discriminator&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 1) 5 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ . As expected, the discriminator has 5 parameters (4 weights coming from the 4 inputs to the output node and 1 bias term added). Now, let us create the generator. . Creating the generator . Let us now create the generator model. We create a very simple one . It accepts as input a single random number | It creates a vector of size 4 | . The illustration below shows this network. It should be noted that the single random input is an arbitrary choice. We could use any number really! . fig = plt.figure(figsize=(4, 4)) ax = fig.gca() ax.axis(&#39;off&#39;) draw_neural_net(ax, .1, 0.9, .1, .6, [1, 4]) plt.tight_layout() plt.title(&quot;Generator NN&quot;); . from keras.layers import ReLU generator = Sequential([ Dense(4, input_shape=(1, )), ReLU(max_value=1.0) ]) generator._name = &quot;Generator&quot; generator.compile( optimizer=Adam(0.001), loss=&#39;binary_crossentropy&#39; ) . generator.summary() . Model: &#34;Generator&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 4) 8 _________________________________________________________________ module_wrapper (ModuleWrappe (None, 4) 0 ================================================================= Total params: 8 Trainable params: 8 Non-trainable params: 0 _________________________________________________________________ . We can verify that the network has 8 parameters (4 weights and one bias value per output node) . Generating samples from Generator . We can now use our generator to generate some samples and plot them. . def gen_fake(n_samples): x_input = np.random.randn(n_samples, 1) X = generator.predict(x_input) y = np.zeros((n_samples, 1)) return X, y . As expected, the samples look random, without any specific pattern and do not resemble the training data as our generator is untrained. Further, it is important to reiterate that the class associated with the fake samples generated from the generator is 0. Thus, we have the line np.zeros((n_samples, 1)) in the code above. . plot_faces(gen_fake(20)[0]) . Image=0 | Image=1 | Image=2 | Image=3 | Image=4 | Image=5 | Image=6 | Image=7 | . Image=8 | Image=9 | Image=10 | Image=11 | Image=12 | Image=13 | Image=14 | Image=15 | . Image=16 | Image=17 | Image=18 | Image=19 | . Sampling from the Real (Train) Dataset . def gen_real(n_samples): ix = np.random.randint(0, faces.shape[0], n_samples) X = faces[ix] y = np.ones((n_samples, 1)) return X, y . plot_faces(gen_real(20)[0]) . Image=0 | Image=1 | Image=2 | Image=3 | Image=4 | Image=5 | Image=6 | Image=7 | . Image=8 | Image=9 | Image=10 | Image=11 | Image=12 | Image=13 | Image=14 | Image=15 | . Image=16 | Image=17 | Image=18 | Image=19 | . We can clearly see the pattern in the images coming from the training dataset. . Training the GAN . The block diagram below shows the main idea behind training GANs. The procedure is similar to alternative least squares. . . def define_gan(g_model, d_model): d_model.trainable = False model = Sequential() model.add(g_model) model.add(d_model) opt = Adam(lr=0.001) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=opt) return model . gan_model = define_gan(generator, discriminator) . It is important to note that we will train two networks: . Discriminator on Fake data (class 0) | Real data (class 1) | . | Combined model consisting og Generator + Discriminator (where the Discriminator is is fixed) on Fake data (class 0) posing as real data (class 1) to the model | . | . Thus, we do not train on the generator separately. . samples_saved = {} losses = {} N_ITER = 1000 STEP = N_ITER//10 for i in range(N_ITER): # Generate some fake data X_fake, y_fake = gen_fake(2) X_real, y_real = gen_real(2) X, y = np.vstack((X_fake, X_real)), np.vstack((y_fake, y_real)) # Discriminator d_loss = discriminator.train_on_batch(X, y) # Generator n_samples = 4 g_loss= gan_model.train_on_batch(np.random.randn(n_samples, 1), np.ones(n_samples)) losses[i] = {&#39;Gen. loss&#39;:g_loss, &#39;Disc. loss&#39;:d_loss} # Save 5 samples samples_saved[i]= gen_fake(5)[0] if i%STEP==0: # Save model generator.save(f&quot;models/gen-{i}&quot;) print(&quot;&quot;) print(&quot;Iteration: {}&quot;.format(i)) print(&quot;Discriminator loss: {:0.2f}&quot;.format(d_loss)) print(&quot;Generator loss: {:0.2f}&quot;.format(g_loss)) . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 0 Discriminator loss: 0.61 Generator loss: 0.72 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 100 Discriminator loss: 0.61 Generator loss: 0.75 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 200 Discriminator loss: 0.59 Generator loss: 0.76 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 300 Discriminator loss: 0.57 Generator loss: 0.72 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 400 Discriminator loss: 0.60 Generator loss: 0.77 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 500 Discriminator loss: 0.61 Generator loss: 0.68 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 600 Discriminator loss: 0.64 Generator loss: 0.66 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 700 Discriminator loss: 0.60 Generator loss: 0.71 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 800 Discriminator loss: 0.65 Generator loss: 0.66 . WARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading. . Iteration: 900 Discriminator loss: 0.70 Generator loss: 0.63 . Convergence . import pandas as pd losses_df = pd.DataFrame(losses) losses_df.T.plot(); plt.xlabel(&quot;Iteration Number&quot;); . You might epxect that over time the generator loss reduces as it becomes better and correspodingly the discriminator has a harder time! . Generating some &quot;fake&quot; images from the trained generator . plot_faces(gen_fake(20)[0]) . Image=0 | Image=1 | Image=2 | Image=3 | Image=4 | Image=5 | Image=6 | Image=7 | . Image=8 | Image=9 | Image=10 | Image=11 | Image=12 | Image=13 | Image=14 | Image=15 | . Image=16 | Image=17 | Image=18 | Image=19 | . You could not tell, right! The generator has been trained well! . Visualising evolution of generator . Let us now visualise the evolution of the generator. To do so, we use the already saved generator models at different iterations and feed them the same &quot;random&quot; input. . o = {} for i in range(0, N_ITER, STEP): for inp in [0., 0.2, 0.4, 0.6, 1.]: o[f&#39;It:{i}-Inp:{inp}&#39;] = load_model(f&quot;models/gen-{i}&quot;).predict(np.array([inp])).reshape(2, 2) . media.show_images(o, border=True, columns=5, height=80, cmap=&#39;Greys&#39;) . It:0-Inp:0.0 | It:0-Inp:0.2 | It:0-Inp:0.4 | It:0-Inp:0.6 | It:0-Inp:1.0 | . It:100-Inp:0.0 | It:100-Inp:0.2 | It:100-Inp:0.4 | It:100-Inp:0.6 | It:100-Inp:1.0 | . It:200-Inp:0.0 | It:200-Inp:0.2 | It:200-Inp:0.4 | It:200-Inp:0.6 | It:200-Inp:1.0 | . It:300-Inp:0.0 | It:300-Inp:0.2 | It:300-Inp:0.4 | It:300-Inp:0.6 | It:300-Inp:1.0 | . It:400-Inp:0.0 | It:400-Inp:0.2 | It:400-Inp:0.4 | It:400-Inp:0.6 | It:400-Inp:1.0 | . It:500-Inp:0.0 | It:500-Inp:0.2 | It:500-Inp:0.4 | It:500-Inp:0.6 | It:500-Inp:1.0 | . It:600-Inp:0.0 | It:600-Inp:0.2 | It:600-Inp:0.4 | It:600-Inp:0.6 | It:600-Inp:1.0 | . It:700-Inp:0.0 | It:700-Inp:0.2 | It:700-Inp:0.4 | It:700-Inp:0.6 | It:700-Inp:1.0 | . It:800-Inp:0.0 | It:800-Inp:0.2 | It:800-Inp:0.4 | It:800-Inp:0.6 | It:800-Inp:1.0 | . It:900-Inp:0.0 | It:900-Inp:0.2 | It:900-Inp:0.4 | It:900-Inp:0.6 | It:900-Inp:1.0 | . We can see above the improvement of the generation over the different iterations and different inputs! That is it for this article. Happing GANning. .",
            "url": "https://nipunbatra.github.io/blog/ml/2021/05/31/GAN.html",
            "relUrl": "/ml/2021/05/31/GAN.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Non stationary kernels in GP",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . from sklearn.gaussian_process.kernels import ConstantKernel, RBF . k = RBF() x = np.linspace(-1, 1, 20).reshape(-1, 1) . sns.heatmap(k(x),cmap=&#39;Greys&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe80c0efee0&gt; . from sklearn.gaussian_process import GaussianProcessRegressor . gpr = GaussianProcessRegressor(kernel=dot_kernel+k) . plt.plot(x, gpr.sample_y(x, n_samples=100),color=&#39;k&#39;,alpha=0.1); . from sklearn.gaussian_process.kernels import DotProduct dot_kernel = DotProduct(sigma_0=0.) . x = np.linspace(-5, 5, 25).reshape(-1, 1) . sns.heatmap(dot_kernel(x),cmap=&#39;Blues&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe80e5f41c0&gt; . from sklearn.preprocessing import MinMaxScaler m = MinMaxScaler() sns.heatmap(m.fit_transform(dot_kernel(x)),cmap=&#39;Blues&#39;,xticklabels=x) . /usr/local/anaconda3/lib/python3.8/site-packages/matplotlib/text.py:1165: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison if s != self._text: . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe80e3e0df0&gt; . sns.heatmap(dot_kernel(x),cmap=&#39;Greys&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe80d92b5e0&gt; . sns.heatmap((dot_kernel*k)(x),cmap=&#39;Greys&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe80d3652e0&gt; . y = 5*x + 4 noise = (np.abs(x.flatten())*np.random.randn(len(x))).reshape(-1,1) y = y + noise . plt.scatter(x, y) plt.plot(x, 5*x + 4, &#39;k&#39;) . [&lt;matplotlib.lines.Line2D at 0x115c28cd0&gt;] . from scipy.stats import multivariate_normal from matplotlib import cm cov = np.array([[ 1 , 0], [0, 1]]) var = multivariate_normal(mean=[0,0], cov=cov) x_grid, y_grid = np.mgrid[-1:1:.01, -1:1:.01] pos = np.dstack((x_grid, y_grid)) z = var.pdf(pos) plt.contourf(x_grid, y_grid, z) plt.gca().set_aspect(&#39;equal&#39;) plt.xlabel(r&quot;$ theta_0$&quot;) plt.ylabel(r&quot;$ theta_1$&quot;) plt.title(r&quot;Prior distribution of $ theta = f( mu, Sigma)$&quot;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x1a18423950&gt; . $$ prod_{i=1}^{n} frac{1}{ sqrt{2 pi sigma^{2}}} e^{- frac{(y_{i}- hat{y}_{i})^{2}}{2 sigma^{2}}} $$ Sample from prior . n_samples = 20 for n in range(n_samples): theta_0_s, theta_1_s = var.rvs() plt.plot(x, theta_1_s*x + theta_0_s, color=&#39;k&#39;,alpha=0.2) plt.scatter(x, y) . &lt;matplotlib.collections.PathCollection at 0x1a18598fd0&gt; . Likelihood of theta . def likelihood(theta_0, theta_1, x, y, sigma): s = 0 x_plus_1 = np.hstack((np.ones_like(x), x)) for i in range(len(x)): y_i_hat = x_plus_1[i, :]@np.array([theta_0, theta_1]) s += (y[i,:]-y_i_hat)**2 return np.exp(-s/(2*sigma*sigma))/np.sqrt(2*np.pi*sigma*sigma) . likelihood(-1, 1, x, y, 4) . array([1.00683395e-22]) . x_grid_2, y_grid_2 = np.mgrid[0:8:.1, 0:8:.1] li = np.zeros_like(x_grid_2) for i in range(x_grid_2.shape[0]): for j in range(x_grid_2.shape[1]): li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j], x, y, 4) . plt.contourf(x_grid_2, y_grid_2, li) plt.gca().set_aspect(&#39;equal&#39;) plt.xlabel(r&quot;$ theta_0$&quot;) plt.ylabel(r&quot;$ theta_1$&quot;) plt.colorbar() plt.scatter(4, 5, s=200, marker=&#39;*&#39;, color=&#39;r&#39;) plt.title(r&quot;Likelihood as a function of ($ theta_0, theta_1$)&quot;) . Text(0.5, 1.0, &#39;Likelihood as a function of ($ theta_0, theta_1$)&#39;) . Likelihood of $ sigma^2$ . x_plus_1 = np.hstack((np.ones_like(x), x)) theta_mle = np.linalg.inv(x_plus_1.T@x_plus_1)@(x_plus_1.T@y) sigma_2_mle = np.linalg.norm(y - x_plus_1@theta_mle)**2 sigma_mle = np.sqrt(sigma_2_mle) sigma_mle . 4.128685902124939 . Posterior . $$ begin{aligned} p( boldsymbol{ theta} | mathcal{X}, mathcal{Y}) &amp;= mathcal{N} left( boldsymbol{ theta} | boldsymbol{m}_{N}, boldsymbol{S}_{N} right) boldsymbol{S}_{N} &amp;= left( boldsymbol{S}_{0}^{-1}+ sigma^{-2} boldsymbol{ Phi}^{ top} boldsymbol{ Phi} right)^{-1} boldsymbol{m}_{N} &amp;= boldsymbol{S}_{N} left( boldsymbol{S}_{0}^{-1} boldsymbol{m}_{0}+ sigma^{-2} boldsymbol{ Phi}^{ top} boldsymbol{y} right) end{aligned} $$ S0 = np.array([[ 1 , 0], [0, 1]]) M0 = np.array([0, 0]) SN = np.linalg.inv(np.linalg.inv(S0) + (sigma_mle**-2)*x_plus_1.T@x_plus_1) MN = SN@(np.linalg.inv(S0)@M0 + (sigma_mle**-2)*(x_plus_1.T@y).squeeze()) . MN, SN . (array([2.97803341, 2.54277597]), array([[2.54243881e-01, 2.97285330e-17], [2.97285330e-17, 4.95625685e-01]])) . from scipy.stats import multivariate_normal from matplotlib import cm cov = np.array([[ 1 , 0], [0, 1]]) var_pos = multivariate_normal(mean=MN, cov=SN) x_grid, y_grid = np.mgrid[0:8:.1, 0:8:.1] pos = np.dstack((x_grid, y_grid)) z = var_pos.pdf(pos) plt.contourf(x_grid, y_grid, z) plt.gca().set_aspect(&#39;equal&#39;) plt.xlabel(r&quot;$ theta_0$&quot;) plt.ylabel(r&quot;$ theta_1$&quot;) plt.title(r&quot;Posterior distribution of $ theta = f( mu, Sigma)$&quot;) plt.scatter(4, 5, s=200, marker=&#39;*&#39;, color=&#39;r&#39;, label=&#39;MLE&#39;) plt.scatter(MN[0], MN[1], s=100, marker=&#39;^&#39;, color=&#39;black&#39;, label=&#39;MAP&#39;) plt.colorbar() plt.legend() plt.savefig(&quot;../images/blr-map.png&quot;) . Sample from posterior . n_samples = 20 for n in range(n_samples): theta_0_s, theta_1_s = var_pos.rvs() plt.plot(x, theta_1_s*x + theta_0_s, color=&#39;k&#39;,alpha=0.2) plt.scatter(x, y) . &lt;matplotlib.collections.PathCollection at 0x1a18e7dd10&gt; . Posterior predictions . $$ begin{aligned} p left(y_{*} | mathcal{X}, mathcal{Y}, boldsymbol{x}_{*} right) &amp;= int p left(y_{*} | boldsymbol{x}_{*}, boldsymbol{ theta} right) p( boldsymbol{ theta} | mathcal{X}, mathcal{Y}) mathrm{d} boldsymbol{ theta} &amp;= int mathcal{N} left(y_{*} | boldsymbol{ phi}^{ top} left( boldsymbol{x}_{*} right) boldsymbol{ theta}, sigma^{2} right) mathcal{N} left( boldsymbol{ theta} | boldsymbol{m}_{N}, boldsymbol{S}_{N} right) mathrm{d} boldsymbol{ theta} &amp;= mathcal{N} left(y_{*} | boldsymbol{ phi}^{ top} left( boldsymbol{x}_{*} right) boldsymbol{m}_{N}, boldsymbol{ phi}^{ top} left( boldsymbol{x}_{*} right) boldsymbol{S}_{N} boldsymbol{ phi} left( boldsymbol{x}_{*} right)+ sigma^{2} right) end{aligned} $$ For a point $x*$ . Predictive mean = $X^Tm_N$ . Predictive variance = $X^TS_NX + sigma^2$ . x_plus_1.T.shape, SN.shape, x_plus_1.shape . ((2, 50), (2, 2), (50, 2)) . pred_var = x_plus_1@SN@x_plus_1.T pred_var.shape . (50, 50) . ## Marginal individual_var = pred_var.diagonal() . y_hat_map = x_plus_1@MN plt.plot(x, y_hat_map, color=&#39;black&#39;) plt.fill_between(x.flatten(), y_hat_map-individual_var, y_hat_map+individual_var, alpha=0.2, color=&#39;black&#39;) plt.scatter(x, y) . &lt;matplotlib.collections.PathCollection at 0x1a1881e450&gt; .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/12/30/gp-non-stationary.html",
            "relUrl": "/ml/2020/12/30/gp-non-stationary.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Understanding Kernels in Gaussian Processes Regression",
            "content": "Disclaimer . This blog post is forked from GPSS 2019 Lab 1. This is produced only for educational purposes. All credit goes to the GPSS organisers. . # Support for maths import numpy as np # Plotting tools from matplotlib import pyplot as plt # we use the following for plotting figures in jupyter %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) # GPy: Gaussian processes library import GPy from IPython.display import display . Covariance functions, aka kernels . We will define a covariance function, from hereon referred to as a kernel, using GPy. The most commonly used kernel in machine learning is the Gaussian-form radial basis function (RBF) kernel. It is also commonly referred to as the exponentiated quadratic or squared exponential kernel &ndash; all are equivalent. . The definition of the (1-dimensional) RBF kernel has a Gaussian-form, defined as: . $$ kappa_ mathrm{rbf}(x,x&#39;) = sigma^2 exp left(- frac{(x-x&#39;)^2}{2 mathscr{l}^2} right) $$It has two parameters, described as the variance, $ sigma^2$ and the lengthscale $ mathscr{l}$. . In GPy, we define our kernels using the input dimension as the first argument, in the simplest case input_dim=1 for 1-dimensional regression. We can also explicitly define the parameters, but for now we will use the default values: . # Create a 1-D RBF kernel with default parameters k = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4) # Preview the kernel&#39;s parameters k . rbf. valueconstraintspriors . &lt;td class=tg-left&gt; variance &lt;/td&gt;&lt;td class=tg-right&gt; 4.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; lengthscale&lt;/td&gt;&lt;td class=tg-right&gt; 0.5&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . fig, ax = plt.subplots() from matplotlib.animation import FuncAnimation from matplotlib import rc ls = [0.0005, 0.05, 0.25, 0.5, 1., 2., 4.] X = np.linspace(0.,1.,500)# 500 points evenly spaced over [0,1] X = X[:,None] mu = np.zeros((500)) def update(iteration): ax.cla() k = GPy.kern.RBF(1) k.lengthscale = ls[iteration] # Calculate the new covariance function at k(x,0) C = k.K(X,X) Z = np.random.multivariate_normal(mu,C,40) for i in range(40): ax.plot(X[:],Z[i,:],color=&#39;k&#39;,alpha=0.2) ax.set_title(&quot;$ kappa_{rbf}(x,x&#39;)$ nLength scale = %s&quot; %k.lengthscale[0]); ax.set_ylim((-4, 4)) num_iterations = len(ls) anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=500) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect In the animation above, as you increase the length scale, the learnt functions keep getting smoother. . fig, ax = plt.subplots() from matplotlib.animation import FuncAnimation from matplotlib import rc var = [0.0005, 0.05, 0.25, 0.5, 1., 2., 4., 9.] X = np.linspace(0.,1.,500)# 500 points evenly spaced over [0,1] X = X[:,None] mu = np.zeros((500)) def update(iteration): ax.cla() k = GPy.kern.RBF(1) k.variance = var[iteration] # Calculate the new covariance function at k(x,0) C = k.K(X,X) Z = np.random.multivariate_normal(mu,C,40) for i in range(40): ax.plot(X[:],Z[i,:],color=&#39;k&#39;,alpha=0.2) ax.set_title(&quot;$ kappa_{rbf}(x,x&#39;)$ nVariance = %s&quot; %k.variance[0]); ax.set_ylim((-4, 4)) num_iterations = len(ls) anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=500) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect In the animation above, as you increase the variance, the scale of values increases. . X1 = np.array([1, 2, 3]).reshape(-1, 1) y1 = np.array([0, 1, 0]).reshape(-1, 1) y2 = np.array([0, -1, 0]).reshape(-1, 1) y3 = np.array([0, 10, 0]).reshape(-1, 1) y4 = np.array([0, 0.3, 0]).reshape(-1, 1) . k = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4) m = GPy.models.GPRegression(X1, y1, k) #m.Gaussian_noise = 0.0 m.optimize() print(k) m.plot(); . rbf. | value | constraints | priors variance | 0.262031485550043 | +ve | lengthscale | 0.24277532672486218 | +ve | . k = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4) m = GPy.models.GPRegression(X1, y2, k) #m.Gaussian_noise = 0.0 m.optimize() print(k) m.plot(); . rbf. | value | constraints | priors variance | 0.262031485550043 | +ve | lengthscale | 0.24277532672486218 | +ve | . In the above two examples, the y values are: 0, 1, 0 and 0, -1, 0. This shows smoothness. Thus, length scale can be big (0.24) . k = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4) m = GPy.models.GPRegression(X1, y3, k) #m.Gaussian_noise = 0.0 m.optimize() print(k) m.plot(); . rbf. | value | constraints | priors variance | 16.918792970578004 | +ve | lengthscale | 0.07805339389352635 | +ve | . In the above example, the y values are: 0, 10, 0. The data set is not smooth. Thus, length scale learnt uis very small (0.24). Noise variance of RBF kernel also increased to accomodate the 10. . k = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4) m = GPy.models.GPRegression(X1, y4, k) #m.Gaussian_noise = 0.0 m.optimize() print(k) m.plot(); . rbf. | value | constraints | priors variance | 5.90821963086592e-06 | +ve | lengthscale | 2.163452641925496 | +ve | . In the above examples, the y values are: 0, 0.3, 0. The data set is the smoothest amongst the four. Thus, length scale learnt is large (2.1). Noise variance of RBF kernel is also small. .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/06/26/gp-understand.html",
            "relUrl": "/ml/2020/06/26/gp-understand.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Sampling from common distributions",
            "content": "import numpy as np import matplotlib.pyplot as plt %matplotlib inline . PRNG . Uniform distribution . I am pasting the code from vega-vis . export default function(seed) { // Random numbers using a Linear Congruential Generator with seed value // Uses glibc values from https://en.wikipedia.org/wiki/Linear_congruential_generator return function() { seed = (1103515245 * seed + 12345) % 2147483647; return seed / 2147483647; }; } . def random_gen(seed, num): out = np.zeros(num) out[0] = (1103515245 * seed + 12345) % 2147483647 for i in range(1, num): out[i] = (1103515245 * out[i-1] + 12345) % 2147483647 return out/2147483647 . plt.hist(random_gen(0, 5000), density=True, alpha=0.5, label=&#39;Our implementation&#39;); . plt.hist(random_gen(0, 5000), density=True, alpha=0.5, label=&#39;Our implementation&#39;); plt.hist(np.random.random(5000), density=True, alpha=0.4, label=&#39;Numpy implementation&#39;); plt.legend() . &lt;matplotlib.legend.Legend at 0x7f51d3cbd290&gt; . Inverse transform sampling . Exponential distribution . Borrowing from Wikipedia. . The probability density function (pdf) of an exponential distribution is $$ f(x ; lambda)= left { begin{array}{ll} lambda e^{- lambda x} &amp; x geq 0 0 &amp; x leq 0 end{array} right. $$ . The exponential distribution is sometimes parametrized in terms of the scale parameter $ beta=1 / lambda:$ $$ f(x ; beta)= left { begin{array}{ll} frac{1}{ beta} e^{-x / beta} &amp; x geq 0 0 &amp; x&lt;0 end{array} right. $$ . The cumulative distribution function is given by $$ F(x ; lambda)= left { begin{array}{ll} 1-e^{- lambda x} &amp; x geq 0 0 &amp; x&lt;0 end{array} right. $$ . from scipy.stats import expon rvs = [expon(scale=s) for s in [1/1., 1/2., 1/3.]] . x = np.arange(0, 10, 0.1) for i, lambda_val in enumerate([1, 2, 3]): plt.plot(x, rvs[i].pdf(x), lw=2, label=r&#39;$ lambda=%s$&#39; %lambda_val) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f51da6d4e10&gt; . For the purposes of this notebook, I will be looking only at the standard exponential or set the scale to 1. . Let us now view the CDF of the standard exponential. . fig, ax = plt.subplots(nrows=2, sharex=True) ax[0].plot(x, expon().pdf(x), lw=2) ax[0].set_title(&quot;PDF&quot;) ax[1].set_title(&quot;CDF&quot;) ax[1].plot(x, expon().cdf(x), lw=2,) . [&lt;matplotlib.lines.Line2D at 0x7f51d8985a90&gt;] . r = expon.rvs(size=1000) . plt.hist(r, normed=True, bins=100) plt.plot(x, expon().pdf(x), lw=2) . /home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: MatplotlibDeprecationWarning: The &#39;normed&#39; kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use &#39;density&#39; instead. &#34;&#34;&#34;Entry point for launching an IPython kernel. . [&lt;matplotlib.lines.Line2D at 0x7f51d86b5ad0&gt;] . Inverse of the CDF of exponential . The cumulative distribution function is given by $$ F(x ; lambda)= left { begin{array}{ll} 1-e^{- lambda x} &amp; x geq 0 0 &amp; x&lt;0 end{array} right. $$ . Let us consider only $x geq 0$. . Let $u = F^{-1}$ be the inverse of the CDF of $F$. . $$ u = 1-e^{- lambda x} 1- u = e^{- lambda x} log(1-u) = - lambda x x = - frac{ log(1-u)}{ lambda} $$ def inverse_transform(lambda_val, num_samples): u = np.random.random(num_samples) x = -np.log(1-u)/lambda_val return x . plt.hist(inverse_transform(1, 5000), bins=100, normed=True,label=&#39;Generated using our function&#39;); plt.plot(x, expon().pdf(x), lw=4, label=&#39;Generated using scipy&#39;) plt.legend() . /home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: MatplotlibDeprecationWarning: The &#39;normed&#39; kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use &#39;density&#39; instead. &#34;&#34;&#34;Entry point for launching an IPython kernel. . &lt;matplotlib.legend.Legend at 0x7f51d5f95410&gt; . Drawing samples from Laplace distribution . A random variable has a Laplace $( mu, b)$ distribution if its probability density function is $$ f(x | mu, b)= frac{1}{2 b} exp left(- frac{|x- mu|}{b} right) $$ . $$F^{-1}(u)= mu-b operatorname{sgn}(u-0.5) ln (1-2|u-0.5|)$$ . def inverse_transform_laplace(b, mu, num_samples): u = np.random.random(num_samples) x = mu-b*np.sign(u-0.5)*np.log(1-2*np.abs(u-0.5)) return x . from scipy.stats import laplace plt.hist(inverse_transform_laplace(1, 0, 5000),bins=100, density=True, label=&#39;Generated using nour function&#39;); x_n = np.linspace(-10, 10, 100) plt.plot(x_n, laplace().pdf(x_n), lw=4, label=&#39;Generated from n scipy&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f51d4a99750&gt; . Box-Muller transform .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/04/16/inverse-transform.html",
            "relUrl": "/ml/2020/04/16/inverse-transform.html",
            "date": " • Apr 16, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Learning Gaussian Process regression parameters using gradient descent",
            "content": ". In previous posts, I have talked about GP regression: . Post 1 on programatically understanding GPs | Post 2 on making use of a popular GP library called GPy | . In this post, I will be talking about how to learn the parameters of a GP. I&#39;ll keep this post simple and specific to a trivial example using RBF kernel (though the methods discussed are general.) . To keep things simple, we will assume a mean prior of zero and we will only be learning the parameters of the kernel function. . Key Idea . Write the expression of log likelihood of data in terms of kernel parameters | Use gradient descent to optimize the objective (negative log likelihood) and update the kernel parameters | Defining log-likelihood . In our previous post we had mentioned (for the noiseless case): . Given train data $$ D= left(x_{i}, y_{i} right), i=1: N $$ Given a test set $X_{*}$ of size $N_{*} times d$ containing $N_{*}$ points in $ mathbb{R}^{d},$ we want to predict function outputs $y_{*}$ We can write: $$ left( begin{array}{l} y y_{*} end{array} right) sim mathcal{N} left( left( begin{array}{l} mu mu_{*} end{array} right), left( begin{array}{cc} K &amp; K_{*} K_{*}^{T} &amp; K_{* *} end{array} right) right) $$ where $$ begin{aligned} K &amp;= operatorname{Ker}(X, X) in mathbb{R}^{N times N} K_{*} &amp;= operatorname{Ker} left(X, X_{*} right) in mathbb{R}^{N times N} K_{* *} &amp;= operatorname{Ker} left(X_{*}, X_{*} right) in mathbb{R}^{N_{*} times N_{*}} end{aligned} $$ . Thus, from the property of conditioning of multivariate Gaussian, we know that: . $$y sim mathcal{N}_N( mu, K)$$ . We will assume $ mu$ to be zero. Thus, we have for the train data, the following expression: . $$y sim mathcal{N}_N(0, K)$$ . For the noisy case, we have: . $$y sim mathcal{N}_N(0, K + sigma_{noise}^2 mathcal{I}_N)$$ . From this expression, we can write the log-likelihood of data computed over the kernel parameters $ theta$ as: . $$ mathcal{LL}( theta) = log( frac{ exp((-1/2)(y-0)^T (K+ sigma_{noise}^2 mathcal{I}_N)^{-1}(y-0))}{(2 pi)^{N/2}|(K+ sigma_{noise}^2 mathcal{I}_N)|^{1/2}})$$ . Thus, we can write: . $$ mathcal{LL}( theta) = log P( mathbf{y} | X, theta)=- frac{1}{2} mathbf{y}^{ top} M^{-1} mathbf{y}- frac{1}{2} log |M|- frac{N}{2} log 2 pi$$ . where $$M = K + sigma_{noise}^2 mathcal{I}_N$$ . Imports . As before, we will be using the excellent Autograd library for automatically computing the gradient of an objective function with respect to the parameters. We will also be using GPy for verifying our calculations. . Let us start with some basic imports. . import autograd.numpy as np from matplotlib import pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) import GPy . Defining our RBF kernel . The definition of the (1-dimensional) RBF kernel has a Gaussian-form, defined as: . $$ kappa_ mathrm{rbf}(x_1,x_2) = sigma^2 exp left(- frac{(x_1-x_2)^2}{2 mathscr{l}^2} right) $$ def rbf(x1, x2, sigma, l): return (sigma**2)*(np.exp(-(x1-x2)**2/(2*(l**2)))) . Defining GPy&#39;s RBF kernel . # Create a 1-D RBF kernel with default parameters k = GPy.kern.RBF(1) # Preview the kernel&#39;s parameters k . rbf. valueconstraintspriors . &lt;td class=tg-left&gt; variance &lt;/td&gt;&lt;td class=tg-right&gt; 1.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; lengthscale&lt;/td&gt;&lt;td class=tg-right&gt; 1.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . Matching our RBF kernel with GPy&#39;s kernel . rbf(1, 0, 1, 1)==k.K(np.array([[1]]), np.array([[0]])).flatten() . array([ True]) . Looks good. Our function is matching GPy&#39;s kernel. . GP Regresion . Creating a data set . # lambda function, call f(x) to generate data f = lambda x: 0.4*x**2 - 0.15*x**3 + 0.5*x**2 - 0.002*x**5 + 0.0002*x**6 +0.5*(x-2)**2 n = 20 np.random.seed(0) X = np.linspace(0.05, 4.95, n)[:,None] Y = f(X) + np.random.normal(0., 0.1, (n,1)) # note that np.random.normal takes mean and s.d. (not variance), 0.1^2 = 0.01 plt.plot(X, Y, &quot;kx&quot;, mew=2, label=&#39;Train points&#39;) plt.xlabel(&quot;x&quot;), plt.ylabel(&quot;f&quot;) plt.legend(); . Function to compute negative log likelihood . Based on our above mentioned theory, we can now write the NLL function as follows . def nll(sigma=1, l=1, noise_std=1): n = X.shape[0] cov = rbf(X, X.T, sigma, l) + (noise_std**2)*np.eye(X.shape[0]) nll_ar = 0.5*(Y.T@np.linalg.inv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) return nll_ar[0,0] . Comparing the NLL from our method with the NLL from GPy . We will now compare the NLL from our method with GPy for a fixed set of parameters . nll(1, 1, 1) . 40.103960984801276 . k.lengthscale = 1 k.variance = 1 m = GPy.models.GPRegression(X, Y, k, normalizer=False) m.Gaussian_noise = 1 print(m) . Name : GP regression Objective : 40.103961039553916 Number of Parameters : 3 Number of Optimization Parameters : 3 Updates : True Parameters: GP_regression. | value | constraints | priors rbf.variance | 1.0 | +ve | rbf.lengthscale | 1.0 | +ve | Gaussian_noise.variance | 1.0 | +ve | . Excellent, we can see that our method gives the same NLL. Looks like we are on the right track! One caveat here is that I have set the normalizer to be False, which means that GPy will not be mean centering the data. . Optimizing the GP using GPy . We will now use GPy to optimize the GP parameters . m = GPy.models.GPRegression(X, Y, k, normalizer=False) m.optimize() print(m) . Name : GP regression Objective : -2.9419881541130053 Number of Parameters : 3 Number of Optimization Parameters : 3 Updates : True Parameters: GP_regression. | value | constraints | priors rbf.variance | 27.837243180547883 | +ve | rbf.lengthscale | 2.732180018958835 | +ve | Gaussian_noise.variance | 0.007573211752763481 | +ve | . It seems that variance close to 28 and length scale close to 2.7 give the optimum objective for the GP . Plotting the NLL as a function of variance and lenghtscale . We will now plot the NLL obtained from our calculations as a function of variance and lengthscale. For comparing our solution with GPy solution, I will be setting noise variance to be 0.0075 . import numpy as numpy x_grid_2, y_grid_2 = numpy.mgrid[0.1:6:0.04, 0.1:4:0.03] li = np.zeros_like(x_grid_2) for i in range(x_grid_2.shape[0]): for j in range(x_grid_2.shape[1]): li[i, j] = nll(x_grid_2[i, j], y_grid_2[i, j], np.sqrt(.007573211752763481)) . plt.contourf(x_grid_2, y_grid_2, li) plt.gca().set_aspect(&#39;equal&#39;) plt.xlabel(r&quot;$ sigma$&quot;) plt.ylabel(r&quot;$l$&quot;) plt.colorbar() plt.title(r&quot;NLL ($ sigma, l$)&quot;) . Text(0.5, 1.0, &#39;NLL ($ sigma, l$)&#39;) . We will now try to find the &quot;optimum&quot; $ sigma$ and lengthscale from this NLL space. . print(li.min()) aa, bb = np.unravel_index(li.argmin(), li.shape) print(x_grid_2[aa, 0]**2, y_grid_2[bb, 0]) . -2.9418973674348727 28.09 0.1 . Excellent, it looks like we are pretty close to the optimum NLL as reported by GPy and our parameters learnt are also pretty similar. But, we have not even done a thorough search. We will now be using gradient descent to help us find the optimum set of parameters. . Gradient descent using autograd . from autograd import elementwise_grad as egrad from autograd import grad . grad_objective = grad(nll, argnum=[0, 1, 2]) . Visualising the objective as a function of iteration . sigma = 2. l = 2. noise = 1. lr = 1e-3 num_iter = 100 nll_arr = np.zeros(num_iter) for iteration in range(num_iter): nll_arr[iteration] = nll(sigma, l, noise) del_sigma, del_l, del_noise = grad_objective(sigma, l, noise) sigma = sigma - lr*del_sigma l = l - lr*del_l noise = noise - lr*del_noise . print(sigma**2, l, noise) . 5.108812267877177 1.9770216805277476 0.11095385387537618 . plt.plot(nll_arr) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;NLL&quot;) . Text(0, 0.5, &#39;NLL&#39;) . Applying gradient descent and visualising the learnt function . sigma = 2. l = 2. noise = 1. lr = 1e-3 num_iter = 100 nll_arr = np.zeros(num_iter) fig, ax = plt.subplots() for iteration in range(num_iter): nll_arr[iteration] = nll(sigma, l, noise) del_sigma, del_l, del_noise = grad_objective(sigma, l, noise) sigma = sigma - lr*del_sigma l = l - lr*del_l noise = noise - lr*del_noise k.lengthscale = l k.variance = sigma**2 m = GPy.models.GPRegression(X, Y, k, normalizer=False) m.Gaussian_noise = noise**2 m.plot(ax=ax)[&#39;dataplot&#39;]; plt.ylim((0, 6)) plt.title(f&quot;Iteration: {iteration:04}, Objective :{nll_arr[iteration]}&quot;) plt.savefig(f&quot;/home/nipunbatra-pc/Desktop/gp_learning/{iteration:04}.png&quot;) plt.cla(); plt.clf() . &lt;Figure size 432x288 with 0 Axes&gt; . !convert -delay 20 -loop 0 /home/nipunbatra-pc/Desktop/gp_learning/*.png gp-learning.gif . . Excellent, we can see the &quot;learning&quot; process over time. Our final objective is comparable to GPy&#39;s objective. . There are a few things I have mentioned, yet have not gone into their details and I would encourage you to try those out. . First, you should try the gradient descent procedure with restarts. Run with different random initialisations and finally report the parameters which give the optimum likelihood. | We assume mean zero prior here. However, we are not processing the data and thus the zero mean assumption is not very well suited to our data. If you reduce the number of data points, you would quickly see the GP prediction to fall close to zero. | There you go. Till next time! .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/03/29/param-learning.html",
            "relUrl": "/ml/2020/03/29/param-learning.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Active Learning with Bayesian Linear Regression",
            "content": "A quick wrap-up for Bayesian Linear Regression (BLR) . We have a feature matrix $X$ and a target vector $Y$. We want to obtain $ theta$ vector in such a way that the error $ epsilon$ for the following equation is minimum. . begin{equation} Y = X^T theta + epsilon end{equation}Prior PDF for $ theta$ is, . begin{equation} p( theta) sim mathcal{N}(M_0, S_0) end{equation}Where $S_0$ is prior covariance matrix, and $M_0$ is prior mean. . Posterier PDF can be given as, . begin{equation} p( theta|X,Y) sim mathcal{N}( theta | M_n, S_n) S_n = (S_0^{-1} + sigma_{mle}^{-2}X^TX) M_n = S_n(S_0^{-1}M_0+ sigma_{mle}^{-2}X^TY) end{equation}Maximum likelihood estimation of $ sigma$ can be calculated as, . begin{equation} theta_{mle} = (X^TX)^{-1}X^TY sigma_{mle} = ||Y - X^T theta_{mle}|| end{equation}Finally, predicted mean $ hat{Y}_{mean}$ and predicted covariance matrix $ hat{Y}_{cov}$ can be given as, . begin{equation} hat{Y} sim mathcal{N}( hat{Y}_{mean}, hat{Y}_{cov}) hat{Y}_{mean} = XM_n hat{Y}_{cov} = X^TS_nX end{equation}Now, let&#39;s put everything together and write a class for Bayesian Linear Regression. . Creating scikit-learn like class with fit predict methods for BLR . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn import datasets from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler from sklearn.model_selection import train_test_split from matplotlib.animation import FuncAnimation from matplotlib import rc import warnings warnings.filterwarnings(&#39;ignore&#39;) seed = 0 # random seed for train_test_split . class BLR(): def __init__(self,S0, M0): # M0 -&gt; prior mean, S0 -&gt; prior covariance matrix self.S0 = S0 self.M0 = M0 def fit(self,x,y, return_self = False): self.x = x self.y = y # Maximum likelihood estimation for sigma parameter theta_mle = np.linalg.pinv(x.T@x)@(x.T@y) sigma_2_mle = np.linalg.norm(y - x@theta_mle)**2 sigma_mle = np.sqrt(sigma_2_mle) # Calculating predicted mean and covariance matrix for theta self.SN = np.linalg.pinv(np.linalg.pinv(self.S0) + (sigma_mle**-2)*x.T@x) self.MN = self.SN@(np.linalg.pinv(self.S0)@self.M0 + (sigma_mle**-2)*(x.T@y).squeeze()) # Calculating predicted mean and covariance matrix for data self.pred_var = x@self.SN@x.T self.y_hat_map = x@self.MN if return_self: return (self.y_hat_map, self.pred_var) def predict(self, x): self.pred_var = x@self.SN@x.T self.y_hat_map = x@self.MN return (self.y_hat_map, self.pred_var) def plot(self, s=1): # s -&gt; size of dots for scatter plot individual_var = self.pred_var.diagonal() plt.figure() plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.plot(self.x[:,1], self.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) plt.fill_between(self.x[:,1], self.y_hat_map-individual_var, self.y_hat_map+individual_var, alpha=0.4, color=&#39;black&#39;, label=&#39;uncertainty&#39;) plt.scatter(self.x[:,1], self.y, label=&#39;actual data&#39;,s=s) plt.title(&#39;MAE is &#39;+str(np.mean(np.abs(self.y - self.y_hat_map)))) plt.legend() . Creating &amp; visualizing dataset . To start with, let&#39;s create a random dataset with degree 3 polynomial function with some added noise. . begin{equation} Y = (5X^3 - 4X^2 + 3X - 2) + mathcal{N}(0,1) end{equation} np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . We&#39;ll try to fit a degree 5 polynomial function to our data. . X = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1)) N_features = X.shape[1] . plt.scatter(X[:,1], Y, s=0.5, label = &#39;data points&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.legend() plt.show() . Learning a BLR model on the entire data . We&#39;ll take $M_0$ (prior mean) as zero vector initially, assuming that we do not have any prior knowledge about $M_0$. We&#39;re taking $S_0$ (prior covariance) as the identity matrix, assuming that all coefficients are completely independent of each other. . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) model = BLR(S0, M0) . model.fit(X, Y) . Visualising the fit . model.plot(s=0.5) . This doesn&#39;t look like a good fit, right? Let&#39;s set the prior closer to the real values and visualize the fit again. . Visualising the fit after changing the prior . np.random.seed(seed) S0 = np.eye(N_features) M0 = np.array([-2, 3, -4, 5, 0, 0]) + np.random.randn(N_features, ) model = BLR(S0, M0) . model.fit(X, Y) model.plot(s=0.5) . Hmm, better. Now let&#39;s see how it fits after reducing the noise and setting the prior mean to zero vector again. . Visualising the fit after reducing the noise . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) model = BLR(S0, M0) . model.fit(X, Y) model.plot(s=0.5) . When the noise was high, the model tended to align with the prior. After keeping the prior closer to the original coefficients, the model was improved as expected. From the last plot, we can say that as noise reduces from the data, the impact of the prior reduces, and the model tries to fit the data more precisely. Therefore, we can say that when data is too noisy or insufficient, a wisely chosen prior can produce a precise fit. . Intuition to Active Learning (Uncertainty Sampling) with an example . Let&#39;s take the case where we want to train a machine learning model to classify if a person is infected with COVID-19 or not, but the testing facilities for the same are not available so widely. We may have very few amounts of data for detected positive and detected negative patients. Now, we want our model to be highly confident or least uncertain about its results; otherwise, it may create havoc for wrongly classified patients, but, our bottleneck is labeled data. Thanks to active learning techniques, we can overcome this problem smartly. How? . We train our model with existing data and test it on all the suspected patients&#39; data. Let&#39;s say we have an uncertainty measure or confidence level about each tested data point (distance from the decision boundary in case of SVM, variance in case of Gaussian processes, or Bayesian Linear Regression). We can choose a patient for which our model is least certain, and send him to COVID-19 testing facilities (assuming that we can send only one patient at a time). Now, we can include his data to the train set and test the model on everyone else. By following the same procedure repeatedly, we can increase the size of our train data and confidence of the model without sending everyone randomly for testing. . This method is called Uncertainty Sampling in Active Learning. Now let&#39;s formally define Active Learning. From Wikipedia, . Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. . Now, we&#39;ll go through the active learning procedure step by step. . Train set, test set, and pool. What is what? . The train set includes labeled data points. The pool includes potential data points to query for a label, and the test set includes labeled data points to check the performance of our model. Here, we cannot actually do a query to anyone, so we assume that we do not have labels for the pool while training, and after each iteration, we include a data point from the pool set to the train set for which our model has the highest uncertainty. . So, the algorithm can be represented as the following, . Train the model with the train set. | Test the performance on the test set (This should keep improving). | Test the model with the pool. | Query for the most uncertain datapoint from the pool. | Add that datapoint into the train set. | Repeat step 1 to step 5 for $K$ iterations ($K$ ranges from $0$ to the pool size). | Creating initial train set, test set, and pool . Let&#39;s take half of the dataset as the test set, and from another half, we will start with some points as the train set and remaining as the pool. Let&#39;s start with 2 data points as the train set. . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) X = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1)) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . train_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed) train_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=2, random_state=seed) . Visualizing train, test and pool. . plt.scatter(test_X[:,1], test_Y, label=&#39;test set&#39;,color=&#39;r&#39;, s=2) plt.scatter(train_X[:,1], train_Y, label=&#39;train set&#39;,marker=&#39;s&#39;,color=&#39;k&#39;, s=50) plt.scatter(pool_X[:,1], pool_Y, label=&#39;pool&#39;,color=&#39;b&#39;, s=2) plt.xlabel(&#39;X&#39;) plt.ylabel(&#39;Y&#39;) plt.legend() plt.show() . Let&#39;s initialize a few dictionaries to keep track of each iteration. . train_X_iter = {} # to store train points at each iteration train_Y_iter = {} # to store corresponding labels to the train set at each iteration models = {} # to store the models at each iteration estimations = {} # to store the estimations on the test set at each iteration test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration . Training &amp; testing initial learner on train set (Iteration 0) . Now we will train the model for the initial train set, which is iteration 0. . train_X_iter[0] = train_X train_Y_iter[0] = train_Y . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) models[0] = BLR(S0, M0) . models[0].fit(train_X_iter[0], train_Y_iter[0]) . Creating a plot method to visualize train, test and pool with estimations and uncertainty. . def plot(ax, model, init_title=&#39;&#39;): # Plotting the pool ax.scatter(pool_X[:,1], pool_Y, label=&#39;pool&#39;,s=1,color=&#39;r&#39;,alpha=0.4) # Plotting the test data ax.scatter(test_X[:,1], test_Y, label=&#39;test data&#39;,s=1, color=&#39;b&#39;, alpha=0.4) # Combining the test &amp; the pool test_pool_X, test_pool_Y = np.append(test_X,pool_X, axis=0), np.append(test_Y,pool_Y) # Sorting test_pool for plotting sorted_inds = np.argsort(test_pool_X[:,1]) test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds] # Plotting test_pool with uncertainty model.predict(test_pool_X) individual_var = model.pred_var.diagonal() ax.plot(test_pool_X[:,1], model.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var , alpha=0.2, color=&#39;black&#39;, label=&#39;uncertainty&#39;) # Plotting the train data ax.scatter(model.x[:,1], model.y,s=40, color=&#39;k&#39;, marker=&#39;s&#39;, label=&#39;train data&#39;) ax.scatter(model.x[-1,1], model.y[-1],s=80, color=&#39;r&#39;, marker=&#39;o&#39;, label=&#39;last added point&#39;) # Plotting MAE on the test set model.predict(test_X) ax.set_title(init_title+&#39; MAE is &#39;+str(np.mean(np.abs(test_Y - model.y_hat_map)))) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.legend() . Plotting the estimations and uncertainty. . fig, ax = plt.subplots() plot(ax, models[0]) . Let&#39;s check the maximum uncertainty about any point for the model. . models[0].pred_var.diagonal().max() . 4.8261426545316604e-29 . Oops!! There is almost no uncertainty in the model. Why? let&#39;s try again with more train points. . train_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed) train_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=7, random_state=seed) . train_X_iter[0] = train_X train_Y_iter[0] = train_Y . S0 = np.eye(N_features) M0 = np.zeros((N_features, )) models[0] = BLR(S0, M0) . models[0].fit(train_X_iter[0], train_Y_iter[0]) . fig, ax = plt.subplots() plot(ax, models[0]) . Now uncertainty is visible, and currently, it&#39;s high at the left-most points. We are trying to fit a degree 5 polynomial here. So our linear regression coefficients are 6, including the bias. If we choose train points equal to or lesser than 6, our model perfectly fits the train points and has no uncertainty. Choosing train points more than 6 induces uncertainty in the model. . Let&#39;s evaluate the performance on the test set. . estimations[0], _ = models[0].predict(test_X) test_mae_error[0] = np.mean(np.abs(test_Y - estimations[0])) . Mean Absolute Error (MAE) on the test set is . test_mae_error[0] . 0.5783654195019617 . Moving the most uncertain point from the pool to the train set . In the previous plot, we saw that the model was least certain about the left-most point. We&#39;ll move that point from the pool to the train set and see the effect. . esimations_pool, _ = models[0].predict(pool_X) . Finding out a point having the most uncertainty. . in_var = models[0].pred_var.diagonal().argmax() to_add_x = pool_X[in_var,:] to_add_y = pool_Y[in_var] . Adding the point from the pool to the train set. . train_X_iter[1] = np.vstack([train_X_iter[0], to_add_x]) train_Y_iter[1] = np.append(train_Y_iter[0], to_add_y) . Deleting the point from the pool. . pool_X = np.delete(pool_X, in_var, axis=0) pool_Y = np.delete(pool_Y, in_var) . Training again and visualising the results (Iteration 1) . This time, we will pass previously learnt prior to the next iteration. . S0 = np.eye(N_features) models[1] = BLR(S0, models[0].MN) . models[1].fit(train_X_iter[1], train_Y_iter[1]) . estimations[1], _ = models[1].predict(test_X) test_mae_error[1] = np.mean(np.abs(test_Y - estimations[1])) . MAE on the test set is . test_mae_error[1] . 0.5779411133071186 . Visualizing the results. . fig, ax = plt.subplots() plot(ax, models[1]) . Before &amp; after adding most uncertain point . fig, ax = plt.subplots(1,2, figsize=(13.5,4.5)) plot(ax[0], models[0],&#39;Before&#39;) plot(ax[1], models[1],&#39;After&#39;) . We can see that including most uncertain point into the train set has produced a better fit and MAE for test set has been reduced. Also, uncertainty has reduced at the left part of the data but it has increased a bit on the right part of the data. . Now let&#39;s do this for few more iterations in a loop and visualise the results. . Active learning procedure . num_iterations = 20 points_added_x= np.zeros((num_iterations+1, N_features)) points_added_y=[] print(&quot;Iteration, Cost n&quot;) print(&quot;-&quot;*40) for iteration in range(2, num_iterations+1): # Making predictions on the pool set based on model learnt in the respective train set estimations_pool, var = models[iteration-1].predict(pool_X) # Finding the point from the pool with highest uncertainty in_var = var.diagonal().argmax() to_add_x = pool_X[in_var,:] to_add_y = pool_Y[in_var] points_added_x[iteration-1,:] = to_add_x points_added_y.append(to_add_y) # Adding the point to the train set from the pool train_X_iter[iteration] = np.vstack([train_X_iter[iteration-1], to_add_x]) train_Y_iter[iteration] = np.append(train_Y_iter[iteration-1], to_add_y) # Deleting the point from the pool pool_X = np.delete(pool_X, in_var, axis=0) pool_Y = np.delete(pool_Y, in_var) # Training on the new set models[iteration] = BLR(S0, models[iteration-1].MN) models[iteration].fit(train_X_iter[iteration], train_Y_iter[iteration]) estimations[iteration], _ = models[iteration].predict(test_X) test_mae_error[iteration]= pd.Series(estimations[iteration] - test_Y.squeeze()).abs().mean() print(iteration, (test_mae_error[iteration])) . Iteration, Cost - 2 0.49023173501654815 3 0.4923391714942153 4 0.49040074812746753 5 0.49610198614600165 6 0.5015282102751122 7 0.5051264429971314 8 0.5099913097301352 9 0.504455016053513 10 0.5029219102020734 11 0.5009762782262487 12 0.5004883097883343 13 0.5005169638980388 14 0.5002731089932334 15 0.49927485683909884 16 0.49698416490822594 17 0.49355398855432897 18 0.49191185613804617 19 0.491164833699368 20 0.4908067530719673 . pd.Series(test_mae_error).plot(style=&#39;ko-&#39;) plt.xlim((-0.5, num_iterations+0.5)) plt.ylabel(&quot;MAE on test set&quot;) plt.xlabel(&quot;# Points Queried&quot;) plt.show() . The plot above shows that MAE on the test set fluctuates a bit initially then reduces gradually as we keep including more points from the pool to the train set. Let&#39;s visualise fits for all the iterations. We&#39;ll discuss this behaviour after that. . Visualizing active learning procedure . print(&#39;Initial model&#39;) print(&#39;Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}&#39;.format(*models[0].MN[::-1])) print(&#39; nFinal model&#39;) print(&#39;Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}&#39;.format(*models[num_iterations].MN[::-1])) . Initial model Y = 1.89 X^5 + 1.54 X^4 + 0.84 X^3 + -6.48 X^2 + 4.74 X + -1.63 Final model Y = 2.50 X^5 + 3.11 X^4 + 0.83 X^3 + -7.08 X^2 + 4.47 X + -1.58 . def update(iteration): ax.cla() plot(ax, models[iteration]) fig.tight_layout() . fig, ax = plt.subplots() anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations+1, 1), interval=250) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) . anim . &lt;/input&gt; Once Loop Reflect We can see that the point having highest uncertainty was chosen in first iteration and it produced the near optimal fit. After that, error reduced gradually. . Now, let&#39;s put everything together and create a class for active learning procedure . Creating a class for active learning procedure . class ActiveL(): def __init__(self, X, y, S0=None, M0=None, test_size=0.5, degree = 5, iterations = 20, seed=1): self.X_init = X self.y = y self.S0 = S0 self.M0 = M0 self.train_X_iter = {} # to store train points at each iteration self.train_Y_iter = {} # to store corresponding labels to the train set at each iteration self.models = {} # to store the models at each iteration self.estimations = {} # to store the estimations on the test set at each iteration self.test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration self.test_size = test_size self.degree = degree self.iterations = iterations self.seed = seed self.train_size = degree + 2 def data_preperation(self): # Adding polynomial features self.X = PolynomialFeatures(degree=self.degree).fit_transform(self.X_init) N_features = self.X.shape[1] # Splitting into train, test and pool train_pool_X, self.test_X, train_pool_Y, self.test_Y = train_test_split(self.X, self.y, test_size=self.test_size, random_state=self.seed) self.train_X, self.pool_X, self.train_Y, self.pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=self.train_size, random_state=self.seed) # Setting BLR prior incase of not given if self.M0 == None: self.M0 = np.zeros((N_features, )) if self.S0 == None: self.S0 = np.eye(N_features) def main(self): # Training for iteration 0 self.train_X_iter[0] = self.train_X self.train_Y_iter[0] = self.train_Y self.models[0] = BLR(self.S0, self.M0) self.models[0].fit(self.train_X, self.train_Y) # Running loop for all iterations for iteration in range(1, self.iterations+1): # Making predictions on the pool set based on model learnt in the respective train set estimations_pool, var = self.models[iteration-1].predict(self.pool_X) # Finding the point from the pool with highest uncertainty in_var = var.diagonal().argmax() to_add_x = self.pool_X[in_var,:] to_add_y = self.pool_Y[in_var] # Adding the point to the train set from the pool self.train_X_iter[iteration] = np.vstack([self.train_X_iter[iteration-1], to_add_x]) self.train_Y_iter[iteration] = np.append(self.train_Y_iter[iteration-1], to_add_y) # Deleting the point from the pool self.pool_X = np.delete(self.pool_X, in_var, axis=0) self.pool_Y = np.delete(self.pool_Y, in_var) # Training on the new set self.models[iteration] = BLR(self.S0, self.models[iteration-1].MN) self.models[iteration].fit(self.train_X_iter[iteration], self.train_Y_iter[iteration]) self.estimations[iteration], _ = self.models[iteration].predict(self.test_X) self.test_mae_error[iteration]= pd.Series(self.estimations[iteration] - self.test_Y.squeeze()).abs().mean() def _plot_iter_MAE(self, ax, iteration): ax.plot(list(self.test_mae_error.values())[:iteration+1], &#39;ko-&#39;) ax.set_title(&#39;MAE on test set over iterations&#39;) ax.set_xlim((-0.5, self.iterations+0.5)) ax.set_ylabel(&quot;MAE on test set&quot;) ax.set_xlabel(&quot;# Points Queried&quot;) def _plot(self, ax, model): # Plotting the pool ax.scatter(self.pool_X[:,1], self.pool_Y, label=&#39;pool&#39;,s=1,color=&#39;r&#39;,alpha=0.4) # Plotting the test data ax.scatter(self.test_X[:,1], self.test_Y, label=&#39;test data&#39;,s=1, color=&#39;b&#39;, alpha=0.4) # Combining test_pool test_pool_X, test_pool_Y = np.append(self.test_X, self.pool_X, axis=0), np.append(self.test_Y, self.pool_Y) # Sorting test_pool sorted_inds = np.argsort(test_pool_X[:,1]) test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds] # Plotting test_pool with uncertainty preds, var = model.predict(test_pool_X) individual_var = var.diagonal() ax.plot(test_pool_X[:,1], model.y_hat_map, color=&#39;black&#39;, label=&#39;model&#39;) ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var , alpha=0.2, color=&#39;black&#39;, label=&#39;uncertainty&#39;) # plotting the train data ax.scatter(model.x[:,1], model.y,s=10, color=&#39;k&#39;, marker=&#39;s&#39;, label=&#39;train data&#39;) ax.scatter(model.x[-1,1], model.y[-1],s=80, color=&#39;r&#39;, marker=&#39;o&#39;, label=&#39;last added point&#39;) # plotting MAE preds, var = model.predict(self.test_X) ax.set_title(&#39;MAE is &#39;+str(np.mean(np.abs(self.test_Y - preds)))) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.legend() def visualise_AL(self): fig, ax = plt.subplots(1,2,figsize=(13,5)) def update(iteration): ax[0].cla() ax[1].cla() self._plot(ax[0], self.models[iteration]) self._plot_iter_MAE(ax[1], iteration) fig.tight_layout() print(&#39;Initial model&#39;) print(&#39;Y = &#39;+&#39; + &#39;.join([&#39;{0:0.2f}&#39;.format(self.models[0].MN[i])+&#39; X^&#39;*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)])) print(&#39; nFinal model&#39;) print(&#39;Y = &#39;+&#39; + &#39;.join([&#39;{0:0.2f}&#39;.format(self.models[self.iterations].MN[i])+&#39; X^&#39;*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)])) anim = FuncAnimation(fig, update, frames=np.arange(0, self.iterations+1, 1), interval=250) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) return anim . Visualizing a different polynomial fit on the same dataset . Let&#39;s try to fit a degree 7 polynomial to the same data now. . np.random.seed(seed) X_init = np.linspace(-1, 1, 1000) noise = np.random.randn(1000, ) * 0.5 Y = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise . model = ActiveL(X_init.reshape(-1,1), Y, degree=7, iterations=20, seed=seed) . model.data_preperation() model.main() model.visualise_AL() . Initial model Y = -1.92 + 3.79 X^1 + -1.81 X^2 + -0.43 X^3 + -0.51 X^4 + -0.27 X^5 + -0.18 X^6 + -0.11 X^7 Final model Y = -1.79 + 4.86 X^1 + -5.38 X^2 + 0.50 X^3 + -0.17 X^4 + 1.19 X^5 + 1.83 X^6 + 1.31 X^7 . &lt;/input&gt; Once Loop Reflect We can clearly see that model was fitting the train points well and uncertainty was high at the left-most position. After first iteration, the left-most point was added to the train set and MAE reduced significantly. Similar phenomeneon happened at iteration 2 with the right-most point. After that error kept reducing at slower rate gradually because fit was near optimal after just 2 iterations. . Active learning for diabetes dataset from the Scikit-learn module . Let&#39;s run our model for diabetes data from sklearn module. The data have various features like age, sex, weight etc. of diabetic people and target is increment in disease after one year. We&#39;ll choose only &#39;weight&#39; feature, which seems to have more correlation with the target. . We&#39;ll try to fit degree 1 polynomial to this data, as our data seems to have a linear fit. First, let&#39;s check the performance of Scikit-learn linear regression model. . X, Y = datasets.load_diabetes(return_X_y=True) X = X[:, 2].reshape(-1,1) # Choosing only feature 2 which seems more relevent to linear regression # Normalizing X = (X - X.min())/(X.max() - X.min()) Y = (Y - Y.min())/(Y.max() - Y.min()) . Visualizing the dataset. . plt.scatter(X, Y) plt.xlabel(&#39;Weight of the patients&#39;) plt.ylabel(&#39;Increase in the disease after a year&#39;) plt.show() . Let&#39;s fit the Scikit-learn linear regression model with 50% train-test split. . from sklearn.linear_model import LinearRegression train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state = seed) . clf = LinearRegression() . clf.fit(train_X, train_Y) pred_Y = clf.predict(test_X) . Visualizing the fit &amp; MAE. . plt.scatter(X, Y, label=&#39;data&#39;, s=5) plt.plot(test_X, pred_Y, label=&#39;model&#39;, color=&#39;r&#39;) plt.xlabel(&#39;Weight of the patients&#39;) plt.ylabel(&#39;Increase in the disease after a year&#39;) plt.title(&#39;MAE is &#39;+str(np.mean(np.abs(pred_Y - test_Y)))) plt.legend() plt.show() . Now we&#39;ll fit the same data with our BLR model . model = ActiveL(X.reshape(-1,1), Y, degree=1, iterations=20, seed=seed) . model.data_preperation() model.main() model.visualise_AL() . Initial model Y = 0.41 + 0.16 X^1 Final model Y = 0.13 + 0.86 X^1 . &lt;/input&gt; Once Loop Reflect Initially, the fit is leaning towards zero slope, which is the influence of bias due to a low number of training points. It&#39;s interesting to see that our initial train points tend to make a vertical fit, but the model doesn&#39;t get carried away by that and stabilizes the self with prior. . print(&#39;MAE for Scikit-learn Linear Regression is&#39;,np.mean(np.abs(pred_Y - test_Y))) print(&#39;MAE for Bayesian Linear Regression is&#39;, model.test_mae_error[20]) . MAE for Scikit-learn Linear Regression is 0.15424985705353944 MAE for Bayesian Linear Regression is 0.15738001811804758 . At the end, results of sklearn linear regression and our active learning based BLR model are comparable even though we&#39;ve used only 20 points to train our model over 221 points used by sklearn. This is because active learning enables us to choose those datapoints for training, which are going to contribute the most towards a precise fit. .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/03/28/Active_Learning_with_Bayesian_Linear_Regression.html",
            "relUrl": "/ml/2020/03/28/Active_Learning_with_Bayesian_Linear_Regression.html",
            "date": " • Mar 28, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Some experiments in Gaussian Processes Regression",
            "content": ". Disclaimer . This blog post is forked from GPSS 2019 Lab 1. This is produced only for educational purposes. All credit goes to the GPSS organisers. . # Support for maths import numpy as np # Plotting tools from matplotlib import pyplot as plt # we use the following for plotting figures in jupyter %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) # GPy: Gaussian processes library import GPy from IPython.display import display . Covariance functions, aka kernels . We will define a covariance function, from hereon referred to as a kernel, using GPy. The most commonly used kernel in machine learning is the Gaussian-form radial basis function (RBF) kernel. It is also commonly referred to as the exponentiated quadratic or squared exponential kernel &ndash; all are equivalent. . The definition of the (1-dimensional) RBF kernel has a Gaussian-form, defined as: . $$ kappa_ mathrm{rbf}(x,x&#39;) = sigma^2 exp left(- frac{(x-x&#39;)^2}{2 mathscr{l}^2} right) $$It has two parameters, described as the variance, $ sigma^2$ and the lengthscale $ mathscr{l}$. . In GPy, we define our kernels using the input dimension as the first argument, in the simplest case input_dim=1 for 1-dimensional regression. We can also explicitly define the parameters, but for now we will use the default values: . # Create a 1-D RBF kernel with default parameters k = GPy.kern.RBF(1) # Preview the kernel&#39;s parameters k . rbf. valueconstraintspriors . &lt;td class=tg-left&gt; variance &lt;/td&gt;&lt;td class=tg-right&gt; 1.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; lengthscale&lt;/td&gt;&lt;td class=tg-right&gt; 1.0&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . We can see from the above table that our kernel has two parameters, variance and lengthscale, both with value 1.0. There is also information on the constraints and priors on each parameter, but we will look at this later. . Visualising the kernel . We can visualise our kernel in a few different ways. We can plot the shape of the kernel by plotting $k(x,0)$ over some sample space $x$ which, looking at the equation above, clearly has a Gaussian shape. This describes the covariance between each sample location and $0$. . # Our sample space: 100 samples in the interval [-4,4] X = np.linspace(-4.,4.,100)[:, None] # we need [:, None] to reshape X into a column vector for use in GPy # First, sample kernel at x&#39; = 0 K = k.K(X, np.array([[0.]])) # k(x,0) . plt.plot(X, K) plt.title(&quot;$ kappa_{rbf}(x,x&#39;)$&quot;); . Writing an animation function routine for visualsing kernel with changing length scale . fig, ax = plt.subplots() from matplotlib.animation import FuncAnimation from matplotlib import rc ls = [0.05, 0.25, 0.5, 1., 2., 4.] def update(iteration): ax.cla() k = GPy.kern.RBF(1) k.lengthscale = ls[iteration] # Calculate the new covariance function at k(x,0) C = k.K(X, np.array([[0.]])) # Plot the resulting covariance vector ax.plot(X,C) ax.set_title(&quot;$ kappa_{rbf}(x,x&#39;)$ nLength scale = %s&quot; %k.lengthscale[0]); ax.set_ylim((0, 1.2)) num_iterations = len(ls) anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=500) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect From the animation above, we can notice that increasing the scale means that a point becomes more correlated with a further away point. Using such a kernel for GP regression and increasing the length scale would mean making the regression smoother. . Writing an animation function routine for visualsing kernel with changing variance . fig, ax = plt.subplots() import os from matplotlib.animation import FuncAnimation from matplotlib import rc variances = [0.01, 0.05, 0.25, 0.5, 1., 2., 4.] def update(iteration): ax.cla() k = GPy.kern.RBF(1) k.variance = variances[iteration] # Calculate the new covariance function at k(x,0) C = k.K(X, np.array([[0.]])) # Plot the resulting covariance vector ax.plot(X,C) ax.set_title(&quot;$ kappa_{rbf}(x,x&#39;)$ nVariance = %s&quot; %k.variance[0]); ax.set_ylim((0, 2)) num_iterations = len(ls) anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=500) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect Alternatively, we can construct a full covariance matrix, $ mathbf{K}_{xx} triangleq k(x,x&#39;)$ with samples $x = x&#39;$. The resulting GP prior is a multivariate normal distribution over the space of samples $x$: $ mathcal{N}( mathbf{0}, mathbf{K}_{xx})$. It should be evident then that the elements of the matrix represents the covariance between respective points in $x$ and $x&#39;$, and that it is exactly $ sigma^2[=1]$ in the diagonal. . X = np.linspace(-4.,4.,30)[:, None] K = k.K(X,X) # Plot the covariance of the sample space plt.pcolor(X.T, X, K) # Format and annotate plot plt.gca().invert_yaxis(), plt.gca().axis(&quot;image&quot;) plt.xlabel(&quot;x&quot;), plt.ylabel(&quot;x&#39;&quot;), plt.colorbar() plt.title(&quot;$ kappa_{rbf}(x,x&#39;)$&quot;); . fig, ax = plt.subplots() cax = fig.add_axes([0.87, 0.2, 0.05, 0.65]) def update(iteration): ax.cla() cax.cla() k = GPy.kern.RBF(1) k.lengthscale = ls[iteration] # Calculate the new covariance function at k(x,0) K = k.K(X,X) # Plot the covariance of the sample space im = ax.pcolor(X.T, X, K) # Format and annotate plot ax.invert_yaxis() ax.axis(&quot;image&quot;) #ax.colorbar() # Plot the resulting covariance vector ax.set_title(&quot;Length scale = %s&quot; %k.lengthscale[0]); #ax.set_ylim((0, 1.2)) fig.colorbar(im, cax=cax, orientation=&#39;vertical&#39;) fig.tight_layout() num_iterations = len(ls) anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations, 1), interval=500) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect The above animation shows the impact of increasing length scale on the covariance matrix. . fig, ax = plt.subplots() cax = fig.add_axes([0.87, 0.2, 0.05, 0.65]) def update(iteration): ax.cla() cax.cla() k = GPy.kern.RBF(1) k.variance = variances[iteration] # Calculate the new covariance function at k(x,0) K = k.K(X,X) # Plot the covariance of the sample space im = ax.pcolor(X.T, X, K) # Format and annotate plot ax.invert_yaxis() ax.axis(&quot;image&quot;) #ax.colorbar() # Plot the resulting covariance vector ax.set_title(&quot;Variance = %s&quot; %k.variance[0]); #ax.set_ylim((0, 1.2)) fig.colorbar(im, cax=cax, orientation=&#39;vertical&#39;) fig.tight_layout() num_iterations = len(ls) anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations, 1), interval=500) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect The above animation shows the impact of increasing variance on the covariance matrix. Notice the scale on the colorbar changing. . GP Regresion . Creating a data set . # lambda function, call f(x) to generate data f = lambda x: 0.4*x**2 - 0.15*x**3 + 0.5*x**2 - 0.002*x**5 + 0.0002*x**6 +0.5*(x-2)**2 np.random.seed(0) # 30 equally spaced sample locations X = np.linspace(0.05, 4.95, 30)[:,None] np.random.shuffle(X) # y = f(X) + epsilon Y = f(X) + np.random.normal(0., 0.1, (30,1)) # note that np.random.normal takes mean and s.d. (not variance), 0.1^2 = 0.01 train_X = X[:10] train_Y = Y[:10] test_X = X[10:] test_Y = Y[10:] # Plot observations plt.plot(train_X, train_Y, &quot;kx&quot;, mew=2, label=&#39;Train points&#39;) plt.plot(test_X, test_Y, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) # Annotate plot plt.xlabel(&quot;x&quot;), plt.ylabel(&quot;f&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f52398e4a90&gt; . Fitting the above data usigng GPR with RBF kernel by varying the length scale (Noiseless case) . Here we assume that observations (train instances) are noise free. Thus, the GP fit must pass exactly through the train points. . fig, ax = plt.subplots() ls = [0.05, 0.25, 0.5, 1., 2., 4.] from sklearn.metrics import mean_absolute_error def update(iteration): ax.cla() k = GPy.kern.RBF(1) k.lengthscale = ls[iteration] m = GPy.models.GPRegression(train_X, train_Y, k) m.Gaussian_noise = 0.0 m.plot(ax=ax) ax.plot(test_X, test_Y, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) ax.legend() ax.set_title(&quot;Length scale = %s, MAE = %s&quot; %(k.lengthscale[0], mean_absolute_error(test_Y, m.predict_noiseless(test_X)[0].flatten()))); fig.tight_layout() num_iterations = len(ls) anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations, 1), interval=500) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect We can see that increasing the length scale makes the fit smoother. . Fitting the above data usigng GPR with RBF kernel by varying the length scale (Noisy case) . Here we assume that observations (train instances) have noise. Thus, the GP fit may not pass exactly through the train points. . fig, ax = plt.subplots() ls = [0.05, 0.25, 0.5, 1., 2., 4.] from sklearn.metrics import mean_absolute_error def update(iteration): ax.cla() k = GPy.kern.RBF(1) k.lengthscale = ls[iteration] m = GPy.models.GPRegression(train_X, train_Y, k) m.plot(ax=ax) ax.plot(test_X, test_Y, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) ax.legend() ax.set_title(&quot;Length scale = %s, MAE = %s&quot; %(k.lengthscale[0], mean_absolute_error(test_Y, m.predict_noiseless(test_X)[0].flatten()))); fig.tight_layout() num_iterations = len(ls) anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations, 1), interval=500) plt.close() rc(&#39;animation&#39;, html=&#39;jshtml&#39;) anim . &lt;/input&gt; Once Loop Reflect Optimizing kernel parameters . Thus far we had been hard coding the kernel parameters. Could we learn these? Yes, we can learn these by applying MLE. This might sound a little weird -- learning prior parameters from data in the Bayesian setting?! . k = GPy.kern.RBF(1) m = GPy.models.GPRegression(train_X, train_Y, k) m.optimize() . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f5239861390&gt; . m . Model: GP regression Objective: 6.219091605935309 Number of Parameters: 3 Number of Optimization Parameters: 3 Updates: True . GP_regression. valueconstraintspriors . &lt;td class=tg-left&gt; rbf.variance &lt;/td&gt;&lt;td class=tg-right&gt; 12.433073229232102&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; rbf.lengthscale &lt;/td&gt;&lt;td class=tg-right&gt; 2.34663339745307&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; Gaussian_noise.variance&lt;/td&gt;&lt;td class=tg-right&gt;0.017278356392144867&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . m.plot() ax = plt.gca() ax.plot(test_X, test_Y, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f5238b39850&gt; . mean_absolute_error(test_Y, m.predict(test_X)[0].flatten()) . 0.1261412756644803 . Above, we see the fit for the learnt kernel parameters. . Other kernels . We have thus far discussed RBF kernel. Let us now have a quck look at Periodic kernel before we look at combining differnt kernels. . k = GPy.kern.StdPeriodic(1, period=2) data = np.linspace(-4.,4.,100)[:, None] C = k.K(data, np.array([[0.]])) plt.plot(data,C) . [&lt;matplotlib.lines.Line2D at 0x7f5238aa7090&gt;] . The periodic kernel does what you might expect -- there is a periodic nature of relation between different points. . Let us now try to fit the periodic kernel to our previously generated data. . m = GPy.models.GPRegression(train_X, train_Y, k) m.optimize() . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f5238adb250&gt; . m.plot() ax = plt.gca() ax.plot(test_X, test_Y, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f5238a2f790&gt; . As we might have expected, the fit is not very good. Let us create a data set having periodicity where our periodic kernel would be of good value. . # lambda function, call f(x) to generate data g = lambda x: np.sin(6*x) + 0.5*np.cos(x) np.random.seed(0) # 30 equally spaced sample locations X2 = np.linspace(0.05, 4.95, 30)[:,None] np.random.shuffle(X2) # y = f(X) + epsilon Y2 = g(X2) + np.random.normal(0., 0.1, (30,1)) # note that np.random.normal takes mean and s.d. (not variance), 0.1^2 = 0.01 train_X2 = X2[:10] train_Y2 = Y2[:10] test_X2 = X2[10:] test_Y2 = Y2[10:] # Plot observations plt.plot(train_X2, train_Y2, &quot;kx&quot;, mew=2, label=&#39;Train points&#39;) plt.plot(test_X2, test_Y2, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) # Annotate plot plt.xlabel(&quot;x&quot;), plt.ylabel(&quot;f&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f52389fd550&gt; . m = GPy.models.GPRegression(train_X2, train_Y2, k) m.optimize() . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f5238c88a50&gt; . m . Model: GP regression Objective: 6.688944496233991 Number of Parameters: 4 Number of Optimization Parameters: 4 Updates: True . GP_regression. valueconstraintspriors . &lt;td class=tg-left&gt; std_periodic.variance &lt;/td&gt;&lt;td class=tg-right&gt; 1.6241936960384638&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; std_periodic.period &lt;/td&gt;&lt;td class=tg-right&gt; 2.0498165862939817&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; std_periodic.lengthscale&lt;/td&gt;&lt;td class=tg-right&gt; 0.5405924861920798&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . &lt;td class=tg-left&gt; Gaussian_noise.variance &lt;/td&gt;&lt;td class=tg-right&gt;0.026138350160739007&lt;/td&gt;&lt;td class=tg-center&gt; +ve &lt;/td&gt;&lt;td class=tg-center&gt; &lt;/td&gt; . m.plot() ax = plt.gca() ax.plot(test_X2, test_Y2, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f52389f31d0&gt; . From the plot above, we can see that just using the periodic kernel is not very useful. Maybe we need to combine the RBF kernel with the periodic kernel? We will be trying two combinations: adding the two kernels and multiplying the two kernels to obtain two new kernels. . k1 = GPy.kern.StdPeriodic(1, period=2) k2 = GPy.kern.RBF(1, lengthscale=1) k_combined_1 = k1+k2 k_combined_2 = k1*k2 . Let us now try to visualise the two kernels. . C1 = k_combined_1.K(data, np.array([[0.]])) C2 = k_combined_2.K(data, np.array([[0.]])) plt.plot(data,k1.K(data, np.array([[0.]])),label=&quot;Periodic&quot;) plt.plot(data,k2.K(data, np.array([[0.]])),label=&quot;RBF&quot;) plt.plot(data,C1,label=&quot;Periodic+RBF&quot;) plt.plot(data,C2,label=&quot;Periodic*RBF&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f5238900650&gt; . We can also visualise the covariance matrices corresponding to the two new kernels we have created. . cov = k_combined_1.K(data,data) # Plot the covariance of the sample space plt.pcolor(data.T, data, cov) # Format and annotate plot plt.gca().invert_yaxis(), plt.gca().axis(&quot;image&quot;) plt.xlabel(&quot;x&quot;), plt.ylabel(&quot;x&#39;&quot;), plt.colorbar() plt.title(&quot;RBF+Periodic&quot;); . cov = k_combined_2.K(data,data) # Plot the covariance of the sample space plt.pcolor(data.T, data, cov) # Format and annotate plot plt.gca().invert_yaxis(), plt.gca().axis(&quot;image&quot;) plt.xlabel(&quot;x&quot;), plt.ylabel(&quot;x&#39;&quot;), plt.colorbar() plt.title(&quot;RBF*Periodic&quot;); . m = GPy.models.GPRegression(train_X2, train_Y2, k_combined_1) m.optimize() m.plot() ax = plt.gca() ax.plot(test_X2, test_Y2, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f521cb662d0&gt; . m = GPy.models.GPRegression(train_X2, train_Y2, k_combined_2) m.optimize() m.plot() print(m) ax = plt.gca() ax.plot(test_X2, test_Y2, &quot;mo&quot;, mew=2, label=&#39;Test points&#39;) ax.legend() . Name : GP regression Objective : 4.136252889408651 Number of Parameters : 6 Number of Optimization Parameters : 6 Updates : True Parameters: GP_regression. | value | constraints | priors mul.std_periodic.variance | 1.7461089633948401 | +ve | mul.std_periodic.period | 2.041745856285753 | +ve | mul.std_periodic.lengthscale | 0.6637866448282967 | +ve | mul.rbf.variance | 1.7461089633948303 | +ve | mul.rbf.lengthscale | 24.98629931665634 | +ve | Gaussian_noise.variance | 4.955970365230769e-16 | +ve | . &lt;matplotlib.legend.Legend at 0x7f521d1de910&gt; . From the above two visualisations, we can see the two kernels in action. . 2D GP . Having studied GPR for 1 dimension, we will now be looking at GPR for 2d data. Let us first create some dataset. . X = np.array([[3, 2], [1, 4], [1, 1], [3, 4], [2,2], [2, 3], [3, 1], [3, 3.5], [2.5, 3.5]]) y = np.array([1, 1, 3, 2, 5.5, 4.5, 0.5, 3, 3.5]) . X.shape, y.shape . ((9, 2), (9,)) . plt.scatter(X[:,0], X[:, 1],s=y*50) . &lt;matplotlib.collections.PathCollection at 0x7f52388f7d10&gt; . The above visualisation shows the dataset where the size of the marker denotes the value at that location. We will now be fitting a RBF kernel (expecting a 2d input) to this data set. . k_2d = GPy.kern.RBF(input_dim=2, lengthscale=1) . X.shape . (9, 2) . m = GPy.models.GPRegression(X, y.reshape(-1, 1), k_2d) m.optimize() . &lt;paramz.optimization.optimization.opt_lbfgsb at 0x7f521d185fd0&gt; . Generating predictions over the entire grid for visualisation. . x_t = np.linspace(0, 4.5, 40) y_t = np.linspace(0, 4.5, 40) XX, YY = np.meshgrid(x_t, y_t) . Z_pred = np.zeros_like(YY) for i in range(40): for j in range(40): Z_pred[i, j] = m.predict(np.array([XX[i, j], YY[i, j]]).reshape(1, 2))[0] . Z_pred.shape . (40, 40) . plt.contourf(XX, YY, Z_pred, levels=30) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7f5238833b50&gt; . The above plot shows the prediction in the 2d space. We can alternatively view the 3d surface plot. . ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(XX, YY, Z_pred, rstride=1, cstride=1, cmap=&#39;viridis&#39;, edgecolor=&#39;none&#39;) ax.set_title(&#39;surface&#39;); ax.view_init(30, 35) . Z_var = np.zeros_like(YY) for i in range(40): for j in range(40): Z_var[i, j] = m.predict(np.array([XX[i, j], YY[i, j]]).reshape(1, 2))[1] . plt.contourf(XX, YY, Z_var, levels=30) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7f521d538e10&gt; . We can above see the variance plot. . Air quality 2d map . Now, we will be using GPs for predicting air quality in New Delhi. See my previous post on how to get AQ data for Delhi.https://nipunbatra.github.io/blog/air%20quality/2018/06/21/aq-india-map.html . I will be creating a function to visualise the AQ estimations using GPs based on different kernels. . The shapefile for Delhi can be downloaded from here. . import pandas as pd import os df = pd.read_csv(os.path.expanduser(&quot;~/Downloads/2018-04-06.csv&quot;)) df = df[(df.country==&#39;IN&#39;)&amp;(df.city==&#39;Delhi&#39;)&amp;(df.parameter==&#39;pm25&#39;)].dropna().groupby(&quot;location&quot;).mean() . df . value latitude longitude . location . Burari Crossing, New Delhi - IMD | 245.583333 | 28.725650 | 77.201157 | . CRRI Mathura Road, New Delhi - IMD | 265.666667 | 28.551200 | 77.273574 | . DTU, New Delhi - CPCB | 214.333333 | 28.750050 | 77.111261 | . IGI Airport Terminal - 3, New Delhi - IMD | 130.666667 | 28.562776 | 77.118005 | . IHBAS, Dilshad Garden,New Delhi - CPCB | 212.583333 | 28.680275 | 77.201157 | . ITO, New Delhi - CPCB | 220.500000 | 28.631694 | 77.249439 | . Lodhi Road, New Delhi - IMD | 176.083333 | 28.591825 | 77.227307 | . Mandir Marg, New Delhi - DPCC | 82.000000 | 28.637269 | 77.200560 | . NSIT Dwarka, New Delhi - CPCB | 184.583333 | 28.609090 | 77.032541 | . North Campus, DU, New Delhi - IMD | 147.833333 | 28.657381 | 77.158545 | . Pusa, New Delhi - IMD | 112.000000 | 28.610304 | 77.099694 | . R K Puram, New Delhi - DPCC | 103.600000 | 28.564610 | 77.167010 | . Shadipur, New Delhi - CPCB | 213.833333 | 28.651478 | 77.147311 | . Sirifort, New Delhi - CPCB | 222.250000 | 28.550425 | 77.215938 | . US Diplomatic Post: New Delhi | 46.625000 | 28.635760 | 77.224450 | . import geopandas gdf = geopandas.GeoDataFrame( df, geometry=geopandas.points_from_xy(df.longitude, df.latitude)) . gdf.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5207d29110&gt; . def plot_air_vis(df, k, shp, title): m = GPy.models.GPRegression(df[[&#39;longitude&#39;,&#39;latitude&#39;]], df[[&#39;value&#39;]], k) m.optimize(max_iters=2000) y_t = np.linspace(28.38,28.9, 40) x_t = np.linspace(76.82, 77.4, 40) XX, YY = np.meshgrid(x_t, y_t) Z_pred = np.zeros_like(YY) Z_var = np.zeros_like(YY) for i in range(40): for j in range(40): Z_pred[i, j], Z_var[i, j] = m.predict_noiseless(np.array([XX[i, j], YY[i, j]]).reshape(1, 2)) data = geopandas.read_file(fp) fig = plt.figure(figsize=(6, 6)) plt.contourf(XX, YY, Z_pred, levels=30,alpha=0.6,cmap=&#39;Purples&#39;) plt.colorbar() gdf.plot(ax=plt.gca(),markersize=gdf[&#39;value&#39;],color=&#39;k&#39;) data.plot(color=&#39;k&#39;,ax=plt.gca(),zorder=-1,alpha=0.4) plt.gca().set_aspect(&quot;equal&quot;) for a in [100, 150, 200,250]: plt.scatter([], [], c=&#39;k&#39;, alpha=1, s=a, label=str(a) + &#39;$ mu g/m^3$&#39;) plt.legend(scatterpoints=1, frameon=True, labelspacing=1, loc=&#39;upper left&#39;,ncol=2) plt.title(title+&quot; t&quot;+str(m.objective_function())) . k_2d = GPy.kern.RBF(input_dim=2, lengthscale=1) k_2d_rbf_2 = GPy.kern.RBF(input_dim=2, lengthscale=3)*k_2d k_2d_rbf_3 = GPy.kern.RBF(input_dim=2, lengthscale=3) + k_2d_rbf_2 k_matern32 = GPy.kern.Matern32(input_dim=2) k_matern52 = GPy.kern.Matern52(input_dim=2) k_rbf_matern = k_matern32 * k_matern52 + k_matern32*k_2d_rbf_3 fp=os.path.expanduser(&quot;~/Downloads/wards delimited.shp&quot;) . plot_air_vis(df, k_2d, fp,&quot;RBF&quot;) plot_air_vis(df, k_matern32, fp,&quot;Matern32&quot;) plot_air_vis(df, k_matern52, fp,&quot;matern52&quot;) plot_air_vis(df, k_2d_rbf_2, fp,&quot;RBF*RBF&quot;) plot_air_vis(df, k_2d_rbf_3, fp,&quot;RBF*RBF+RBF&quot;) plot_air_vis(df, k_rbf_matern, fp,&quot;Matern32*Matern52+Matern32*RBF&quot;) . There you go. Till next time! .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/03/26/gp.html",
            "relUrl": "/ml/2020/03/26/gp.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Some Neural Network Classification",
            "content": "from sklearn.datasets import make_moons import matplotlib.pyplot as plt import numpy as np %matplotlib inline . X, y = make_moons() . plt.scatter(X[:, 0], X[:, 1], c= y) . &lt;matplotlib.collections.PathCollection at 0x126d7af50&gt; . from keras.models import Sequential from sklearn.metrics import accuracy_score import os . from keras.layers import Dense, Activation from keras.utils import to_categorical model_simple = Sequential([ Dense(1, input_shape=(2,)), Activation(&#39;relu&#39;), Dense(2), Activation(&#39;softmax&#39;), ]) model_complex = Sequential([ Dense(6, input_shape=(2,)), Activation(&#39;relu&#39;), Dense(4), Activation(&#39;relu&#39;), Dense(3), Activation(&#39;relu&#39;), Dense(2), Activation(&#39;softmax&#39;), ]) model_complex_2 = Sequential([ Dense(10, input_shape=(2,)), Activation(&#39;relu&#39;), Dense(8, ), Activation(&#39;relu&#39;), Dense(8), Activation(&#39;relu&#39;), Dense(2), Activation(&#39;softmax&#39;), ]) . model_simple.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model_complex.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model_complex_2.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . def make_plot(X, y, model, dataset, model_type, noise, n_iter=80,cmap=&#39;PRGn&#39;): h=200 if dataset==&quot;moon&quot;: X, y = make_moons(noise=noise) if dataset==&quot;iris&quot;: X, y = load_iris()[&#39;data&#39;][:, :2], load_iris()[&#39;target&#39;] print(X.shape, y.shape) y_binary = to_categorical(y) xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-0.2, X[:, 0].max()+0.2, h), np.linspace(X[:, 1].min()-0.2, X[:, 1].max()+0.2, h)) XX = np.c_[xx.ravel(), yy.ravel()] for i in range(n_iter): model.fit(X, y_binary, epochs=1, verbose=0) Z = np.argmax(model.predict(XX), axis=1).reshape(xx.shape) y_hat = np.argmax(model.predict(X), axis=1) train_accuracy = accuracy_score(y, y_hat) contours = plt.contourf(xx, yy, Z, h , cmap=cmap, alpha=0.4) plt.title(&quot;Iteration: &quot;+str(i)+&quot; n Accuracy:&quot;+str(train_accuracy)) plt.colorbar() plt.scatter(X[:, 0], X[:, 1], c= y, cmap=cmap) if not os.path.exists(f&quot;/Users/nipun/Desktop/animation-keras/{dataset}/{model_type}/{noise}/&quot;): os.makedirs(f&quot;/Users/nipun/Desktop/animation-keras/{dataset}/{model_type}/{noise}/&quot;) plt.savefig(f&quot;/Users/nipun/Desktop/animation-keras/{dataset}/{model_type}/{noise}/{i:03}.png&quot;) plt.clf() . make_plot(X, y, model_simple, &quot;moon&quot;, &quot;simple&quot;, None) . (100, 2) (100,) . &lt;Figure size 432x288 with 0 Axes&gt; . !convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/moon/simple/None/*.png moon-simple-none.gif . . make_plot(X, y, model_complex, &quot;moon&quot;, &quot;complex&quot;, None, 500) . (100, 2) (100,) . &lt;Figure size 432x288 with 0 Axes&gt; . !convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/moon/complex/None/*.png moon-complex-none.gif . . make_plot(X, y, model_complex_2, &quot;moon&quot;, &quot;complex&quot;, 0.3, 700) . (100, 2) (100,) . &lt;Figure size 432x288 with 0 Axes&gt; . !convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/moon/complex/0.3/*.png moon-complex-03.gif . . model_simple_2 = Sequential([ Dense(1, input_shape=(2,)), Activation(&#39;relu&#39;), Dense(3), Activation(&#39;softmax&#39;), ]) model_simple_2.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . make_plot(X, y, model_simple_2, &quot;iris&quot;, &quot;simple&quot;, None, 500) . (150, 2) (150,) . &lt;Figure size 432x288 with 0 Axes&gt; . !convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/iris/simple/None/*.png iris-simple.gif . . model_complex_iris = Sequential([ Dense(12, input_shape=(2,)), Activation(&#39;relu&#39;), Dense(6), Activation(&#39;relu&#39;), Dense(4), Activation(&#39;relu&#39;), Dense(3), Activation(&#39;softmax&#39;), ]) model_complex_iris.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . make_plot(X, y, model_complex_iris, &quot;iris&quot;, &quot;complex&quot;, None, 500) . (150, 2) (150,) . &lt;Figure size 432x288 with 0 Axes&gt; . !convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/iris/complex/None/*.png iris-complex.gif . .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/03/08/keras-neural-non-linear.html",
            "relUrl": "/ml/2020/03/08/keras-neural-non-linear.html",
            "date": " • Mar 8, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Neural Networks from scratch",
            "content": "import numpy as np import matplotlib.pyplot as plt %matplotlib inline . def relu(x): return np.max(0, x) . X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) y = np.array([0, 1, 1, 0]) . plt.scatter(X[:, 0], X[:, 1],c=y) . &lt;matplotlib.collections.PathCollection at 0x120334d50&gt; . X.shape . (4, 2) . X . array([[0, 0], [0, 1], [1, 0], [1, 1]]) . layers = [2, 1] . B = [np.zeros(n) for n in layers] . W = [None]*len(layers) . W[0] = np.zeros((X.shape[1], layers[0])) . W . [array([[0., 0.], [0., 0.]]), None] . for i in range(1, len(layers)): W[i] = np.zeros((layers[i-1], layers[i])) . W[1] . array([[0.], [0.]]) . X.shape, W[0].shape . ((4, 2), (2, 2)) . X.shape . (4, 2) .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/03/02/linear-scratch.html",
            "relUrl": "/ml/2020/03/02/linear-scratch.html",
            "date": " • Mar 2, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Learning neural network for XOR",
            "content": "import autograd.numpy as np import matplotlib.pyplot as plt %matplotlib inline np.numpy_boxes.ArrayBox.__repr__ = lambda self: str(self._value) . X = np.array([[0, 0], [0, 1], [1, 0], [1, 1] ]) y = np.array([[0], [1], [1], [0]]) . X.shape, y.shape . ((4, 2), (4, 1)) . N, N_0 = X.shape N, N_2 = y.shape N_1 = 2 . W = [np.array([0]), np.array([[1, 1], [1, 1]]), np.array([[1, -2]])] b = [np.array([0]), np.array([[0], [-1]]), np.array([[0]])] B = [] . A = [X] A.extend([None]*(len(W)-1)) Z = [None]*(len(W)) . def relu(z): temp = z.copy() temp[temp&lt;0] = 0 return temp def sigmoid(z): return 1./(1+np.exp(-z)) . for i in range(1, len(W)): Z[i] = A[i-1]@(W[i].T) + b[i].T A[i] =relu(Z[i]) . A[2]==y . array([[ True], [ True], [ True], [ True]]) . Excellent, now let us start from random weight initialisations and use backprop to come to our result . shapes = [X.shape[1], 2, 1] activations = [&#39;empty&#39;,&#39;sigmoid&#39;,&#39;sigmoid&#39;] activation_func = {&#39;sigmoid&#39;:sigmoid, &#39;relu&#39;:relu} W = [None]*(len(shapes)) b = [None]*(len(shapes)) np.random.seed(0) # Dummy W[0] = np.array([0]) b[0] = np.array([0]) for i in range(1, len(shapes)): W[i] = np.random.randn(shapes[i], shapes[i-1]) b[i] = np.random.randn(shapes[i], 1) Z = [None]*(len(W)) Z[0] = np.array([0]) A = [X] A.extend([None]*(len(W)-1)) . def make_plot(iteration, loss, W, b, cmap=&#39;PRGn&#39;,close=True): h = 100 xx, yy = np.meshgrid(np.linspace(-0.1, 1.1, h), np.linspace(-0.1, 1.1, h)) XX = np.c_[xx.ravel(), yy.ravel()] A = [XX] A.extend([None]*(len(W)-1)) Z = [None]*(len(W)) for i in range(1, len(W)): Z[i] = A[i-1]@(W[i].T) + b[i].T A[i] =sigmoid(Z[i]) pred= A[2].reshape(xx.shape) pred[pred&gt;0.5] = 1 pred[pred&lt;=0.5] = 0 contours = plt.contourf(xx, yy, pred, h , cmap=cmap, alpha=0.2) plt.colorbar() plt.title(f&quot;Iteration: {iteration} n Loss: {loss}&quot;) plt.scatter(X[:, 0], X[:, 1], c= y.flatten(), cmap=cmap, s=200) plt.savefig(f&quot;/home/nipunbatra-pc/Desktop/xor/{iteration:04}.png&quot;) if close: plt.clf() . make_plot(0, 2.9, W, b, close=False) . def objective(W, b): for i in range(1, len(W)): Z[i] = A[i-1]@(W[i].T) + b[i].T A[i] = activation_func[activations[i]](Z[i]) y_hat = A[2] loss = (-y.T@np.log(y_hat) - (1-y).T@np.log(1-y_hat)).squeeze() return loss . objective(W, b) . array(2.9991465) . from autograd import elementwise_grad as egrad from autograd import grad . grad_objective = grad(objective, argnum=[0, 1]) . (del_W0_auto, del_W1_auto, del_W2_auto), (del_b0_auto, del_b1_auto, del_b2_auto) = grad_objective(W, b) . del_W2_auto . array([[0.60353799, 0.35399637]]) . del_W2_ours = (A[2]-y).T@A[1] . del_W2_ours,del_W2_auto . ([[0.60353799 0.35399637]], array([[0.60353799, 0.35399637]])) . del_b2_ours = (A[2]-y).sum(axis=0).reshape(-1, 1) . del_b2_ours,del_b2_auto . ([[0.6632421]], array([[0.6632421]])) . del_A1_ours = (A[2]-y)@W[2] del_Z1_ours = np.multiply(del_A1_ours, sigmoid(Z[1])*(1-sigmoid(Z[1]))) del_W1_ours = del_Z1_ours.T@A[0] np.allclose(del_W1_ours, del_W1_auto) . True . del_b1_ours = (del_Z1_ours.sum(axis=0)).reshape(-1, 1) np.allclose(del_b1_ours, del_b1_auto) . True . epochs = 140 alpha =1 losses = np.zeros(epochs) print_every = 20 W = [None]*(len(shapes)) b = [None]*(len(shapes)) np.random.seed(0) # Dummy W[0] = np.array([0]) b[0] = np.array([0]) for i in range(1, len(shapes)): W[i] = np.random.randn(shapes[i], shapes[i-1]) b[i] = np.random.randn(shapes[i], 1) Z = [None]*(len(W)) Z[0] = np.array([0]) A = [X] A.extend([None]*(len(W)-1)) del_Z = [None]*(len(W)+1) del_A = [None]*(len(W)+1) del_W = [None]*(len(W)) del_b = [None]*(len(W)) for iteration in range(epochs): for i in range(1, len(W)): Z[i] = A[i-1]@(W[i].T) + b[i].T A[i] = activation_func[activations[i]](Z[i]) y_hat = A[2] loss = (-y.T@np.log(y_hat) - (1-y).T@np.log(1-y_hat)).squeeze() losses[iteration] = loss if iteration%print_every==0: print(iteration, loss) make_plot(iteration, loss, W, b, close=True) del_A[2] = -np.multiply(y, A[2]) + np.multiply((1-y), (1-A[2])) del_Z[2] = A[2]-y del_W[2] = (A[2]-y).T@A[1] del_b[2] = (del_Z[2].sum(axis=0)).reshape(-1, 1) del_A[1] = del_Z[2]@W[2] del_Z[1] = np.multiply(del_A[1], sigmoid(Z[1])*(1-sigmoid(Z[1]))) del_W[1] = del_Z[1].T@A[0] del_b[1] = (del_Z[1].sum(axis=0)).reshape(-1, 1) for i in range(1, len(shapes)): W[i] = W[i] - alpha*del_W[i] b[i] = b[i] - alpha*del_b[i] . 0 2.9991464995409807 20 2.850067543754094 40 2.5045921819726082 60 1.5756597251036364 80 0.5779054501565161 100 0.3097308274202594 120 0.2028529568023768 . &lt;Figure size 432x288 with 0 Axes&gt; . make_plot(iteration, loss, W, b, close=False) . make_plot(0, 2.9, W, b, close=False) . array([[0], [1], [1], [0]]) . !convert -delay 20 -loop 0 /home/nipunbatra-pc/Desktop/xor/*.png xor-own.gif . .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/02/28/xor-relu-vector.html",
            "relUrl": "/ml/2020/02/28/xor-relu-vector.html",
            "date": " • Feb 28, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://nipunbatra.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Bayesian Linear Regression",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . x = np.linspace(-1, 1, 50).reshape(-1, 1) . y = 5*x + 4 noise = (np.abs(x.flatten())*np.random.randn(len(x))).reshape(-1,1) y = y + noise . plt.scatter(x, y) plt.plot(x, 5*x + 4, &#39;k&#39;) . [&lt;matplotlib.lines.Line2D at 0x115c28cd0&gt;] . from scipy.stats import multivariate_normal from matplotlib import cm cov = np.array([[ 1 , 0], [0, 1]]) var = multivariate_normal(mean=[0,0], cov=cov) x_grid, y_grid = np.mgrid[-1:1:.01, -1:1:.01] pos = np.dstack((x_grid, y_grid)) z = var.pdf(pos) plt.contourf(x_grid, y_grid, z) plt.gca().set_aspect(&#39;equal&#39;) plt.xlabel(r&quot;$ theta_0$&quot;) plt.ylabel(r&quot;$ theta_1$&quot;) plt.title(r&quot;Prior distribution of $ theta = f( mu, Sigma)$&quot;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x1a18423950&gt; . $$ prod_{i=1}^{n} frac{1}{ sqrt{2 pi sigma^{2}}} e^{- frac{(y_{i}- hat{y}_{i})^{2}}{2 sigma^{2}}} $$ Sample from prior . n_samples = 20 for n in range(n_samples): theta_0_s, theta_1_s = var.rvs() plt.plot(x, theta_1_s*x + theta_0_s, color=&#39;k&#39;,alpha=0.2) plt.scatter(x, y) . &lt;matplotlib.collections.PathCollection at 0x1a18598fd0&gt; . Likelihood of theta . def likelihood(theta_0, theta_1, x, y, sigma): s = 0 x_plus_1 = np.hstack((np.ones_like(x), x)) for i in range(len(x)): y_i_hat = x_plus_1[i, :]@np.array([theta_0, theta_1]) s += (y[i,:]-y_i_hat)**2 return np.exp(-s/(2*sigma*sigma))/np.sqrt(2*np.pi*sigma*sigma) . likelihood(-1, 1, x, y, 4) . array([1.00683395e-22]) . x_grid_2, y_grid_2 = np.mgrid[0:8:.1, 0:8:.1] li = np.zeros_like(x_grid_2) for i in range(x_grid_2.shape[0]): for j in range(x_grid_2.shape[1]): li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j], x, y, 4) . plt.contourf(x_grid_2, y_grid_2, li) plt.gca().set_aspect(&#39;equal&#39;) plt.xlabel(r&quot;$ theta_0$&quot;) plt.ylabel(r&quot;$ theta_1$&quot;) plt.colorbar() plt.scatter(4, 5, s=200, marker=&#39;*&#39;, color=&#39;r&#39;) plt.title(r&quot;Likelihood as a function of ($ theta_0, theta_1$)&quot;) . Text(0.5, 1.0, &#39;Likelihood as a function of ($ theta_0, theta_1$)&#39;) . Likelihood of $ sigma^2$ . x_plus_1 = np.hstack((np.ones_like(x), x)) theta_mle = np.linalg.inv(x_plus_1.T@x_plus_1)@(x_plus_1.T@y) sigma_2_mle = np.linalg.norm(y - x_plus_1@theta_mle)**2 sigma_mle = np.sqrt(sigma_2_mle) sigma_mle . 4.128685902124939 . Posterior . $$ begin{aligned} p( boldsymbol{ theta} | mathcal{X}, mathcal{Y}) &amp;= mathcal{N} left( boldsymbol{ theta} | boldsymbol{m}_{N}, boldsymbol{S}_{N} right) boldsymbol{S}_{N} &amp;= left( boldsymbol{S}_{0}^{-1}+ sigma^{-2} boldsymbol{ Phi}^{ top} boldsymbol{ Phi} right)^{-1} boldsymbol{m}_{N} &amp;= boldsymbol{S}_{N} left( boldsymbol{S}_{0}^{-1} boldsymbol{m}_{0}+ sigma^{-2} boldsymbol{ Phi}^{ top} boldsymbol{y} right) end{aligned} $$ S0 = np.array([[ 1 , 0], [0, 1]]) M0 = np.array([0, 0]) SN = np.linalg.inv(np.linalg.inv(S0) + (sigma_mle**-2)*x_plus_1.T@x_plus_1) MN = SN@(np.linalg.inv(S0)@M0 + (sigma_mle**-2)*(x_plus_1.T@y).squeeze()) . MN, SN . (array([2.97803341, 2.54277597]), array([[2.54243881e-01, 2.97285330e-17], [2.97285330e-17, 4.95625685e-01]])) . from scipy.stats import multivariate_normal from matplotlib import cm cov = np.array([[ 1 , 0], [0, 1]]) var_pos = multivariate_normal(mean=MN, cov=SN) x_grid, y_grid = np.mgrid[0:8:.1, 0:8:.1] pos = np.dstack((x_grid, y_grid)) z = var_pos.pdf(pos) plt.contourf(x_grid, y_grid, z) plt.gca().set_aspect(&#39;equal&#39;) plt.xlabel(r&quot;$ theta_0$&quot;) plt.ylabel(r&quot;$ theta_1$&quot;) plt.title(r&quot;Posterior distribution of $ theta = f( mu, Sigma)$&quot;) plt.scatter(4, 5, s=200, marker=&#39;*&#39;, color=&#39;r&#39;, label=&#39;MLE&#39;) plt.scatter(MN[0], MN[1], s=100, marker=&#39;^&#39;, color=&#39;black&#39;, label=&#39;MAP&#39;) plt.colorbar() plt.legend() plt.savefig(&quot;../images/blr-map.png&quot;) . Sample from posterior . n_samples = 20 for n in range(n_samples): theta_0_s, theta_1_s = var_pos.rvs() plt.plot(x, theta_1_s*x + theta_0_s, color=&#39;k&#39;,alpha=0.2) plt.scatter(x, y) . &lt;matplotlib.collections.PathCollection at 0x1a18e7dd10&gt; . Posterior predictions . $$ begin{aligned} p left(y_{*} | mathcal{X}, mathcal{Y}, boldsymbol{x}_{*} right) &amp;= int p left(y_{*} | boldsymbol{x}_{*}, boldsymbol{ theta} right) p( boldsymbol{ theta} | mathcal{X}, mathcal{Y}) mathrm{d} boldsymbol{ theta} &amp;= int mathcal{N} left(y_{*} | boldsymbol{ phi}^{ top} left( boldsymbol{x}_{*} right) boldsymbol{ theta}, sigma^{2} right) mathcal{N} left( boldsymbol{ theta} | boldsymbol{m}_{N}, boldsymbol{S}_{N} right) mathrm{d} boldsymbol{ theta} &amp;= mathcal{N} left(y_{*} | boldsymbol{ phi}^{ top} left( boldsymbol{x}_{*} right) boldsymbol{m}_{N}, boldsymbol{ phi}^{ top} left( boldsymbol{x}_{*} right) boldsymbol{S}_{N} boldsymbol{ phi} left( boldsymbol{x}_{*} right)+ sigma^{2} right) end{aligned} $$ For a point $x*$ . Predictive mean = $X^Tm_N$ . Predictive variance = $X^TS_NX + sigma^2$ . x_plus_1.T.shape, SN.shape, x_plus_1.shape . ((2, 50), (2, 2), (50, 2)) . pred_var = x_plus_1@SN@x_plus_1.T pred_var.shape . (50, 50) . ## Marginal individual_var = pred_var.diagonal() . y_hat_map = x_plus_1@MN plt.plot(x, y_hat_map, color=&#39;black&#39;) plt.fill_between(x.flatten(), y_hat_map-individual_var, y_hat_map+individual_var, alpha=0.2, color=&#39;black&#39;) plt.scatter(x, y) . &lt;matplotlib.collections.PathCollection at 0x1a1881e450&gt; .",
            "url": "https://nipunbatra.github.io/blog/ml/2020/02/20/bayesian-linear-regression.html",
            "relUrl": "/ml/2020/02/20/bayesian-linear-regression.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nipunbatra.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Gaussian Processes",
            "content": "An example . . Let us look at the GIF above. It shows a non-linear fit with uncertainty on a set of points in the 2d space. The uncertainty is shown by the gray shadowed region. The animation shows how the fit and the uncertainty varies as we keep adding more points (shown as big circles). As expected, as more points are added, the uncertainty of the fit in the vicinity of the added points reduces. This is an example of Gaussian Processes (GP) regression in play. . Introduction . There exist some great online resources for Gaussian Processes (GPs) including an excellent recent Distill.Pub article. This blog post is an attempt with a programatic flavour. In this notebook, we will build the intuition and learn some basics of GPs. This notebook is heavily inspired by the awesome tutorial by Richard Turner. Here is the link to the slides and video. Lectures videos and notes from Nando De Freitas&#39; course are an amazing resource for GPs (and anything ML!). . Some imports . import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&#39;ignore&#39;) %matplotlib inline . A function to make the Matplotlib plots prettier . SPINE_COLOR = &#39;gray&#39; def format_axes(ax): for spine in [&#39;top&#39;, &#39;right&#39;]: ax.spines[spine].set_visible(False) for spine in [&#39;left&#39;, &#39;bottom&#39;]: ax.spines[spine].set_color(SPINE_COLOR) ax.spines[spine].set_linewidth(0.5) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.yaxis.set_ticks_position(&#39;left&#39;) for axis in [ax.xaxis, ax.yaxis]: axis.set_tick_params(direction=&#39;out&#39;, color=SPINE_COLOR) return ax . One dimensional Gaussian/Normal . We will start the discussion with 1d Gaussians. Let us write some simple code to generate/sample data from $ mathcal{N}( mu=0, sigma=1)$ . one_dim_normal_data = np.random.normal(0, 1, size=10000) . Let us now visualise the data in a 1d space using scatter plot . plt.scatter(one_dim_normal_data, np.zeros_like(one_dim_normal_data), alpha=0.2, c=&#39;gray&#39;, edgecolors=&#39;k&#39;, marker=&#39;o&#39;) format_axes(plt.gca()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d08faed90&gt; . As we would expect, there are a lot of samples close to zero (mean) and as we go further away from zero, the number of samples keeps reducing. We can also visualise the same phenomenon using a normed histogram shown below. . plt.hist(one_dim_normal_data, density=True, bins=20, color=&#39;gray&#39;) format_axes(plt.gca()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d090a4a60&gt; . We can notice that there is a high probability of drawing samples close to the mean and the probability is low far from the mean. . However, since histograms come with their own set of caveats, let us use kernel desnity estimation for obtaining the probability density of 1d Gaussian. . from sklearn.neighbors import KernelDensity x_d = np.linspace(-4, 4, 100) # instantiate and fit the KDE model kde = KernelDensity(bandwidth=1.0, kernel=&#39;gaussian&#39;) kde.fit(one_dim_normal_data[:, None]) # score_samples returns the log of the probability density logprob = kde.score_samples(x_d[:, None]) plt.fill_between(x_d, np.exp(logprob), alpha=0.2, color=&#39;gray&#39;) plt.plot(one_dim_normal_data, np.full_like(one_dim_normal_data, -0.01), &#39;|k&#39;, markeredgewidth=0.1) format_axes(plt.gca()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d0a08c340&gt; . We can now see a smoother version of the histogram and can again verify the properties of 1D Gaussian. Let us now vary the variance of 1D Gaussian and make the same plots to enhance our understanding of the concept. . fig, ax = plt.subplots(ncols=3, sharey=True, figsize=(9, 3)) x_d = np.linspace(-6, 6, 400) for i, var in enumerate([0.5, 1, 2]): one_dim_normal_data = np.random.normal(0, var, size=10000) kde = KernelDensity(bandwidth=1.0, kernel=&#39;gaussian&#39;) kde.fit(one_dim_normal_data[:, None]) # score_samples returns the log of the probability density logprob = kde.score_samples(x_d[:, None]) ax[i].fill_between(x_d, np.exp(logprob), alpha=0.2, color=&#39;gray&#39;) ax[i].plot(one_dim_normal_data, np.full_like(one_dim_normal_data, -0.01), &#39;|k&#39;, markeredgewidth=0.1) format_axes(ax[i]) ax[i].set_title(f&quot;Variance = {var}&quot;) . We can see that how increasing the variance makes the data more spread. . Bi-variate Gaussian . Having discussed the case of 1d Gaussian, now let us move to multivariate Gaussians. As a special case, let us first consider bi-variate or 2d Gaussian. It&#39;s parameters are the mean vector which will have 2 elements and a covariance matrix. . We can write the distribution as: $$ begin{pmatrix} X_1 X_2 end{pmatrix} sim mathcal{N} left( begin{pmatrix} mu_1 mu_2 end{pmatrix} , begin{pmatrix} a &amp; rho rho &amp; b end{pmatrix} right) $$ . where $ mu_1$, $ mu_2$ are the means for $X_1$ and $X_2$ respectively; $a$ is the standard deviation for $X_1$, $b$ is the standard deviation for $X_2$ and $ rho$ is the correlation between $X_1$ and $X_2$ . Let us now draw some data from: $$ begin{pmatrix} X_1 X_2 end{pmatrix} sim mathcal{N} left( begin{pmatrix} 0 0 end{pmatrix} , begin{pmatrix} 1 &amp; 0.7 0.7 &amp; 1 end{pmatrix} right) $$ . data = np.random.multivariate_normal(mean = np.array([0, 0]), cov = np.array([[1, 0.7], [0.7, 1]]), size=(10000, )) . plt.scatter(data[:, 0], data[:, 1], alpha=0.05,c=&#39;gray&#39;) plt.axhline(0, color=&#39;k&#39;, lw=0.2) plt.axvline(0, color=&#39;k&#39;, lw=0.2) plt.xlabel(r&quot;$X_1$&quot;) plt.ylabel(r&quot;$X_2$&quot;) format_axes(plt.gca()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d0a235610&gt; . We can see from the plot above that the data is distributed around mean [0, 0]. We can also see the positive correlation between $X_1$ and $X_2$ . Marginalisation for bivariate Gaussian . Let us look into an interesting plot provided by Seaborn. . import pandas as pd data_df = pd.DataFrame(data, columns=[r&#39;$X_1$&#39;,r&#39;$X_2$&#39;]) . import seaborn as sns g = sns.jointplot(x= r&#39;$X_1$&#39;, y=r&#39;$X_2$&#39;, data=data_df, kind=&quot;reg&quot;,color=&#39;gray&#39;) . The central plot is exactly the same as the scatter plot we made earlier. But, we see two additional 1d KDE plots at the top and the right. What do these tell us? These tell us the marginal 1d distributions of $X_1$ and $X_2$. . The marginal distribution of $X_1$ is the distribution of $X_1$ considering all values of $X_2$ and vice versa. One of the interesting properties of Gaussian distributions is that the marginal distribution of a Gaussian is also a Gaussian distribution. MathematicalMonk on Youtube has a great set of lectures on this topic that I would highly recommend! . What would you expect the marginal distribution of $X_1$ to look like? No prizes for guessing. . Given $$ begin{pmatrix} X_1 X_2 end{pmatrix} sim mathcal{N} left( begin{pmatrix} mu_1 mu_2 end{pmatrix} , begin{pmatrix} a &amp; rho rho &amp; b end{pmatrix} right) $$ . we have the marginal distribution of: $$X_1 sim mathcal{N}( mu_1, a)$$ and $$X_2 sim mathcal{N}( mu_2, b)$$ . def plot_jointplot_2d(a, b, rho): data = np.random.multivariate_normal(mean = np.array([0, 0]), cov = np.array([[a, rho], [rho, b]]), size=(10000, )) data_df = pd.DataFrame(data, columns=[r&#39;$X_1$&#39;,r&#39;$X_2$&#39;]) g = sns.jointplot(x= r&#39;$X_1$&#39;, y=r&#39;$X_2$&#39;, data=data_df, kind=&quot;reg&quot;,color=&#39;gray&#39;) . Ok, let us know try to plot a few jointplots for different covariance matrices. We would be passing in the values of $a$, $b$ and $ rho$ which would make up the covariance matrix as: begin{pmatrix} a &amp; rho rho &amp; b end{pmatrix} . We would make these plots for mean zero. . plot_jointplot_2d(1, 1, -0.7) . In the plot above, for $a=1$, $b=1$ and $ rho=0.7$ we can see the negative correlation (but high) between $X_1$ and $X_2$. . Let us now increase the variance in $X_1$ and keep all other paramaters constant. . plot_jointplot_2d(2, 1, -0.7) . One can see from the plot above that the variance in $X_1$ is much higher now and the plot extends from -6 to +6 for $X_1$ while earlier it was restricted from -4 to 4. . plot_jointplot_2d(1, 1, 0.0) . One can see from the plot above that the correlation between $X_1$ and $X_2$ is zero. . Surface plots for bi-variate Gaussian . We will now look into surface plots for bi-variate Gaussian. This is yet another way to plot and understand Gaussian distributions. I borrow code from an excellent tuorial on plotting bivariate Gaussians. . from scipy.stats import multivariate_normal from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm def make_pdf_2d_gaussian(mu, sigma): N = 60 X = np.linspace(-3, 3, N) Y = np.linspace(-3, 4, N) X, Y = np.meshgrid(X, Y) # Pack X and Y into a single 3-dimensional array pos = np.empty(X.shape + (2,)) pos[:, :, 0] = X pos[:, :, 1] = Y F = multivariate_normal(mu, sigma) Z = F.pdf(pos) # Create a surface plot and projected filled contour plot under it. fig = plt.figure() ax = fig.gca(projection=&#39;3d&#39;) ax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True, cmap=cm.Greys) ax.set_xlabel(r&quot;$X_1$&quot;) ax.set_ylabel(r&quot;$X_2$&quot;) ax.set_zlabel(&quot;PDF&quot;) cset = ax.contourf(X, Y, Z, zdir=&#39;z&#39;, offset=-0.15, cmap=cm.Greys) # Adjust the limits, ticks and view angle ax.set_zlim(-0.15,0.25) ax.set_zticks(np.linspace(0,0.2,5)) ax.view_init(27, -15) ax.set_title(f&#39;$ mu$ = {mu} n $ Sigma$ = {sigma}&#39;) . mu = np.array([0., 0.]) sigma = np.array([[ 1. , -0.5], [-0.5, 1]]) make_pdf_2d_gaussian(mu, sigma) . From the plot above, we can see the surface plot showing the probability density function for the Gaussian with mean begin{pmatrix} 0 0 end{pmatrix} and covariance matrix: begin{pmatrix} 1 &amp; -0.5 -0.5 &amp; 1 end{pmatrix} . It can be seen that the probability peaks arounds $X_1=0$ and $X_2=0$. The bottom plot shows the same concept using contour plots which we will heavily use from now on. The different circles in the bottom contour plot denote the loci of same probability density. Since the contour plot requires a lesser dimension, it will be easier to use in our further analysis. . Also, from the contour plots, we can see the correlation between $X_1$ and $X_2$. . mu = np.array([0., 0.]) sigma = np.array([[ 1. , 0], [0, 1]]) make_pdf_2d_gaussian(mu, sigma) . In the plot above, we can see that $X_1$ and $X_2$ are not correlated. . Contour plots for 2D Gaussians . Having seen the relationship between the surface plots and the contour plots, we will now exclusively focus on the contour plots. Here is a simple function to generate the contour plot for 2g gaussian with mean and covariance as the arguments. . def plot_2d_contour_pdf(mu, sigma): X = np.linspace(-3, 3, 60) Y = np.linspace(-3, 4, 60) X, Y = np.meshgrid(X, Y) # Pack X and Y into a single 3-dimensional array pos = np.empty(X.shape + (2,)) pos[:, :, 0] = X pos[:, :, 1] = Y F = multivariate_normal(mu, sigma) Z = F.pdf(pos) plt.xlabel(r&quot;$X_1$&quot;) plt.ylabel(r&quot;$X_2$&quot;) plt.title(f&#39;$ mu$ = {mu} n $ Sigma$ = {sigma}&#39;) plt.contourf(X, Y, Z, zdir=&#39;z&#39;, offset=-0.15, cmap=cm.Greys) plt.colorbar() format_axes(plt.gca()) . mu = np.array([0., 0.]) sigma = np.array([[ 1. , 0.5], [0.5, 1.]]) plot_2d_contour_pdf(mu, sigma) . The plot above shows the contour plot for 2d gaussian with mean [0, 0] and covariance [[ 1. , 0.5], [0.5, 1.]]. We can see the correlation between $X_1$ and $X_2$ . Sample from 2d gaussian and visualising it on XY plane . We will now sample a point from a 2d Gaussian and describe a new way of visualising it. . . The left most plot shows the covariance matrix. . | The middle plot shows the contour plot. The dark point marked in the contour plot is a sampled point (at random) from this 2d Gaussian distribution. . | The right most plot is an alternative representation of the sampled point. The x-axis corresponds to the labels $X_1$ and $X_2$ and the corresponding y-axis are the coordinates of the point in the $X_1$, $X_2$ dimension shown in the contour plot. . | . We will now write a function to generate a random sample from a 2d gaussian given it&#39;s mean and covariance matrix. . def plot_2d_contour_pdf_dimensions(mu, sigma, random_num): fig, ax = plt.subplots(ncols=3, figsize=(12, 4)) X = np.linspace(-3, 3, 60) Y = np.linspace(-3, 3, 60) X, Y = np.meshgrid(X, Y) # Pack X and Y into a single 3-dimensional array pos = np.empty(X.shape + (2,)) pos[:, :, 0] = X pos[:, :, 1] = Y F = multivariate_normal(mu, sigma) Z = F.pdf(pos) random_point = F.rvs(random_state=random_num) sns.heatmap(sigma, ax=ax[0], annot=True) ax[1].contour(X, Y, Z, cmap=cm.Greys) ax[1].scatter(random_point[0], random_point[1], color=&#39;k&#39;,s=100) ax[1].set_xlabel(r&quot;$X_1$&quot;) ax[1].set_ylabel(r&quot;$X_2$&quot;) data_array = pd.Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;]) data_array.plot(ax=ax[2], kind=&#39;line&#39;, marker=&#39;o&#39;,color=&#39;k&#39;) plt.xticks(np.arange(len(data_array.index)), data_array.index.values) ax[2].set_ylim(-3, 3) format_axes(ax[0]) format_axes(ax[1]) format_axes(ax[2]) ax[0].set_title(&quot;Covariance Matrix&quot;) ax[1].set_title(&quot;Contour of pdf&quot;) ax[2].set_title(&quot;Visualising the point&quot;) plt.suptitle(f&quot;Random state = {random_num}&quot;, y=1.1) plt.tight_layout() import os if not os.path.exists(&quot;images&quot;): os.makedirs(&quot;images&quot;) if not os.path.exists(f&quot;images/{sigma[0, 1]}&quot;): os.makedirs(f&quot;images/{sigma[0, 1]}&quot;) plt.savefig(f&quot;images/{sigma[0, 1]}/{random_num}.jpg&quot;, bbox_inches=&quot;tight&quot;) plt.close() . We will now create 20 such samples and animate them . for i in range(20): plot_2d_contour_pdf_dimensions( mu, np.array([[ 1. , 0.1], [0.1, 1.]]), i) . !convert -delay 20 -loop 0 images/0.1/*.jpg sigma-0-1.gif . . Since the correlation between the two variables $X_1$ and $X_2$ was low (0.1), we can the see that rightmost plot jumping a lot, i.e. to say that the values of $X_1$ and $X_2$ are not tighly constrained to move together. . for i in range(20): plot_2d_contour_pdf_dimensions( mu, np.array([[ 1. , 0.7], [0.7, 1.]]), i) . !convert -delay 20 -loop 0 images/0.7/*.jpg sigma-0-7.gif . . The above GIF shows the same plot/animation for the 2d Gaussian where the correlation between the two variables is high (0.7). Thus, we can see that the two variables tend to move up and down together. . Conditional Bivariate Distribution . All excellent till now. Now, let us move to the case in which some variable&#39;s values are known. We would then look to find the distribution of the other variables conditional on the value of the known variable. I borrow some text from Wikipedia on the subject. . $$ begin{pmatrix} X_1 X_2 end{pmatrix} sim mathcal{N} left( begin{pmatrix} 0 0 end{pmatrix} , begin{pmatrix} 1 &amp; rho rho &amp; 1 end{pmatrix} right) $$The conditional expectation of $X_2$ given $X_1$ is: $ operatorname{E}(X_2 mid X_1=x_1)= rho x_1 $ . and the conditional variance is: $ operatorname{var}(X_2 mid X_1 = x_1) = 1- rho^2$ . So, the question now is: suppose we fix $X_1 = 1$, what is the distribution of $X_2$. Again, Gaussians are amazing - the conditional distributionon is again a Gaussian. Let us make some plots to understand better. The following plots would be showing the distribution of $X_2$ with fixed $X_1$ . def plot_2d_contour_pdf_dimensions_fixed_x1(sigma, random_num, x1 = 1): mu = np.zeros(2) fig, ax = plt.subplots(ncols=3, figsize=(12, 4)) X = np.linspace(-3, 3, 60) Y = np.linspace(-3, 3, 60) X, Y = np.meshgrid(X, Y) # Pack X and Y into a single 3-dimensional array pos = np.empty(X.shape + (2,)) pos[:, :, 0] = X pos[:, :, 1] = Y F = multivariate_normal(mu, sigma) Z = F.pdf(pos) rho = sigma[0, 1] F_cond_x1 = multivariate_normal(rho*x1, 1-rho**2) random_point_x2 = F_cond_x1.rvs(random_state=random_num) sns.heatmap(sigma, ax=ax[0], annot=True) ax[1].contour(X, Y, Z, cmap=cm.Greys) ax[1].scatter(x1, random_point_x2, color=&#39;k&#39;,s=100) ax[1].set_xlabel(r&quot;$X_1$&quot;) ax[1].set_ylabel(r&quot;$X_2$&quot;) data_array = pd.Series([x1, random_point_x2], index=[&#39;X1&#39;,&#39;X2&#39;]) data_array.plot(ax=ax[2], kind=&#39;line&#39;, color=&#39;k&#39;) ax[2].scatter(x=0, y=x1, color=&#39;red&#39;, s=100) ax[2].scatter(x=1, y=random_point_x2, color=&#39;k&#39;, s=100) plt.xticks(np.arange(len(data_array.index)), data_array.index.values) ax[2].set_ylim(-3, 3) format_axes(ax[0]) format_axes(ax[1]) format_axes(ax[2]) ax[0].set_title(&quot;Covariance Matrix&quot;) ax[1].set_title(&quot;Contour of pdf&quot;) ax[2].set_title(&quot;Visualising the point&quot;) plt.suptitle(f&quot;Random state = {random_num}&quot;, y=1.1) plt.tight_layout() import os if not os.path.exists(&quot;images/conditional/&quot;): os.makedirs(&quot;images/conditional/&quot;) if not os.path.exists(f&quot;images/conditional/{sigma[0, 1]}&quot;): os.makedirs(f&quot;images/conditional/{sigma[0, 1]}&quot;) plt.savefig(f&quot;images/conditional/{sigma[0, 1]}/{random_num}.jpg&quot;, bbox_inches=&quot;tight&quot;) plt.close() . for i in range(20): plot_2d_contour_pdf_dimensions_fixed_x1(np.array([[ 1. , 0.1], [0.1, 1.]]), i) . !convert -delay 20 -loop 0 images/conditional/0.1/*.jpg conditional-sigma-0-1.gif . . The above animation shows the movement of $X_2$ with $X_1=1$. The $X_1=1$ is shown in red in the righmost plot. In the middle plot, we can confirm that the movement is only in the $X_2$ dimension. Further, since the correlation between $X_1$ and $X_2$ is weak, the righmost plot seems to wiggle or jump a lot! . for i in range(20): plot_2d_contour_pdf_dimensions_fixed_x1(np.array([[ 1. , 0.7], [0.7, 1.]]), i) . !convert -delay 20 -loop 0 images/conditional/0.7/*.jpg conditional-sigma-0-7.gif . . In the plot above, we repeat the same p|rocedure but with a covariance matrix having a much higher correlation between $X_1$ and $X_2$. From the righmost plot, we can clearly see that the jumps in $X2$ are far lesser. This is expected, since the two variables are correlated! . Visualising the same procedure for 5 dimensional Gaussian . We will now repeat the same procedure we did for 2d case in 5 dimensions. . covariance_5d = np.array([[1, 0.9, 0.8, 0.6, 0.4], [0.9, 1, 0.9, 0.8, 0.6], [0.8, 0.9, 1, 0.9, 0.8], [0.6, 0.8, 0.9, 1, 0.9], [0.4, 0.6, 0.8, 0.9, 1]]) . def plot_5d_contour_pdf_dimensions(cov, random_num): fig, ax = plt.subplots(ncols=2, figsize=(6, 3)) mu = np.zeros(5) F = multivariate_normal(mu, cov) random_point = F.rvs(random_state=random_num) sns.heatmap(cov, ax=ax[0], annot=True) data_array = pd.Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;, &#39;X5&#39;]) data_array.plot(ax=ax[1], kind=&#39;line&#39;, marker=&#39;o&#39;,color=&#39;k&#39;) plt.xticks(np.arange(len(data_array.index)), data_array.index.values) ax[1].set_ylim(-3, 3) for i in range(2): format_axes(ax[i]) ax[0].set_title(&quot;Covariance Matrix&quot;) ax[-1].set_title(&quot;Visualising the point&quot;) plt.suptitle(f&quot;Random state = {random_num}&quot;, y=1.1) plt.tight_layout() import os if not os.path.exists(&quot;images/5d/&quot;): os.makedirs(&quot;images/5d&quot;) plt.savefig(f&quot;images/5d/{random_num}.jpg&quot;, bbox_inches=&quot;tight&quot;) plt.close() . plot_5d_contour_pdf_dimensions(covariance_5d, 2) . for i in range(20): plot_5d_contour_pdf_dimensions(covariance_5d, i) . !convert -delay 20 -loop 0 images/5d/*.jpg 5d.gif . . From the visualisation above we can see that: . since X1 and X2 are highly correlated, they move up and down together | but, X1 and X5 have low correlation, thus, they can seem to wiggle almost independently of each other. | . We are now getting somewhere. If the correlation between the variables is very high, we will get a smooth curve joining them. Right? Almost getting to the point where we can draw the introductory plot shown at the top of the post. . Conditional Multivariate Distribution . Ok, now let us draw the conditional distribution over this higher 5d space. We will fix the values of some of the variables and see the distribution of the others. . Borrowing from Wikipedia . If $N$-dimensional $x$ is partitioned as follows . $$ mathbf{x} = begin{bmatrix} mathbf{x}_A mathbf{x}_B end{bmatrix} text{ with sizes } begin{bmatrix} q times 1 (N-q) times 1 end{bmatrix} $$and accordingly $μ$ and $Σ$ are partitioned as follows . $$ boldsymbol mu = begin{bmatrix} boldsymbol mu_A boldsymbol mu_B end{bmatrix} text{ with sizes } begin{bmatrix} q times 1 (N-q) times 1 end{bmatrix} $$ $$ boldsymbol Sigma = begin{bmatrix} boldsymbol Sigma_{AA} &amp; boldsymbol Sigma_{AB} boldsymbol Sigma_{BA} &amp; boldsymbol Sigma_{BB} end{bmatrix} text{ with sizes } begin{bmatrix} q times q &amp; q times (N-q) (N-q) times q &amp; (N-q) times (N-q) end{bmatrix} $$then the distribution of $x_A$ conditional on $x_B=b$ is multivariate normal $(x_A|x_B=b) sim mathcal{N}( bar{ mu}, bar{ Sigma})$ . $$ bar{ boldsymbol mu} = boldsymbol mu_A + boldsymbol Sigma_{AB} boldsymbol Sigma_{BB}^{-1} left( mathbf{B} - boldsymbol mu_B right) $$and covariance matrix . $$ overline{ boldsymbol Sigma} = boldsymbol Sigma_{AA} - boldsymbol Sigma_{AB} boldsymbol Sigma_{BB}^{-1} boldsymbol Sigma_{BA}. $$ Let us for our example take $X_5 = -2$. . We have: . $x_A = [x_1, x_2, x_3, x_4]$ and $x_B = [x_5]$ . Assuming the covariance matrix of size 5 X 5 is referred as $C$ . $$ boldsymbol Sigma_{AA} = begin{bmatrix} C_{11} &amp; C_{12} &amp; C_{13} &amp; C_{14} C_{21} &amp; C_{22} &amp; C_{23} &amp; C_{24} C_{31} &amp; C_{32} &amp; C_{33} &amp; C_{34} C_{41} &amp; C_{42} &amp; C_{43} &amp; C_{44} end{bmatrix} $$$$ boldsymbol Sigma_{AB} = begin{bmatrix} C_{15} C_{25} C_{35} C_{45} end{bmatrix} $$$$ boldsymbol Sigma_{BA} = begin{bmatrix} C_{51}&amp; C_{52} &amp; C_{53} &amp; C_{54} end{bmatrix} $$$$ boldsymbol Sigma_{BB} = begin{bmatrix} C_{55} end{bmatrix} $$ Putting in the numbers we get: . sigma_AA = covariance_5d[:4, :4] . sigma_AA . array([[1. , 0.9, 0.8, 0.6], [0.9, 1. , 0.9, 0.8], [0.8, 0.9, 1. , 0.9], [0.6, 0.8, 0.9, 1. ]]) . sigma_AB = covariance_5d[:4, 4].reshape(-1, 1) . sigma_AB . array([[0.4], [0.6], [0.8], [0.9]]) . sigma_BA = covariance_5d[4, :4].reshape(1, -1) . sigma_BA . array([[0.4, 0.6, 0.8, 0.9]]) . sigma_BB = covariance_5d[4, 4].reshape(-1, 1) . sigma_BB . array([[1.]]) . Now, calculating $ bar{ mu}$ . mu_bar = np.zeros((4, 1)) + sigma_AB@np.linalg.inv(sigma_BB)*(-2) . mu_bar . array([[-0.8], [-1.2], [-1.6], [-1.8]]) . Since, $x_5$ has highest correlation with $x_4$ it makes sense for $x_5=-2$ to have the mean of $x_4$ to be close to -2. . Now, calculating $ bar{ Sigma}$ . sigma_bar = sigma_AA - sigma_AB@np.linalg.inv(sigma_BB)@sigma_BA . sigma_bar . array([[0.84, 0.66, 0.48, 0.24], [0.66, 0.64, 0.42, 0.26], [0.48, 0.42, 0.36, 0.18], [0.24, 0.26, 0.18, 0.19]]) . Now, we have the new mean and covariance matrices for $x_A = [x_1, x_2, x_3, x_4]$ and $x_B = [x_5] = [-2]$. Let us now draw some samples fixing $x_5 = -2$ . cov = sigma_bar mu = mu_bar.flatten() def plot_5d_samples_fixed_x2(random_num): fig, ax = plt.subplots(ncols=2, figsize=(6, 3)) F = multivariate_normal(mu, cov) sns.heatmap(cov, ax=ax[0], annot=True) random_point = F.rvs(random_state=random_num) data_array = pd.Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;]) data_array[&#39;X5&#39;] = -2 data_array.plot(ax=ax[1], kind=&#39;line&#39;, marker=&#39;.&#39;,color=&#39;k&#39;) plt.scatter([4], [-2], color=&#39;red&#39;, s=100) plt.xticks(np.arange(len(data_array.index)), data_array.index.values) ax[1].set_ylim(-3, 3) for i in range(2): format_axes(ax[i]) ax[0].set_title(&quot;Covariance Matrix&quot;) ax[-1].set_title(&quot;Visualising the point&quot;) plt.suptitle(f&quot;Random state = {random_num}&quot;, y=1.1) plt.tight_layout() import os if not os.path.exists(&quot;images/5d/conditional/1&quot;): os.makedirs(&quot;images/5d/conditional/1&quot;) plt.savefig(f&quot;images/5d/conditional/1/{random_num}.jpg&quot;, bbox_inches=&quot;tight&quot;) plt.close() . for i in range(20): plot_5d_samples_fixed_x2(i) . !convert -delay 20 -loop 0 images/5d/conditional/1/*.jpg 5d-conditional-1.gif . . Let&#39;s increase to 20 dimensions now! . We can not surely write the covariance matrix for 20 dimensions. Let us use a small trick called the kernel function to create this matrix. We will come it later. For now, let us think of this function as a function which: . outputs low numbers for $x_1$ and $x_2$ if they differ by a lot | outputs high number for $x_1$ and $x_2$ if they are very close | . def rbf_kernel(x_1, x_2, sig): return np.exp((-(x_1-x_2)**2)/2*(sig**2)) . rbf_kernel(1, 1, 0.4) . 1.0 . Since 1=1, the above function evaluates to 1 showing that 1 is similar to 1 . rbf_kernel(1, 2, 0.4) . 0.9231163463866358 . Since 1 and 2 are close, the function above evaluates to close to 1 . rbf_kernel(1, 2, 1) . 0.6065306597126334 . Ok, we use the same first two arguments 1 and 2 but change the last one to 1 from 0.4 and we see that the function evaluates to a much smaller number. Thus, we can see that increase the sig parameter leads to quicker dropoff in similarity between pair of points. Or, in other words, higher sig means that the influence of a point x_1 reduces quicker. . Let us now create the covariance matrix of size (20, 20) using this kernel function. . C = np.zeros((20, 20)) . for i in range(20): for j in range(20): C[i, j] = rbf_kernel(i, j, 0.5) . Let us plot the heatmap of the covariance matrix . sns.heatmap(C) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d11ccbee0&gt; . The above heatmap confirms that there is correlation between nearby points, but close to zero or zero correlation otherwise. . Let us draw some samples from this 20 dimensional Gaussian . def plot_20d_samples(random_num): fig, ax = plt.subplots(figsize=(10, 3)) F = multivariate_normal(np.zeros(20), C) random_point = F.rvs(random_state=random_num) index = [f&#39;X{i}&#39; for i in range(1, 21)] data_array = pd.Series(random_point, index=index) data_array.plot(ax=ax, kind=&#39;line&#39;, marker=&#39;.&#39;,color=&#39;k&#39;) plt.xticks(np.arange(len(data_array.index)), data_array.index.values) plt.suptitle(f&quot;Random state = {random_num}&quot;, y=1.1) plt.tight_layout() import os if not os.path.exists(&quot;images/20d/&quot;): os.makedirs(&quot;images/20d/&quot;) plt.ylim(-3, 3) plt.savefig(f&quot;images/20d/{random_num}.jpg&quot;, bbox_inches=&quot;tight&quot;) plt.close() . for i in range(50): plot_20d_samples(i) . !convert -delay 20 -loop 0 images/20d/*.jpg 20d.gif . . From the animation above, we can see different family of functions of mean zero across these 20 points. We&#39;re really getting close now! . Let us now condition on a few elements . We will create a new ordering of these variables such that the known variables occur towards the end. This allows for easy calculations for conditioning. . order = [2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 1, 5, 10] . new_C = np.zeros_like(C) . old_order = range(20) . for i in range(20): for j in range(20): new_C[i, j] = C[order[i], order[j]] . sns.heatmap(new_C, xticklabels=order, yticklabels=order, cmap=&#39;jet&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d10b88d00&gt; . Now, we can condition on (x1 = 1, x2 = 3, x6 = -3, X11 = 1). We will use the same procedure we used above in the case of 5d. . B = np.array([1, 3, -3, 1]).reshape(-1, 1) B . array([[ 1], [ 3], [-3], [ 1]]) . sigma_AA_20d = new_C[:-B.size, :-B.size] sigma_AA_20d.shape . (16, 16) . sigma_BB_20d = new_C[-B.size:, -B.size:] sigma_BB_20d.shape . (4, 4) . sigma_AB_20d = new_C[:-B.size, -B.size:] sigma_AB_20d.shape . (16, 4) . sigma_BA_20d = new_C[-B.size:, :-B.size] sigma_BA_20d.shape . (4, 16) . mu_bar_20d = np.zeros((20-B.size, 1)) + sigma_AB_20d@np.linalg.inv(sigma_BB_20d)@(B) . sigma_bar_20d = sigma_AA_20d - sigma_AB_20d@np.linalg.inv(sigma_BB_20d)@sigma_BA_20d . sns.heatmap(sigma_bar_20d, xticklabels=order[:-B.size], yticklabels=order[:-B.size], cmap=&#39;jet&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d091fd0a0&gt; . def plot_20d_samples_known_x(random_num): fig, ax = plt.subplots(figsize=(10, 3)) F = multivariate_normal(mu_bar_20d.flatten(), sigma_bar_20d) random_point = F.rvs(random_state=random_num) index = [f&#39;X{i+1}&#39; for i in order[:-B.size]] data_array = pd.Series(random_point, index=index) data_array[&#39;X1&#39;] = 1 data_array[&#39;X2&#39;] = 3 data_array[&#39;X6&#39;] = -3 data_array[&#39;X11&#39;] = -1 data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]] data_array.plot(ax=ax, kind=&#39;line&#39;, marker=&#39;.&#39;,color=&#39;k&#39;) plt.xticks(np.arange(len(data_array.index)), data_array.index.values) plt.scatter([0, 1,5, 10], [1, 3, -3, -1], color=&#39;red&#39;,s=100) plt.suptitle(f&quot;Random state = {random_num}&quot;, y=1.1) plt.tight_layout() import os if not os.path.exists(&quot;images/20d/conditional/&quot;): os.makedirs(&quot;images/20d/conditional/&quot;) plt.grid() plt.ylim(-4, 4) plt.savefig(f&quot;images/20d/conditional/{random_num}.jpg&quot;, bbox_inches=&quot;tight&quot;) plt.close() . for i in range(50): plot_20d_samples_known_x(i) . !convert -delay 20 -loop 0 images/20d/conditional/*.jpg 20d-conditional.gif . . From the plot above, we can see the known points in red and the other points wiggle to show the families of functions that we fit. Let us now draw a lot of samples and plot the mean and variance in these samples for the unknown X variables. We could have obtained the mean and variance directly using Gaussian marginalisation, but, for now let us just draw many samples. . F = multivariate_normal(mu_bar_20d.flatten(), sigma_bar_20d) dfs = {} for random_num in range(100): random_point = F.rvs(random_state=random_num) index = [f&#39;X{i+1}&#39; for i in order[:-B.size]] data_array = pd.Series(random_point, index=index) data_array[&#39;X1&#39;] = 1 data_array[&#39;X2&#39;] = 3 data_array[&#39;X6&#39;] = -3 data_array[&#39;X11&#39;] = -1 data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]] dfs[random_num] = data_array . fig, ax = plt.subplots(figsize=(10, 3)) pd.DataFrame(dfs).mean(axis=1).plot(yerr=pd.DataFrame(dfs).std(axis=1),marker=&#39;o&#39;, color=&#39;k&#39;) plt.xticks(np.arange(len(data_array.index)), data_array.index.values) plt.scatter([0, 1,5, 10], [1, 3, -3, -1], color=&#39;red&#39;,s=100) format_axes(plt.gca()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d0fcc8d00&gt; . From the plot above, we can see the uncertainty (standard deviation) and the mean values for different variables. As expected, the uncertainty close to the known points (red) is low. Also, owing to the smooth nature of the covariance function we can see the means of unknown points close to known points are fairly similar. . To summarise: We can very clearly see that there is low variance in zones where we have the known values and high variance otherwise. The farther we go away from a known value, the more is the variance! . Kernels! . We will now take a small plunge into the world of kernels. As mentioned earlier, we will limit the discussion to generating to covariance matrix. . We will be redefining the function mentioned above to include two parameters l and s . s is the scale of variance | l is the influence of the point to neighbouring points | . def sig(x1, x2, l, s): return s**2*(np.exp((-1/2*(l**2))*((x1-x2)**2))) . Cov_matrix = np.zeros((100, 100)) . fig, ax = plt.subplots(ncols=4, sharex=True, sharey=True) s = 1 for ix, l in enumerate([0.001, 0.01, 0.1, 1]): for i in range(100): for j in range(100): Cov_matrix[i, j] = sig(i, j, l, 1) im = ax[ix].imshow(Cov_matrix, cmap=&#39;jet&#39;) ax[ix].set_title(f&quot;l={l}&quot;) fig.subplots_adjust(right=0.8) cbar_ax = fig.add_axes([0.85, 0.35, 0.05, 0.3]) fig.colorbar(im, cax=cbar_ax) plt.suptitle(f&quot;Covariance matrix for varying l and s = {s}&quot;) . Text(0.5, 0.98, &#39;Covariance matrix for varying l and s = 1&#39;) . In the plot above, we can the covariance matrices for fixed s=1 and varying l. It can be seen that for very low l, the correlations between far away points is also significant. At l=1, this ceases to be the case. . fig, ax = plt.subplots(ncols=4, sharex=True, sharey=True, figsize=(12, 3)) for ix, s in enumerate([1, 10, 20, 30]): for i in range(100): for j in range(100): Cov_matrix[i, j] = sig(i, j, 0.1, s) sns.heatmap(Cov_matrix, cmap=&#39;jet&#39;, ax=ax[ix]) ax[ix].set_title(f&quot;s={s}&quot;) plt.suptitle(&quot;Covariance matrix for varying s and l = 0.1&quot;) . Text(0.5, 0.98, &#39;Covariance matrix for varying s and l = 0.1&#39;) . Ok, this is great. We can see the different scales on the colorbars with increasing s and fixing l . Now, let us try and redo the 20 point dataset with varying kernel parameters with conditioning on some known data. . def fit_plot_gp(kernel_s, kernel_l, known_data, total_data_points, save=False): &quot;&quot;&quot; kernel_s: sigma^2 param of kernel kernel_l: l (width) param of kernel known_data: {pos: value} total_data_points &quot;&quot;&quot; o = list(range(20)) for key in known_data.keys(): o.remove(key) o.extend(list(known_data.keys())) C = np.zeros((total_data_points, total_data_points)) for i in range(total_data_points): for j in range(total_data_points): C[i, j] = sig(i, j, kernel_l, kernel_s) # Making known variables shift new_C = np.zeros_like(C) for i in range(20): for j in range(20): new_C[i, j] = C[o[i], o[j]] B = np.array(list(known_data.values())).reshape(-1, 1) sigma_BA_20d = new_C[-B.size:, :-B.size] sigma_AB_20d = new_C[:-B.size, -B.size:] sigma_BB_20d = new_C[-B.size:, -B.size:] sigma_AA_20d = new_C[:-B.size, :-B.size] mu_bar_20d = np.zeros((20-B.size, 1)) + sigma_AB_20d@np.linalg.inv(sigma_BB_20d)@(B) sigma_bar_20d = sigma_AA_20d - sigma_AB_20d@np.linalg.inv(sigma_BB_20d)@sigma_BA_20d F = multivariate_normal(mu_bar_20d.flatten(), sigma_bar_20d) dfs = {} for random_num in range(100): random_point = F.rvs(random_state=random_num) index = [f&#39;X{i+1}&#39; for i in o[:-B.size]] data_array = pd.Series(random_point, index=index) for k, v in known_data.items(): data_array[f&#39;X{k+1}&#39;] = v data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]] dfs[random_num] = data_array fig, ax = plt.subplots(figsize=(10, 3)) mean_vector = pd.DataFrame(dfs).mean(axis=1) mean_vector.plot(marker=&#39;.&#39;, color=&#39;k&#39;) yerr=pd.DataFrame(dfs).std(axis=1) plt.fill_between(range(len(mean_vector)), mean_vector+yerr, mean_vector-yerr, color=&#39;gray&#39;,alpha=0.4) plt.xticks(np.arange(len(data_array.index)), data_array.index.values) plt.scatter(list(known_data.keys()), list(known_data.values()), color=&#39;gray&#39;,s=200,zorder=1) format_axes(plt.gca()) plt.title(f&quot; l = {kernel_l} and s = {kernel_s}&quot;) import os if save: if not os.path.exists(&quot;images/20d/conditional-points/&quot;): os.makedirs(&quot;images/20d/conditional-points/&quot;) plt.grid() plt.xticks(np.arange(len(data_array.index)), np.arange(len(data_array.index))) plt.ylim(-4, 4) plt.title(f&quot;Known data: {known_data}&quot;) plt.savefig(f&quot;images/20d/conditional-points/{len(known_data.keys())}.jpg&quot;, bbox_inches=&quot;tight&quot;) plt.close() . known_d = {0:-2, 1:3, 9:-1, 14:-1} . fit_plot_gp(1, 0.5, known_d, 20) . The above plot shows the uncertainty and the family of functions for l=0.5 and s=1. . fit_plot_gp(5, 0.5, known_d, 20) . Keeping l=0.5, the above plot shows how increasing s increases the uncertainty of estimation. . fit_plot_gp(1, 1, known_d, 20) . The above plot shows how increasing l reduces the influence between far away points. . fit_plot_gp(1, 100, known_d, 20) . The above plot increases l to a very large value. Seems to be just moving around the mean? . np.random.seed(0) order_points_added = np.random.choice(range(20), size=9, replace=False) k = {} for i in range(9): k[order_points_added[i]] = np.random.choice(range(-3, 3)) fit_plot_gp(1, 0.5, k, 20, True) . !convert -delay 40 -loop 0 images/20d/conditional-points/*.jpg 20d-conditional-main.gif . Let us create a small animation where we keep on adding points and see how the uncertainty and estimation changes . . Creating a scikit-learn like function containing fit and predict . I&#39;ll now bring in the formal definitions, summarise the discussion and write a function akin to scikit-learn which can accept train data to estimate for test data. . Formally defining GPs . A Gaussian process is fully specified by a mean function m(x) and covariance function K(x, x&#39;) : . $$ f(x) sim GP (m(x),K(x, x&#39;) $$Let us consider a case of noiseless GPs now . Noiseless GPs . Given train data $$D = {(x_i, y_i), i = 1:N}$$ . Given a test set $X_{*}$ of size $N_* times d $ containing $N_*$ points in ${ rm I !R}^d$, we want to predict function outputs $y_{*}$ . We can write: . $$ begin{pmatrix} y y_* end{pmatrix} sim mathcal{N} left( begin{pmatrix} mu mu_* end{pmatrix} , begin{pmatrix} K &amp; K_* K_*^T &amp; K_{**} end{pmatrix} right) $$where . $$ K = Ker(X, X) in { rm I !R}^{N times N} K_* = Ker(X, X_*) in { rm I !R}^{N times N_*} K_{**} = Ker(X_*, X_*) in { rm I !R}^{N_* times N_*} $$We had previously used the kernel which we will continue to use . def sig(x1, x2, l, s): return s**2*(np.exp((-1/2*(l**2))*((x1-x2)**2))) . We can then write: . $$ p(y_*|X_*, X, y) sim mathcal{N}( mu&#39;, Sigma&#39;) mu&#39; = mu_* + K_*^TK^{-1}(x- mu) Sigma&#39; = K_{**} - K_*^TK^{-1}K_* $$ class NoiselessGP_inversion: def __init__(self, l=0.1, s=1, prior_mean=0): self.l = l self.s = s self.prior_mean = prior_mean def prior_sample(self, x, n): &quot;&quot;&quot; Sample GP on x &quot;&quot;&quot; self.sample_k = self.create_cov_matrix(x, x, self.l, self.s) for i in range(n): pass def kernel(self, a, b, l, s): &quot;&quot;&quot; Borrowed from Nando De Freita&#39;s lecture code https://www.cs.ubc.ca/~nando/540-2013/lectures/gp.py &quot;&quot;&quot; sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T) return s**2*np.exp(-.5 * (1/l) * sqdist) def fit(self, train_x, train_y): self.train_x = train_x self.train_y = train_y self.N = len(train_x) self.K = self.kernel(train_x, train_x, self.l, self.s) def predict(self, test_x): self.N_star = len(test_x) self.K_star = self.kernel(self.train_x, test_x, self.l, self.s) self.K_star_star = self.kernel(test_x, test_x, self.l, self.s) self.posterior_mu = self.prior_mean + self.K_star.T@np.linalg.inv(self.K)@(self.train_y-self.prior_mean) self.posterior_sigma = self.K_star_star - self.K_star.T@np.linalg.inv(self.K)@self.K_star return self.posterior_mu, self.posterior_sigma . clf = NoiselessGP_inversion() . train_x = np.array([-4, -3, -2, -1, 1]).reshape(5,1) train_y = np.sin(train_x) test_x = np.linspace(-5, 5, 50).reshape(-1, 1) test_y = np.sin(test_x) . plt.plot(train_x, train_y,&#39;ko-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f9d0ef432b0&gt;] . clf.fit(train_x, train_y) . posterior_mu, posterior_var = clf.predict(test_x) . plt.plot(test_x, clf.posterior_mu,&#39;k&#39;,label=&#39;Predicted&#39;,lw=1) plt.plot(test_x, test_y, &#39;purple&#39;,label=&#39;GT&#39;,lw=2) plt.plot(train_x, train_y, &#39;ko&#39;,label=&#39;Training Data&#39;) plt.fill_between(test_x.flatten(), (clf.posterior_mu.flatten() - clf.posterior_sigma.diagonal().flatten()), (clf.posterior_mu.flatten() + clf.posterior_sigma.diagonal().flatten()), color=&#39;gray&#39;, alpha=0.3 ) plt.legend() format_axes(plt.gca()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d0f567340&gt; . Cholesky decomposition . We had previously used matrix inversion to do the computation for computing the posterior mean and variance in our GP. However, the matrices involved may be poorly conditioned and thus Cholesky decomposition is often favoured. . From Wikipedia, the Cholesky decomposition of a matrix $A$ is given as: $$ mathbf{A} = mathbf{L L}^T $$ . where $L$ is a real lower triangular matrix. . We can thus re-write the posterior mean and covariance as: . $$ p(y_*|X_*, X, y) sim mathcal{N}( mu&#39;, Sigma&#39;) K = LL^T $$We are now going to use the as follows: if $A omega = B$, then $ omega$ = $A$ $B$ . We now have: $$ alpha = K^{-1}(x- mu) or, alpha = {LL^T}^{-1}(x- mu) or, alpha = L^{-T}L^{-1}(x- mu) Let, L^{-1}(x- mu) = gamma Thus, L gamma = x- mu Thus, gamma = L setminus (x- mu) Thus, alpha = L^{T} setminus (L setminus (x- mu)) $$ . In Python, the same can be written as: . L = np.linalg.cholesky(K) alpha = np.linalg.solve(L.T, np.linalg.solve(L, x-mu)) . Thus, we can find the posterior mean as: $$ mu&#39; = mu_* + K_*^T alpha $$ . We also know that $$ Sigma&#39; = K_{**} - K_*^TK^{-1}K_* $$ . Let us now define $$ v = L setminus K_{*} or, v = L^{-1}K_{*} Thus, v^{T} = K_{*}^TL^{-T} Thus, v^{T}v = K_{*}^TL^{-T}L^{-1}K_{*} Thus, v^{T}v = K_*^TK^{-1}K_* = K_{**} - Sigma&#39; $$ . $$ Sigma&#39; = K_{**} - v^{T}v $$ Let us know rewrite the code with Cholesky decomposition. . class NoiselessGP_Cholesky: def __init__(self, l=0.1, s=1, prior_mean=0): self.l = l self.s = s self.prior_mean = prior_mean def prior_sample(self, x, n): &quot;&quot;&quot; Sample GP on x &quot;&quot;&quot; self.sample_k = self.create_cov_matrix(x, x, self.l, self.s) for i in range(n): pass def kernel(self, a, b, l, s): &quot;&quot;&quot; Borrowed from Nando De Freita&#39;s lecture code https://www.cs.ubc.ca/~nando/540-2013/lectures/gp.py &quot;&quot;&quot; sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T) return s**2*np.exp(-.5 * (1/l) * sqdist) def fit(self, train_x, train_y): self.train_x = train_x self.train_y = train_y self.N = len(train_x) self.K = self.kernel(train_x, train_x, self.l, self.s) self.L = np.linalg.cholesky(self.K) def predict(self, test_x): self.N_star = len(test_x) self.K_star = self.kernel(self.train_x, test_x, self.l, self.s) self.K_star_star = self.kernel(test_x, test_x, self.l, self.s) self.alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, self.train_y-self.prior_mean)) self.v = np.linalg.solve(self.L, self.K_star) self.posterior_mu = self.prior_mean + self.K_star.T@self.alpha self.posterior_sigma = self.K_star_star - self.v.T@self.v return self.posterior_mu, self.posterior_sigma . clf = NoiselessGP_Cholesky() clf.fit(train_x, train_y) posterior_mu_cholesky, posterior_var_cholesky = clf.predict(test_x) . We will now compare our Cholesky decomposition based decompostion with the earlier one. . np.allclose(posterior_mu_cholesky, posterior_mu) . True . np.allclose(posterior_var_cholesky, posterior_var) . True . Ok, all looks good till now! Let us now move on to the case for Noisy GPs. . Noisy GPs . Previously, we had assumed a noiseless model, which is to say, for the observed data, we had: $$y_i = f(x_i)$$ . We now make the model more flexible by saying that there can be noise in the observed data as well, thus: $$ y_i = f(x_i) + epsilon epsilon sim mathcal{N}(0, sigma_y^2) $$ . One of the main difference compared to the noiseless model would be that in the noisy model, we will have some uncertainty even about the training points. . Everything about our model remains the same, except for the change in the covariance matrix $K$ for the training points, which is now given as: . $$K_y = sigma_y^2 mathbf{I_n} + K $$We can now rewrite the function as follows: . class NoisyGP: def __init__(self, l = 0.1, s = 1, prior_mean = 0, sigma_y = 1): self.l = l self.s = s self.prior_mean = prior_mean self.sigma_y = sigma_y def prior_sample(self, x, n): &quot;&quot;&quot; Sample GP on x &quot;&quot;&quot; self.sample_k = self.create_cov_matrix(x, x, self.l, self.s) for i in range(n): pass def kernel(self, a, b, l, s): &quot;&quot;&quot; Borrowed from Nando De Freita&#39;s lecture code https://www.cs.ubc.ca/~nando/540-2013/lectures/gp.py &quot;&quot;&quot; sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T) return s**2*np.exp(-.5 * (1/l) * sqdist) def fit(self, train_x, train_y): self.train_x = train_x self.train_y = train_y self.N = len(train_x) self.K = self.kernel(train_x, train_x, self.l, self.s) + self.sigma_y*np.eye(len(train_x)) self.L = np.linalg.cholesky(self.K) def predict(self, test_x): self.N_star = len(test_x) self.K_star = self.kernel(self.train_x, test_x, self.l, self.s) self.K_star_star = self.kernel(test_x, test_x, self.l, self.s) self.alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, self.train_y-self.prior_mean)) self.v = np.linalg.solve(self.L, self.K_star) self.posterior_mu = self.prior_mean + self.K_star.T@self.alpha self.posterior_sigma = self.K_star_star - self.v.T@self.v return self.posterior_mu, self.posterior_sigma . clf = NoisyGP(sigma_y=0.2) clf.fit(train_x, train_y) posterior_mu_noisy, posterior_var_noisy = clf.predict(test_x) . plt.plot(test_x, clf.posterior_mu,&#39;k&#39;,label=&#39;Predicted&#39;,lw=1) plt.plot(test_x, test_y, &#39;purple&#39;,label=&#39;GT&#39;,lw=2) plt.plot(train_x, train_y, &#39;ko&#39;,label=&#39;Training Data&#39;) plt.fill_between(test_x.flatten(), (clf.posterior_mu.flatten() - clf.posterior_sigma.diagonal().flatten()), (clf.posterior_mu.flatten() + clf.posterior_sigma.diagonal().flatten()), color=&#39;gray&#39;, alpha=0.3 ) plt.legend() format_axes(plt.gca()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9d11584550&gt; . We can now see that our model has some uncertainty even on the train points! .",
            "url": "https://nipunbatra.github.io/blog/ml/2019/08/20/Gaussian-Processes.html",
            "relUrl": "/ml/2019/08/20/Gaussian-Processes.html",
            "date": " • Aug 20, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Placement-Preparation-2018-1-HashMap",
            "content": "In this blogpost, we will take a question from Cracking the Coding Interview. I discussed this question with Masters students at IITGN. We came up with some great answers. I&#39;ll show how we increasingly went towards better solutions starting from naive ones. . Problem statement . Find all integer solutions to the problem $a^3 + b^3 = c^3 + d^3$ . where $1&lt;=a&lt;=n, 1&lt;=b&lt;=n, 1&lt;=c&lt;=n, 1&lt;=d&lt;=n$ . First attempt : Naive bruteforce $O(n^4)$ . Let&#39;s write a very simple first attempt. We will write four nested loops. This will be $O(n^4)$ solution. . def f1(n): out = [] for a in range(1, n+1): for b in range(1, n+1): for c in range(1, n+1): for d in range(1, n+1): if a**3 + b**3 == c**3 + d**3: out.append((a, b, c, d)) return out . f1_time = %timeit -o f1(50) . 6.65 s ± 203 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . f1_time.average . 6.646897936570895 . Second attempt : Reduce computations in brute force method . Let&#39;s now try to optimise f1. We will still use a solution of $O(n^4)$ solution. However, we add one small optimisation fo f1. We break from the innermost loop once we find a match. This will hopefull save us some computations. . def f2(n): out = [] for a in range(1, n+1): for b in range(1, n+1): for c in range(1, n+1): for d in range(1, n+1): if a**3 + b**3 == c**3 + d**3: out.append((a, b, c, d)) break return out . f2_time = %timeit -o f2(50) . 6.29 s ± 26.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Ok. We&#39;re little better than f1. Every reduced computation is time saved! . Third attempt : Reduce repeated computations by saving cubes of numbers . One of the student came up with an excellent observation. Why should we keep on computing cubes of numbers? This is a repeated operation. Let&#39;s instead store them in a dictionary. . def f3(n): cubes = {} for x in range(1, n+1): cubes[x] = x**3 out = [] for a in range(1, n+1): for b in range(1, n+1): for c in range(1, n+1): for d in range(1, n+1): if cubes[a] + cubes[b] == cubes[c] + cubes[d]: out.append((a, b, c, d)) break return out . f3_time = %timeit -o f3(50) . 1.05 s ± 4.11 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) . Ok. We now mean business! This is about 6 times quicker than our previous version. . Fourth attempt : Reduce one loop $O(n^3)$ . In this solution, we will reduce one loop. We can solve for $d^3 = a^3 + b^3 - c^3$ and find all the integer solutions. Now, there&#39;s another clever optimisation that I have added. We can precompute the cubes and the cuberoots corresponding to numbers from 1 to N and perfect cubes from 1 to $N^3$ respectively. . def f4(n): cubes = {} cuberoots = {} for x in range(1, n+1): x3 = x**3 cubes[x] = x3 cuberoots[x3] = x out = [] for a in range(1, n+1): for b in range(1, n+1): for c in range(1, n+1): d3 = (cubes[a] + cubes[b] - cubes[c]) if d3 in cuberoots: out.append((a, b, c, cuberoots[d3])) return out . f4_time = %timeit -o f4(50) . 21.7 ms ± 1.99 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . This is seriously fast now! . Fifth attempt : Reduce another loop $O(n^2)$ . In this solution, we will reduce one more loop. We can compute $a^3 + b^3$ for all a, b. And then find c and d where $c^3 + d^3$ is the same as $a^3 + b^3$. This has a few Python tricks inside! One of the special cases to handle is of the type $1^3 + 2^3 = 2^3 + 1^3$ . def f5(n): out = [] cubes = {} for x in range(1, n+1): cubes[x] = x**3 sum_a3_b3 = {} for a in range(1, n+1): for b in range(1, n+1): temp = cubes[a]+cubes[b] if temp in sum_a3_b3: sum_a3_b3[temp].append((a, b)) else: sum_a3_b3[temp] = [(a, b)] for c in range(1, n+1): for d in range(1, n+1): sum_c3_d3 = cubes[c] + cubes[d] if sum_c3_d3 in sum_a3_b3: for (a, b) in sum_a3_b3[sum_c3_d3]: out.append((a, b, c, d)) return out . f5_time = %timeit -o f5(50) . 1.97 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . Plain Wow! Going from 6 seconds to about 2 ms! Let&#39;s plot the timings on a log scale to learn more. . %matplotlib inline import matplotlib.pyplot as plt import pandas as pd . s = pd.Series({&#39;Naive (O(N^4))&#39;:f1_time.average, &#39;Naive (O(N^4)) with break&#39;:f2_time.average, &#39;Naive (O(N^4)) with break and precomputing cubes&#39;:f3_time.average, &#39;(O(N^3))&#39;:f4_time.average, &#39;(O(N^2))&#39;:f5_time.average}) . s.plot(kind=&#39;bar&#39;, logy=True) plt.ylabel(&quot;Time&quot;); . Hope this was fun! .",
            "url": "https://nipunbatra.github.io/blog/academia/2018/08/18/Placement-Preparation-2018-1-HashMap.html",
            "relUrl": "/academia/2018/08/18/Placement-Preparation-2018-1-HashMap.html",
            "date": " • Aug 18, 2018"
        }
        
    
  
    
        ,"post17": {
            "title": "Visualising Electricity Access Over Space and Time",
            "content": "In this post, I&#39;ll explore electricity access, i.e. globally what fraction of people have access to electricity. Beyond the goal of finding the electricity access, this post will also serve to illustrate how the coolness coefficient of the Python visualisation ecosystem! . I&#39;ll be using data from World Bank for electricity access. See the image below for the corresponding page. . . Downloading World Bank data . Now, a Python package called wbdata provides a fairly easy way to access World Bank data. I&#39;d be using it to get data in Pandas DataFrame. . %matplotlib inline import pandas as pd import wbdata import matplotlib.pyplot as plt import datetime data_date = (datetime.datetime(1990, 1, 1), datetime.datetime(2016, 1, 1)) df_elec = wbdata.get_data(&quot;EG.ELC.ACCS.ZS&quot;, pandas=True, data_date=data_date) . df_elec.head() . country date Arab World 2016 88.768654 2015 88.517967 2014 88.076774 2013 88.389705 2012 87.288244 Name: value, dtype: float64 . Downloading Geodata and Reading Using GeoPandas . I&#39;d now be downloading shapefile data for different countries. This will help us to spatially plot the data for the different countries. . !wget http://naciscdn.org/naturalearth/10m/cultural/ne_10m_admin_0_countries_lakes.zip . --2018-06-26 15:52:50-- http://naciscdn.org/naturalearth/10m/cultural/ne_10m_admin_0_countries_lakes.zip Resolving naciscdn.org (naciscdn.org)... 146.201.97.163 Connecting to naciscdn.org (naciscdn.org)|146.201.97.163|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 5077755 (4.8M) [application/x-zip-compressed] Saving to: ‘ne_10m_admin_0_countries_lakes.zip’ ne_10m_admin_0_coun 100%[===================&gt;] 4.84M 246KB/s in 22s 2018-06-26 15:53:12 (228 KB/s) - ‘ne_10m_admin_0_countries_lakes.zip’ saved [5077755/5077755] . Extracting shapefile . import zipfile zip_ref = zipfile.ZipFile(&#39;ne_10m_admin_0_countries_lakes.zip&#39;, &#39;r&#39;) zip_ref.extractall(&#39;.&#39;) zip_ref.close() . import geopandas as gpd gdf = gpd.read_file(&#39;ne_10m_admin_0_countries_lakes.shp&#39;)[[&#39;ADM0_A3&#39;, &#39;geometry&#39;]] . gdf.head() . ADM0_A3 geometry . 0 IDN | (POLYGON ((117.7036079039552 4.163414542001791... | . 1 MYS | (POLYGON ((117.7036079039552 4.163414542001791... | . 2 CHL | (POLYGON ((-69.51008875199994 -17.506588197999... | . 3 BOL | POLYGON ((-69.51008875199994 -17.5065881979999... | . 4 PER | (POLYGON ((-69.51008875199994 -17.506588197999... | . Visualising electricity access in 2016 . Getting electricity access data for 2016 . df_2016 = df_elec.unstack()[[&#39;2016&#39;]].dropna() . df_2016.head() . date 2016 . country . Afghanistan 84.137138 | . Albania 100.000000 | . Algeria 99.439568 | . Andorra 100.000000 | . Angola 40.520607 | . In order to visualise electricity access data over the map, we would have to join the GeoPandas object gdf and df_elec . Joining gdf and df_2016 . Now, gdf uses alpha_3 codes for country names like AFG, etc., whereas df_2016 uses country names. We will thus use pycountry package to get code names corresponding to countries in df_2016 as shown in this StackOverflow post. . import pycountry countries = {} for country in pycountry.countries: countries[country.name] = country.alpha_3 codes = [countries.get(country, &#39;Unknown code&#39;) for country in df_2016.index] df_2016[&#39;Code&#39;] = codes . df_2016.head() . date 2016 Code . country . Afghanistan 84.137138 | AFG | . Albania 100.000000 | ALB | . Algeria 99.439568 | DZA | . Andorra 100.000000 | AND | . Angola 40.520607 | AGO | . Now, we can join the two data sources . merged_df_2016 = gpd.GeoDataFrame(pd.merge(gdf, df_2016, left_on=&#39;ADM0_A3&#39;, right_on=&#39;Code&#39;)) . merged_df_2016.head() . ADM0_A3 geometry 2016 Code . 0 IDN | (POLYGON ((117.7036079039552 4.163414542001791... | 97.620000 | IDN | . 1 MYS | (POLYGON ((117.7036079039552 4.163414542001791... | 100.000000 | MYS | . 2 CHL | (POLYGON ((-69.51008875199994 -17.506588197999... | 100.000000 | CHL | . 3 PER | (POLYGON ((-69.51008875199994 -17.506588197999... | 94.851746 | PER | . 4 ARG | (POLYGON ((-68.4486097329999 -52.3466170159999... | 100.000000 | ARG | . Finally plotting! . # Example borrowed from http://ramiro.org/notebook/geopandas-choropleth/ cmap=&#39;OrRd&#39; figsize = (16, 5) ax = merged_df_2016.plot(column=&#39;2016&#39;, cmap=cmap, figsize=figsize,legend=True) title = &#39;Electricity Access(% of population) in {}&#39;.format(&#39;2016&#39;) gdf[~gdf.ADM0_A3.isin(merged_df_2016.ADM0_A3)].plot(ax=ax, color=&#39;#fffafa&#39;, hatch=&#39;///&#39;) ax.set_title(title, fontdict={&#39;fontsize&#39;: 15}, loc=&#39;left&#39;) ax.set_axis_off() . Creating animation for access across time . !mkdir -p elec_access . def save_png_year(year, path=&quot;elec_access&quot;): df_year = df_elec.unstack()[[&#39;{}&#39;.format(year)]].dropna() codes = [countries.get(country, &#39;Unknown code&#39;) for country in df_year.index] df_year[&#39;Code&#39;] = codes merged_df_year = gpd.GeoDataFrame(pd.merge(gdf, df_year, left_on=&#39;ADM0_A3&#39;, right_on=&#39;Code&#39;)) figsize = (16, 5) ax = merged_df_year.plot(column=&#39;{}&#39;.format(year), cmap=cmap, figsize=figsize,legend=True,vmin=0.0, vmax=100.0) title = &#39;Electricity Access(% of population) in {}&#39;.format(year) gdf[~gdf.ADM0_A3.isin(merged_df_year.ADM0_A3)].plot(ax=ax, color=&#39;#fffafa&#39;, hatch=&#39;///&#39;) ax.set_title(title, fontdict={&#39;fontsize&#39;: 15}, loc=&#39;left&#39;) ax.set_axis_off() plt.savefig(&#39;{}/{}.png&#39;.format(path, year), dpi=300) plt.close() . for year in range(1990, 2017): save_png_year(year) . # Borrowed from http://www.kevinwampler.com/blog/2016/09/10/creating-animated-gifs-using-python.html def create_gifv(input_files, output_base_name, fps): import imageio output_extensions = [&quot;gif&quot;] input_filenames = [&#39;elec_access/{}.png&#39;.format(year) for year in range(1990, 2017)] poster_writer = imageio.get_writer(&quot;{}.png&quot;.format(output_base_name), mode=&#39;i&#39;) video_writers = [ imageio.get_writer(&quot;{}.{}&quot;.format(output_base_name, ext), mode=&#39;I&#39;, fps=fps) for ext in output_extensions] is_first = True for filename in input_filenames: img = imageio.imread(filename) for writer in video_writers: writer.append_data(img) if is_first: poster_writer.append_data(img) is_first = False for writer in video_writers + [poster_writer]: writer.close() . create_gifv(&quot;elec_access/*.png&quot;, &quot;electricity_access&quot;, 4) . . Across Africa and SE Asia, one can clearly see a gradual improvement in access! Hope you had fun reading this post :) .",
            "url": "https://nipunbatra.github.io/blog/sustainability/2018/06/26/map-electricity-access.html",
            "relUrl": "/sustainability/2018/06/26/map-electricity-access.html",
            "date": " • Jun 26, 2018"
        }
        
    
  
    
        ,"post18": {
            "title": "Mapping location of air quality sensing in India",
            "content": "In this notebook, I&#39;ll show a quick example of how to use Folium (which internally uses LeafletJS) for visualising the location of air quality monitors in India. The purpose of this notebook is eductional in nature. . Standard Imports . import numpy as np import matplotlib.pyplot as plt import pandas as pd %matplotlib inline . Downloading data from OpenAQ for 2018-04-06 . !wget --no-check-certificate https://openaq-data.s3.amazonaws.com/2018-04-06.csv -P /Users/nipun/Downloads/ . --2020-02-29 17:52:50-- https://openaq-data.s3.amazonaws.com/2018-04-06.csv Resolving openaq-data.s3.amazonaws.com (openaq-data.s3.amazonaws.com)... 52.216.99.123 Connecting to openaq-data.s3.amazonaws.com (openaq-data.s3.amazonaws.com)|52.216.99.123|:443... connected. WARNING: cannot verify openaq-data.s3.amazonaws.com&#39;s certificate, issued by ‘CN=DigiCert Baltimore CA-2 G2,OU=www.digicert.com,O=DigiCert Inc,C=US’: Unable to locally verify the issuer&#39;s authority. HTTP request sent, awaiting response... 200 OK Length: 133839107 (128M) [text/csv] Saving to: ‘/Users/nipun/Downloads/2018-04-06.csv.1’ 2018-04-06.csv.1 37%[======&gt; ] 47.37M 3.79MB/s eta 40s ^C . import pandas as pd df = pd.read_csv(&quot;/Users/nipun/Downloads/2018-04-06.csv&quot;) df = df[(df.country==&#39;IN&#39;)&amp;(df.parameter==&#39;pm25&#39;)].dropna().groupby(&quot;location&quot;).mean() . df . value latitude longitude . location . Adarsh Nagar, Jaipur - RSPCB | 79.916667 | 26.902909 | 75.836853 | . Anand Kala Kshetram, Rajamahendravaram - APPCB | 42.750000 | 16.987287 | 81.736318 | . Ardhali Bazar, Varanasi - UPPCB | 103.666667 | 25.350599 | 82.908307 | . Asanol Court Area, Asanol - WBPCB | 56.833333 | 23.685297 | 86.945968 | . Ashok Nagar, Udaipur - RSPCB | 114.750000 | 24.588617 | 73.632140 | . ... | ... | ... | ... | . Vasundhara, Ghaziabad, UP - UPPCB | 223.333333 | 28.660335 | 77.357256 | . Vikas Sadan, Gurgaon, Haryana - HSPCB | 280.250000 | 28.450124 | 77.026305 | . Vindhyachal STPS, Singrauli - MPPCB | 144.000000 | 24.108970 | 82.645580 | . Ward-32 Bapupara, Siliguri - WBPCB | 195.000000 | 26.688305 | 88.412668 | . Zoo Park, Hyderabad - TSPCB | 82.500000 | 17.349694 | 78.451437 | . 79 rows × 3 columns . Downloading World GeoJson file . !wget --no-check-certificate https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json . --2020-02-29 17:53:17-- https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected. WARNING: cannot verify raw.githubusercontent.com&#39;s certificate, issued by ‘CN=DigiCert SHA2 High Assurance Server CA,OU=www.digicert.com,O=DigiCert Inc,C=US’: Unable to locally verify the issuer&#39;s authority. HTTP request sent, awaiting response... 200 OK Length: 252515 (247K) [text/plain] Saving to: ‘world-countries.json’ world-countries.jso 100%[===================&gt;] 246.60K 376KB/s in 0.7s 2020-02-29 17:53:19 (376 KB/s) - ‘world-countries.json’ saved [252515/252515] . Creating india.json correspdonding to Indian data . import json e = json.load(open(&#39;world-countries.json&#39;,&#39;r&#39;)) json.dump(e[&#39;features&#39;][73], open(&#39;india.json&#39;,&#39;w&#39;)) . import folium folium_map = folium.Map(width = &#39;60%&#39;,height=800,location=[20, 77], zoom_start=5, tiles=&quot;Stamen Terrain&quot;,min_lat=7, max_lat=35, min_lon=73, max_lon=90) for x in df.iterrows(): name = x[0] lat, lon = x[1][&#39;latitude&#39;], x[1][&#39;longitude&#39;] folium.CircleMarker([lat, lon], radius=5, color=&#39;#000000&#39;,fill_color=&#39;#D3D3D3&#39; , fill_opacity=1).add_to(folium_map) folium.GeoJson(&#39;india.json&#39;).add_to(folium_map) . &lt;folium.features.GeoJson at 0x11e497bd0&gt; . folium_map.save(&quot;map.html&quot;) . . There you go!Till next time. .",
            "url": "https://nipunbatra.github.io/blog/air%20quality/2018/06/21/aq-india-map.html",
            "relUrl": "/air%20quality/2018/06/21/aq-india-map.html",
            "date": " • Jun 21, 2018"
        }
        
    
  
    
        ,"post19": {
            "title": "Active Learning",
            "content": "We all get it - AI is the new electricity. Deep neural nets are everywhere around us. But you know what, getting labelled training data can still be a big issue in many domains. This is where active learning comes in - given that we only have a small amount of labelled data, do we randomly get labels for other samples, or can we create a smarter strategy for the same? Active learning deals with the latter. . Various strategies for active learning have been proposed in the past. In this post, I&#39;ll work out a trivial example of what is called query by committee. The key idea is that we create a committee of learners and choose to acquire labels for the unlabelled points for which there is maximum disaggrement amongst the committee. . I&#39;d recommend the new readers to go through this survey. . In this particular post, I&#39;d be looking at active learning via query by committee, where the committee members are trained on different subsets of the train data. In a future post, I&#39;ll write about active learning via query by committee, where the committee members are trained on the same data, but with different parameters. . Standard imports . import numpy as np import matplotlib.pyplot as plt import pandas as pd import warnings warnings.filterwarnings(&#39;ignore&#39;) np.random.seed(0) %matplotlib inline . Creating dataset . X = np.arange(1, 1001, 1) Y = 10*X + 4 + 400* np.random.randn(1000, ) . plt.scatter(X, Y, s=0.1) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) . Text(0, 0.5, &#39;Y&#39;) . Learning a linear regression model on the entire data . from sklearn.linear_model import LinearRegression clf = LinearRegression() . clf.fit(X.reshape(-1,1), Y) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . clf.intercept_ . -10.370897712972692 . clf.coef_ . array([9.99254389]) . Visualising the fit . plt.scatter(X, Y, s=0.1) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.plot(X, clf.coef_[0]*X + clf.intercept_, color=&#39;k&#39;, label=&#39;Best fit on all data&#39;) plt.legend() plt.text(500, clf.coef_[0]*500 + clf.intercept_ +4000, &quot;Y = {0:0.2f} X + {1:0.2f}&quot;.format(clf.coef_[0], clf.intercept_) ) . Text(500, 8985.90104506115, &#39;Y = 9.99 X + -10.37&#39;) . Creating the initial train set, the test set and the pool . from sklearn.model_selection import train_test_split . train_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5) . train_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, test_size=495) . plt.scatter(train_X, train_Y) . &lt;matplotlib.collections.PathCollection at 0x1a211e9150&gt; . Creating a committee each learnt on different subset of the data . committee_size = 5 . train_X_com = {0:{}} train_Y_com = {0:{}} models_com = {0:{}} iteration = 0 for cur_committee in range(committee_size): train_X_com[iteration][cur_committee], _, train_Y_com[iteration][cur_committee], _ = train_test_split(train_X, train_Y, train_size=0.5, random_state=cur_committee) models_com[iteration][cur_committee] = LinearRegression() models_com[iteration][cur_committee].fit(train_X_com[iteration][cur_committee].reshape(-1,1), train_Y_com[iteration][cur_committee]) . Plotting the fit of the committee on the entire dataset . plt.scatter(X, Y, s=0.2) for cur_committee in range(committee_size): plt.plot(X, models_com[0][cur_committee].coef_[0]*X + models_com[0][cur_committee].intercept_, label=&#39;Model {0} nY = {1:0.2f} X + {2:0.2f}&#39;.format(cur_committee, models_com[0][cur_committee].coef_[0], models_com[0][cur_committee].intercept_)) plt.legend() . Evaluate the performance on the test set . estimations_com = {0:{}} for cur_committee in range(committee_size): estimations_com[0][cur_committee] = models_com[0][cur_committee].predict(test_X.reshape(-1, 1)) . test_mae_error = {0:(pd.DataFrame(estimations_com[0]).mean(axis=1) - test_Y).abs().mean()} . The MAE on the test set is: . test_mae_error[0] . 565.8837967341798 . Active learning procedure . num_iterations = 20 points_added_x=[] points_added_y=[] print(&quot;Iteration, Cost n&quot;) print(&quot;-&quot;*40) for iteration in range(1, num_iterations): # For each committee: making predictions on the pool set based on model learnt in the respective train set estimations_pool = {cur_committee: models_com[iteration-1][cur_committee].predict(pool_X.reshape(-1, 1)) for cur_committee in range(committee_size)} # Finding points from the pool with highest disagreement among the committee - highest standard deviation in_var = pd.DataFrame(estimations_pool).std(axis=1).argmax() to_add_x = pool_X[in_var] to_add_y = pool_Y[in_var] points_added_x.append(to_add_x) points_added_y.append(to_add_y) # For each committee - Adding the point where the committe most disagrees for com in range(committee_size): if iteration not in train_X_com: train_X_com[iteration] = {} train_Y_com[iteration] = {} models_com[iteration] = {} train_X_com[iteration][com] = np.append(train_X_com[iteration-1][com], to_add_x) train_Y_com[iteration][com] = np.append(train_Y_com[iteration-1][com], to_add_y) # Deleting the point from the pool pool_X = np.delete(pool_X, in_var) pool_Y = np.delete(pool_Y, in_var) # Training on the new set for each committee for cur_committee in range(committee_size): models_com[iteration][cur_committee] = LinearRegression() models_com[iteration][cur_committee].fit(train_X_com[iteration][cur_committee].reshape(-1,1), train_Y_com[iteration][cur_committee]) estimations_com[iteration] = {} for cur_committee in range(committee_size): estimations_com[iteration][cur_committee] = models_com[iteration][cur_committee].predict(test_X.reshape(-1, 1)) test_mae_error[iteration]=(pd.DataFrame(estimations_com[iteration]).mean(axis=1) - test_Y).abs().mean() print(iteration, (test_mae_error[iteration])) . Iteration, Cost - 1 406.17664898054875 2 402.9897752715986 3 348.45182739054235 4 348.49519515039907 5 349.04197938475716 6 348.68188577804807 7 352.40882668573266 8 373.60417208279864 9 377.25044571705723 10 372.5302143045216 11 335.30243056115603 12 336.6073606660666 13 343.2867837998923 14 347.0491266373306 15 349.7464195274436 16 351.5990833631039 17 349.21957548034976 18 338.8765223206476 19 337.0132510959355 . pd.Series(test_mae_error).plot(style=&#39;ko-&#39;) plt.xlim((-0.5, num_iterations+0.5)) plt.ylabel(&quot;MAE on test set&quot;) plt.xlabel(&quot;# Points Queried&quot;) . Text(0.5, 0, &#39;# Points Queried&#39;) . As expected, the error goes down as we increase the number of points queried . fig, ax = plt.subplots() import os from matplotlib.animation import FuncAnimation plt.rcParams[&#39;animation.ffmpeg_path&#39;] = os.path.expanduser(&#39;/Users/nipun/ffmpeg&#39;) def update(iteration): ax.cla() ax.scatter(X, Y, s=0.2) ax.set_title(&quot;Iteration: {} n MAE = {:0.2f}&quot;.format(iteration, test_mae_error[iteration])) for cur_committee in range(committee_size): ax.plot(X, models_com[iteration][cur_committee].coef_[0]*X + models_com[iteration][cur_committee].intercept_, label=&#39;Model {0} nY = {1:0.2f} X + {2:0.2f}&#39;.format(cur_committee, models_com[iteration][cur_committee].coef_[0], models_com[iteration][cur_committee].intercept_)) ax.scatter(points_added_x[iteration], points_added_y[iteration],s=100, color=&#39;red&#39;) ax.legend() fig.tight_layout() anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=1000) plt.close() . from IPython.display import HTML HTML(anim.to_html5_video()) . Your browser does not support the video tag. From the animation, we can see that how adding a new point to the train set (shown in red) reduces the variation in prediction amongst the different committee members. .",
            "url": "https://nipunbatra.github.io/blog/ml/2018/06/16/active-committee.html",
            "relUrl": "/ml/2018/06/16/active-committee.html",
            "date": " • Jun 16, 2018"
        }
        
    
  
    
        ,"post20": {
            "title": "Signal denoising using RNNs in PyTorch",
            "content": "In this post, I&#39;ll use PyTorch to create a simple Recurrent Neural Network (RNN) for denoising a signal. I started learning RNNs using PyTorch. However, I felt that many of the examples were fairly complex. So, here&#39;s an attempt to create a simple educational example. . Problem description . Given a noisy sine wave as an input, we want to estimate the denoised signal. This is shown in the figure below. . . Customary imports . import numpy as np import math, random import matplotlib.pyplot as plt %matplotlib inline np.random.seed(0) . Creating noisy and denoised signals . Let&#39;s now write functions to cerate a sine wave, add some noise on top of it. This way we&#39;re able to create a noisy verison of the sine wave. . # Generating a clean sine wave def sine(X, signal_freq=60.): return np.sin(2 * np.pi * (X) / signal_freq) # Adding uniform noise def noisy(Y, noise_range=(-0.35, 0.35)): noise = np.random.uniform(noise_range[0], noise_range[1], size=Y.shape) return Y + noise # Create a noisy and clean sine wave def sample(sample_size): random_offset = random.randint(0, sample_size) X = np.arange(sample_size) out = sine(X + random_offset) inp = noisy(out) return inp, out . Let&#39;s now invoke the functions we defined to generate the figure we saw in the problem description. . inp, out = sample(100) plt.plot(inp, label=&#39;Noisy&#39;) plt.plot(out, label =&#39;Denoised&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x106beb828&gt; . Creating dataset . Now, let&#39;s write a simple function to generate a dataset of such noisy and denoised samples. . def create_dataset(n_samples=10000, sample_size=100): data_inp = np.zeros((n_samples, sample_size)) data_out = np.zeros((n_samples, sample_size)) for i in range(n_samples): sample_inp, sample_out = sample(sample_size) data_inp[i, :] = sample_inp data_out[i, :] = sample_out return data_inp, data_out . Now, creating the dataset, and dividing it into train and test set. . data_inp, data_out = create_dataset() train_inp, train_out = data_inp[:8000], data_out[:8000] test_inp, test_out = data_inp[8000:], data_out[8000:] . import torch import torch.nn as nn from torch.autograd import Variable . Creating RNN . We have 1d sine waves, which we want to denoise. Thus, we have input dimension of 1. Let&#39;s create a simple 1-layer RNN with 30 hidden units. . input_dim = 1 hidden_size = 30 num_layers = 1 class CustomRNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomRNN, self).__init__() self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True) self.linear = nn.Linear(hidden_size, output_size, ) self.act = nn.Tanh() def forward(self, x): pred, hidden = self.rnn(x, None) pred = self.act(self.linear(pred)).view(pred.data.shape[0], -1, 1) return pred r= CustomRNN(input_dim, hidden_size, 1) . r . CustomRNN ( (rnn): RNN(1, 30, batch_first=True) (linear): Linear (30 -&gt; 1) (act): Tanh () ) . Training . # Storing predictions per iterations to visualise later predictions = [] optimizer = torch.optim.Adam(r.parameters(), lr=1e-2) loss_func = nn.L1Loss() for t in range(301): hidden = None inp = Variable(torch.Tensor(train_inp.reshape((train_inp.shape[0], -1, 1))), requires_grad=True) out = Variable(torch.Tensor(train_out.reshape((train_out.shape[0], -1, 1))) ) pred = r(inp) optimizer.zero_grad() predictions.append(pred.data.numpy()) loss = loss_func(pred, out) if t%20==0: print(t, loss.data[0]) loss.backward() optimizer.step() . 0 0.5774930715560913 20 0.12028147280216217 40 0.11251863092184067 60 0.10834833979606628 80 0.11243857443332672 100 0.11533079296350479 120 0.09951132535934448 140 0.078636534512043 160 0.08674494177103043 180 0.07217984646558762 200 0.06266186386346817 220 0.05793667957186699 240 0.0723448321223259 260 0.05628745257854462 280 0.050240203738212585 300 0.06297950446605682 . Great. As expected, the loss reduces over time. . Generating prediction on test set . t_inp = Variable(torch.Tensor(test_inp.reshape((test_inp.shape[0], -1, 1))), requires_grad=True) pred_t = r(t_inp) . # Test loss print(loss_func(pred_t, Variable(torch.Tensor(test_out.reshape((test_inp.shape[0], -1, 1))))).data[0]) . 0.06105425953865051 . Visualising sample denoising . sample_num = 23 plt.plot(pred_t[sample_num].data.numpy(), label=&#39;Pred&#39;) plt.plot(test_out[sample_num], label=&#39;GT&#39;) plt.legend() plt.title(&quot;Sample num: {}&quot;.format(sample_num)) . &lt;matplotlib.text.Text at 0x1064675c0&gt; . Bidirectional RNN . Seems reasonably neat to me! If only the first few points were better esimtated. Any idea why they&#39;re not? Maybe, we need a bidirectional RNN? Let&#39;s try one, and I&#39;ll also add dropout to prevent overfitting. . bidirectional = True if bidirectional: num_directions = 2 else: num_directions = 1 class CustomRNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomRNN, self).__init__() self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True, bidirectional=bidirectional, dropout=0.1) self.linear = nn.Linear(hidden_size*num_directions, output_size, ) self.act = nn.Tanh() def forward(self, x): pred, hidden = self.rnn(x, None) pred = self.act(self.linear(pred)).view(pred.data.shape[0], -1, 1) return pred r= CustomRNN(input_dim, hidden_size, 1) r . CustomRNN ( (rnn): RNN(1, 30, batch_first=True, dropout=0.1, bidirectional=True) (linear): Linear (60 -&gt; 1) (act): Tanh () ) . # Storing predictions per iterations to visualise later predictions = [] optimizer = torch.optim.Adam(r.parameters(), lr=1e-2) loss_func = nn.L1Loss() for t in range(301): hidden = None inp = Variable(torch.Tensor(train_inp.reshape((train_inp.shape[0], -1, 1))), requires_grad=True) out = Variable(torch.Tensor(train_out.reshape((train_out.shape[0], -1, 1))) ) pred = r(inp) optimizer.zero_grad() predictions.append(pred.data.numpy()) loss = loss_func(pred, out) if t%20==0: print(t, loss.data[0]) loss.backward() optimizer.step() . 0 0.6825199127197266 20 0.11104971915483475 40 0.07732641696929932 60 0.07210152596235275 80 0.06964801251888275 100 0.06717491149902344 120 0.06266810745000839 140 0.06302479654550552 160 0.05954732000827789 180 0.05402040109038353 200 0.05266999825835228 220 0.06145058199763298 240 0.0500367134809494 260 0.05388529226183891 280 0.053044941276311874 300 0.046826526522636414 . t_inp = Variable(torch.Tensor(test_inp.reshape((test_inp.shape[0], -1, 1))), requires_grad=True) pred_t = r(t_inp) . # Test loss print(loss_func(pred_t, Variable(torch.Tensor(test_out.reshape((test_inp.shape[0], -1, 1))))).data[0]) . 0.050666142255067825 . sample_num = 23 plt.plot(pred_t[sample_num].data.numpy(), label=&#39;Pred&#39;) plt.plot(test_out[sample_num], label=&#39;GT&#39;) plt.legend() plt.title(&quot;Sample num: {}&quot;.format(sample_num)) . &lt;matplotlib.text.Text at 0x126f22710&gt; . Hmm. The estimated signal looks better for the initial few points. But, gets worse for the final few points. Oops! Guess, now the reverse RNN causes problems for its first few points! . From RNNs to GRU . Let&#39;s now replace our RNN with GRU to see if the model improves. . bidirectional = True if bidirectional: num_directions = 2 else: num_directions = 1 class CustomRNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomRNN, self).__init__() self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first=True, bidirectional=bidirectional, dropout=0.1) self.linear = nn.Linear(hidden_size*num_directions, output_size, ) self.act = nn.Tanh() def forward(self, x): pred, hidden = self.rnn(x, None) pred = self.act(self.linear(pred)).view(pred.data.shape[0], -1, 1) return pred r= CustomRNN(input_dim, hidden_size, 1) r . CustomRNN ( (rnn): GRU(1, 30, batch_first=True, dropout=0.1, bidirectional=True) (linear): Linear (60 -&gt; 1) (act): Tanh () ) . # Storing predictions per iterations to visualise later predictions = [] optimizer = torch.optim.Adam(r.parameters(), lr=1e-2) loss_func = nn.L1Loss() for t in range(201): hidden = None inp = Variable(torch.Tensor(train_inp.reshape((train_inp.shape[0], -1, 1))), requires_grad=True) out = Variable(torch.Tensor(train_out.reshape((train_out.shape[0], -1, 1))) ) pred = r(inp) optimizer.zero_grad() predictions.append(pred.data.numpy()) loss = loss_func(pred, out) if t%20==0: print(t, loss.data[0]) loss.backward() optimizer.step() . 0 0.6294281482696533 20 0.11452394723892212 40 0.08548719435930252 60 0.07101015746593475 80 0.05964939296245575 100 0.053830236196517944 120 0.06312716007232666 140 0.04494623467326164 160 0.04309168830513954 180 0.04010637104511261 200 0.035212572664022446 . t_inp = Variable(torch.Tensor(test_inp.reshape((test_inp.shape[0], -1, 1))), requires_grad=True) pred_t = r(t_inp) . # Test loss print(loss_func(pred_t, Variable(torch.Tensor(test_out.reshape((test_inp.shape[0], -1, 1))))).data[0]) . 0.03618593513965607 . sample_num = 23 plt.plot(pred_t[sample_num].data.numpy(), label=&#39;Pred&#39;) plt.plot(test_out[sample_num], label=&#39;GT&#39;) plt.legend() plt.title(&quot;Sample num: {}&quot;.format(sample_num)) . &lt;matplotlib.text.Text at 0x11661e208&gt; . The GRU prediction seems to far better! Maybe, the RNNs suffer from the vanishing gradients problem? . Visualising estimations as model improves . Let&#39;s now write a simple function to visualise the estimations as a function of iterations. We&#39;d expect the estimations to improve over time. . plt.rcParams[&#39;animation.ffmpeg_path&#39;] = &#39;./ffmpeg&#39; from matplotlib.animation import FuncAnimation fig, ax = plt.subplots(figsize=(4, 3)) fig.set_tight_layout(True) # Query the figure&#39;s on-screen size and DPI. Note that when saving the figure to # a file, we need to provide a DPI for that separately. print(&#39;fig size: {0} DPI, size in inches {1}&#39;.format( fig.get_dpi(), fig.get_size_inches())) def update(i): label = &#39;Iteration {0}&#39;.format(i) ax.cla() ax.plot(np.array(predictions)[i, 0, :, 0].T, label=&#39;Pred&#39;) ax.plot(train_out[0, :], label=&#39;GT&#39;) ax.legend() ax.set_title(label) anim = FuncAnimation(fig, update, frames=range(0, 201, 4), interval=20) anim.save(&#39;learning.mp4&#39;,fps=20) plt.close() . fig size: 72.0 DPI, size in inches [ 4. 3.] . from IPython.display import Video Video(&quot;learning.mp4&quot;) . Your browser does not support the video element. This looks great! We can see how our model learns to learn reasonably good denoised signals over time. It doesn&#39;t start great though. Would a better initialisation help? I certainly feel that for this particular problem it would, as predicting the output the same as input is a good starting point! . Bonus: Handling missing values in denoised training data . The trick to handling missing values in the denoised training data (the quantity we wish to estimate) is to compute the loss only over the present values. This requires creating a mask for finding all entries except missing. . One such way to do so would be: mask = out &gt; -1* 1e8 where out is the tensor containing missing values. . Let&#39;s first add some unknown values (np.NaN) in the training output data. . for num_unknown_values in range(50): train_out[np.random.choice(list(range(0, 8000))), np.random.choice(list(range(0, 100)))] = np.NAN . np.isnan(train_out).sum() . 50 . Testing using a network with few parameters. . r= CustomRNN(input_dim, 2, 1) r . CustomRNN ( (rnn): GRU(1, 30, batch_first=True, dropout=0.1, bidirectional=True) (linear): Linear (60 -&gt; 1) (act): Tanh () ) . # Storing predictions per iterations to visualise later predictions = [] optimizer = torch.optim.Adam(r.parameters(), lr=1e-2) loss_func = nn.L1Loss() for t in range(20): hidden = None inp = Variable(torch.Tensor(train_inp.reshape((train_inp.shape[0], -1, 1))), requires_grad=True) out = Variable(torch.Tensor(train_out.reshape((train_out.shape[0], -1, 1))) ) pred = r(inp) optimizer.zero_grad() predictions.append(pred.data.numpy()) # Create a mask to compute loss only on defined quantities mask = out &gt; -1* 1e8 loss = loss_func(pred[mask], out[mask]) if t%20==0: print(t, loss.data[0]) loss.backward() optimizer.step() . 0 0.6575785279273987 . There you go! We&#39;ve also learnt how to handle missing values! . I must thank Simon Wang and his helpful inputs on the PyTorch discussion forum. .",
            "url": "https://nipunbatra.github.io/blog/ml/2018/01/13/denoising.html",
            "relUrl": "/ml/2018/01/13/denoising.html",
            "date": " • Jan 13, 2018"
        }
        
    
  
    
        ,"post21": {
            "title": "CS Ph.D. lessons to my younger self",
            "content": "I completed my CS Ph.D. from IIIT Delhi (Note the three Is) in March 2017. My Ph.D. was a lesson filled journey. Here’s a letter from me to my younger self for CS Ph.D. lessons. . Dear younger self, . As you embark this Ph.D. journey, I wanted to share some lessons and experiences. While I do not claim to be able to follow this 100 %, I am more aware than I was before. I think I might have done a better job at my Ph.D. if I&#39;d taken note of the points I am going to mention now (in no particular order). . Take care of your health . You&#39;re joining fresh after completing your undergraduate studies. You are full of energy. But, your body won&#39;t remain the same. You&#39;ll realise that there is a difference between the early and late 20s. You&#39;ll be debugging your code, and will feel it&#39;ll take only 5 minutes more, and after three hours realise you&#39;re well past your routine, your neck may hurt! Very soon, your body might hunch forward because of long sitting hours staring at your screen, and you may start gaining weight. Remember that when optimising for a happy life, you can&#39;t neglect your health for long. As one of my mentors would say, take time off. Set it on your calendar! Ensure that you get some form of exercise routinely. . By health, I mean not only your physical health but also your mental health. There&#39;ll be times that you&#39;d be exhausted and dejected. No matter what you do, you don&#39;t seem to make any progress. You&#39;ll start developing doubts about your abilities. You&#39;ll start questioning your decision to pursue a Ph.D. Remember that you&#39;re not alone! Taking time off would help. It would also greatly help to have friends to talk and share. Don&#39;t keep all your anxieties and worries to yourself! . Meet well! . You&#39;d be participating in a lot of meetings over the tenure of your Ph.D. Most often you&#39;d be meeting your research advisor. You&#39;d be very excited to discuss every possible detail of your work with your advisor. But, remember, that your advisor probably has some courses to take. They might have a group of students working under them, and have multiple projects running simultaneously. So, you&#39;d need to plan and strategise to make the best use of the meeting time you get with your advisor. Here&#39;s a template that I tried to follow. First, set the context by giving a summary of previous meeting discussion. Setting the context will ensure that you both are on the same page. Present a brief agenda. Then, present the main findings. Have some thoughts on why your method works or does not work. Over the course of your Ph.D., you&#39;ll realise that your advisor is an expert in deducing why something works, and why something doesn&#39;t. It&#39;ll be precious for you to observe your advisor and learn and apply this yourself. In my earlier years, I&#39;d often just go and show all my results to my advisor. You really shouldn&#39;t stop there. Think about what you should conclude from your findings. Come up with specific questions for your advisor. Ensure that you make the best use of the limited meeting time you get with your advisor. . Paper rejection is tough, but not the end of the world . It&#39;s likely that you&#39;ll have a few papers rejected during your Ph.D. I couldn&#39;t believe when I got my first paper rejected. How could it be! In anguish, I felt the reviewer didn&#39;t do a good job. I took it very personally! With more rejections, I got somewhat better at it. I also learned lessons along the way. One of the lessons that stuck with me was to not ponder too much on the reviews for 1-2 days after the paper was rejected. I wasn&#39;t able to take the reviews with an unbiased attitude anyway. After 1-2 days, I would often appreciate the points made by the reviewers. Sometimes, I wouldn&#39;t, but, then reviewing is complex, anyway! It should also be noted that of all my total submissions, only about one-third or so got accepted. Top CS conferences are very competitive, so paper rejections are likely to be more common than not! . Better emails go a long way . You&#39;d be writing a ton of emails during your Ph.D., way more often than meeting in person. It&#39;s important that you learn the art and science of writing better emails. Poorly written emails are less likely to get a response. At the beginning of my Ph.D., I wrote poor emails. The subject line wasn&#39;t indicative of what to expect in the post, which made it incredibly hard to find relevant threads later! I often jumbled up multiple projects/topics in one email. I often wrote very long emails. Many of my emails were a brain dump and not organised well. Eventually, by observing how my advisor and other senior people wrote emails, I improved in this important skill. I think that many of the points about meetings are also applicable to emails. You need to set the context, discuss how you proceeded, and why you did so, summarise the observation, form some conclusions and questions, and if possible be specific on what inputs you need. . Embrace the scientific method . I was an engineer by training before I started my Ph.D. So, I&#39;d be thinking like - &quot;this seems cool. Let&#39;s try this approach. Should be fun to solve&quot;. I went this way for some time, till I had a meeting with a senior faculty. The first question he asked me was - &quot;what&#39;s your hypothesis?&quot;. I stood dumbfounded. I realised that thus far I&#39;d been a solution-first researcher without doing so in a scientific way. I&#39;d try and pick up approaches, if they didn&#39;t work, move on. After a bit of rewiring, I improved my understanding and application of the scientific method. So, my work would now be more structured. I&#39;d be wasting less time on random trails or unimportant problems. The key lesson is to have a checklist (mental or physical) of common pitfalls - like, improving the accuracy of a system from 95% to 99% on an unimportant or extremely contrived problem; or, starting data collection without much thought to the final experiments and analysis you&#39;d plan to conduct to prove your hypothesis; or, the worst of all, having no hypothesis to start off with! . Mid-PhD. crisis time: you&#39;re not alone . I&#39;d heard that every Ph.D. student goes through some crisis period. I thought I&#39;d be an exception. How wrong I was. This period occurred after I&#39;d been academically weaned. My coursework had finished. There was little barring research to keep me busy. Paper rejections stopped surprising me. Ideas not working stopped surprising me. I started questioning if I should quit Ph.D. midway. I started looking at forums on this topic and realised that this problem was common. During this period, I kept in touch with folks who&#39;d completed their PhDs. Everyone suggested that this is normal, and a Ph.D. wouldn&#39;t be so valuable if it weren&#39;t this difficult! I feel that staying in touch with people who could relate to me was important. It was also helpful that despite the so-called inputs not converting to output, my advisors continued encouraging me, meeting regularly, and discussing scientifically correct ways of finding solutions to the Ph.D. problem. Miraculously this phase ended and led to the most successful phase of my Ph.D. where I was able to submit and get accepted a few top-tier conference papers. The key lesson is to stick it out, don&#39;t feel alone or worthless, keep talking to cheerful people and keep reporting progress to your advisor on a regular basis. . It all boils down to addition and subtraction: Blog posts . I won&#39;t lie, I often get intimidated by research papers and the huge amount of techniques in our field. Yes, I still do. One of best teachers at my university spoke - &quot;it all boils down to addition and subtraction.&quot; This has stuck with me. I took a programming and visualisation approach to understanding concepts in my field. At a higher level, I&#39;d be thinking that if I had to teach this concept to someone, how would I go about it. For example, if say, I wanted to study dynamic time warping, I started with some trivial problems where I&#39;d use the concept. On such trivial problems, it would be easy to understand what the algorithm would be doing. I&#39;d often end up writing detailed Jupyter/IPython notebooks showing how the algorithm works, programming and visualsing the various steps. All these formed a part of my technical blog, which I would update on a regular basis. The learning in writing these blog posts was immense. While these blog posts are public, I think I am the biggest beneficiary. Not only does one gain a good understanding of the concept involved, but one also gains confidence about the subject and one&#39;s ability to understand! The key lesson is to document your learnings, understandings, and try to abstract out your specific problem and think of teaching the concept to someone who doesn&#39;t know much about your problem. . Teaching is learning . A teaching assistantship is often dreaded. Why waste my valuable time on it? It turns out, I learned a lot from my TA experiences. I realised that learning with an intention to share the learning ensured that I learned well. I didn&#39;t cut corners. Moreover, with an emphasis on teaching well, I often had to think hard about making concepts relatable. This exercise helped me a lot! In my first semester, I was a TA for an introduction to programming course. Before the course, I thought I knew Python reasonably well. After the course, I realised that now I knew it way better than earlier. Since Python turned out to be the language I did most of my research coding in, it turned out to be a great learning experience. Besides, I made a lot of friends with my students in the course! The key lesson here is somewhat related to the lesson on blog posts. Thinking how to explain stuff usually always helped me get a better understanding! . Good research is only half done! . Talks are a great way to advertise your research and get some good feedback. It&#39;s easy to go very excited and fit everything from the paper into a few slides in a 15-20 minute presentation. This is a great recipe for disaster! Good presentations often leave the audience feeling thankful that they attended the talk. Bad talks ensure that people open their laptops and start answering emails they&#39;ve not replied for ages! A good talk requires preparation. I repeat. A good talk requires preparation. Even if you&#39;re a pro. Over the course of the years, I developed my style learning from my advisors, other faculties, other presenters. There were a lot of lessons involved! One of the key lessons for me was to jot down a script for my slides. While I felt I could mostly do a good job speaking without speaker notes, I think I was better with them! Of course, it took a lot of effort! Another important lesson was to deliver a talk in terms of things the audience could relate with, and thus keeping them engaged. I also maintained that the slides are there to support me and not replace me. Thus, I would never put much text into them! I&#39;d put a lot of efforts in maintaining consistency across slides, using similar figures, conventions. All of this to ensure that the viewer doesn&#39;t lose interest! . Of course, despite all these efforts, I would usually always deliver a pathetic first talk. I was lucky that most of these first pathetic talks came in front of colleagues and not the main audience. So, &quot;Practice! Practice! And practice!&quot; is the mantra. . Volunteer to review papers . Your advisor would likely be reviewing a lot of papers throughout the year. Many of these would probably be in your sub-area of CS. It was an excellent exercise for me when I&#39;d help my advisor review some of the papers. Taking part in the review process makes you appreciate the time and effort put in by the TPC and the conference management team! No one is paid for all this effort! I particularly remember excitedly telling my advisor that I&#39;d champion one of the papers he gave me to review. He smiled and said that paper wouldn&#39;t probably go through. The discussion that followed helped me learn the traits of a good and a bad paper from a reviewer&#39;s perspective. Once you get the hang of it and do the same critical analysis of your paper, you&#39;d be able to improve your paper! . Open source is the right way . CS research often involves writing code for our approach, some baselines, data management and analysis. This set of code leads to the generation of results and analysis in our papers. Now, when I started my Ph.D. and started working on a problem, I thought that it should be trivial to compare our work to previous literature. It turns out; I was wrong. Reproducibility or the act of generating results from previous literature is a huge challenge. Making your code reproducible, such that others can use it for their analysis and paper takes a lot of effort. I felt that it was the right thing to make my code open source and easy for others to use. One of my most cited paper came as a result of introducing more transparency, comparability, and reproducibility. I&#39;m also not surprised that I&#39;m the biggest benefactor by making my code good enough to be made public. The steps I took to make the code more readable, and reproducible, helped me a lot going back to my code to tweak or re-run some experiments. In the age of Github, there aren&#39;t really many excuses for not putting efforts towards - &quot;Let&#39;s make scientific articles comparable again.&quot; . Funding for conferences/PhD scholarships . While we may crib about our stipends not being comparable to industry folks, let&#39;s not forget that a Ph.D. can be very expensive. Coming from India, the travel costs to conferences would be high! Yes, very few of the international conferences I attended happened in India. However, some research labs like MSR and Google and some government agencies provide funding for &quot;good&quot; conferences. Many conferences also provide economic support. I was lucky that I could cut some of the costs for my advisor and department by getting travel fundings. Various organisations also offer Ph.D. fellowships. I was fortunate to receive one from TCS. These fellowships not only allow for some industry mentors but also include financial support. It&#39;s one less thing to worry if we can get some of the finances worked out. The key lesson isn&#39;t a particularly surprising one - apply for fellowship and grants whenever you can! . Good paper writing takes time and practice . Before joining grad school, I was a great writer. After completing it, I could manage some writing. Confused? When I entered grad school did I realise how pathetic my scientific writing skills were. While I&#39;d been speaking and writing English since I was four or so, it wasn&#39;t my first language. The red coloured paper returned by my advisor on my first draft was an eye opener (technically, it was color in LaTeX and not a hard copy!). For someone like me who loves to learn by observation, the process of taking my first draft and seeing it turn into a well-written document courtesy my advisor was wonderful. The key lesson is to observe what makes a good draft. I came up with a structure that I would follow in almost all of my papers: . What is the general problem? | How can we convert it to a CS problem? | What&#39;s the related work and where does it fail to address the problem? | Why would our approach work? and what&#39;s the gist of it? | What&#39;s the experimental setup I&#39;ve used for evaluation? | How well does our approach perform? | In light of above, what can be conclude about the problem given our approach? | . I&#39;d write using such a structure in the abstract and just expand it into different sections in the paper. . Oh, and yes, don&#39;t forget Grammarly and some scripts from Matt Might! Something that really helped me was to run my drafts, even before the final version with my colleagues to get some reviews. . Accept limitations of your work and be honest about them . We&#39;ve all seen those few points on the graph that make our algorithm look bad. Only if we didn&#39;t have those points, our technique would look so amazing! There may be a tendency to &quot;avoid&quot; the limitations. My key learning on this front has been to accept the limitations of my work and be very honest about them. Science loses if we &quot;hide&quot; facts. Rather, it&#39;s the limitations which make things interesting. They force us to go back and review everything. Is this an issue with our approach, or implementation, or dataset, or something more fundamental? Maintaining this integrity was always a top priority! There&#39;ve been instances where paper reviewers have shown appreciation for us being clear and honest about the limitations. . Sometimes it helps to disconnect . We live in an age of data deluge. There are so many forums to network and brand your work. There are so many forums like Reddit and HackerNews to remain on top of the latest in the field. While I tried to remain up to date with the latest, it helped when I would sometimes disconnect. This used to be the way I studied in primary and secondary school. So, if any idea or interesting problem comes to mind, I would sometimes try and think it through before googling it. Similarly, if the code gives some error, I found that my programming and understanding improved if I would spend some time thinking before googling! The key lesson is to think before you search. . Is the grass really greener on the other side . As a Ph.D. student sitting in India, I&#39;d often think how great it would have been to be a Ph.D. student at say MIT, Stanford, or some other top CS university! Maybe, I would have a &quot;better&quot; publication record. Procrastinating in such situations is easy. My key learning in this aspect came from a discussion I had about 15 years back in my high school when my teacher told me that it&#39;s the students who make the school. So, the key lesson was to accept that I may not be from the most well-known school in the world, but nothing stops me from doing world-class research. So, accepting what I had, and trying to change what I could was the main learning. Of course, research is highly collaborative these days. Eventually, by the time I had graduated, I had worked with people from Imperial London, Univ. of Southampton, University of Virginia, UCLA and CMU. . Invest in good hardware . I get it. Our Ph.D. stipends aren&#39;t super amazing. I chose to buy the &quot;best&quot; laptop I could in a reasonable budget. Why bother with Apple-like expensive laptops. I could do with a regular laptop and just install Linux on it. It turns out, I was wrong. Within two years, my laptop had a broken hinge, I had driver issues (on Linux), the laptop wasn&#39;t super light, the battery wasn&#39;t all that great. Then, it took me a hard time to move to a superior laptop. It was expensive. But, it more than made up in terms of the increased productivity. The battery would often last a work day; my back pain got better due to a lighter laptop. I was no longer afraid of updating my system. So, while I am talking about a laptop here, the lesson is more general. The cost of a &quot;good&quot; piece of hardware can be easily recovered many times over in terms of increased productivity. . Take care! . More to come . Networking at conferences, discussion buddy, elevator pitch, attending thesis and faculty job talks.. | .",
            "url": "https://nipunbatra.github.io/blog/academia/2018/01/07/cs-phd-lessons.html",
            "relUrl": "/academia/2018/01/07/cs-phd-lessons.html",
            "date": " • Jan 7, 2018"
        }
        
    
  
    
        ,"post22": {
            "title": "Neural Networks for Collaborative Filtering",
            "content": "Recently, I had a chance to read an interesting WWW 2017 paper entitled: Neural Collaborative Filtering. The first paragraph of the abstract reads as follows: . In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback. . I&#39;d recently written a blog post on using Keras (deep learning library) for implementing traditional matrix factorization based collaborative filtering. So, I thought to get my hands dirty with building a prototype for the paper mentioned above. The authors have already provided their code on Github, which should serve as a reference for the paper and not my post, whose purpose is merely educational! . Here&#39;s how the proposed network architecture looks in the paper: . . There are a few terms that we need to understand: . User (u) and Item (i) are used to create embeddings (low-dimensional) for user and item | Generalized Matrix Factorisation (GMF) combines the two embeddings using the dot product. This is our regular matrix factorisation. | Multi-layer perceptron can also create embeddings for user and items. However, instead of taking a dot product of these to obtain the rating, we can concatenate them to create a feature vector which can be passed on to the further layers. | Neural MF can then combine the predictions from MLP and GMF to obtain the following prediction. | As done in my previous post, I&#39;ll use the MovieLens-100k dataset for illustration. Please refer to my previous post for more details. . Peak into the dataset . import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&#39;ignore&#39;) %matplotlib inline . dataset = pd.read_csv(&quot;/Users/nipun/Downloads/ml-100k/u.data&quot;,sep=&#39; t&#39;,names=&quot;user_id,item_id,rating,timestamp&quot;.split(&quot;,&quot;)) . dataset.head() . user_id item_id rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . So, each record (row) shows the rating for a user, item (movie) pair. It should be noted that I use item and movie interchangeably in this post. . len(dataset.user_id.unique()), len(dataset.item_id.unique()) . (943, 1682) . We assign a unique number between (0, #users) to each user and do the same for movies. . dataset.user_id = dataset.user_id.astype(&#39;category&#39;).cat.codes.values dataset.item_id = dataset.item_id.astype(&#39;category&#39;).cat.codes.values . dataset.head() . user_id item_id rating timestamp . 0 195 | 241 | 3 | 881250949 | . 1 185 | 301 | 3 | 891717742 | . 2 21 | 376 | 1 | 878887116 | . 3 243 | 50 | 2 | 880606923 | . 4 165 | 345 | 1 | 886397596 | . Train test split . We&#39;ll now split our dataset of 100k ratings into train (containing 80k ratings) and test (containing 20k ratings). Given the train set, we&#39;d like to accurately estimate the ratings in the test set. . from sklearn.model_selection import train_test_split train, test = train_test_split(dataset, test_size=0.2) . train.head() . user_id item_id rating timestamp . 13185 71 | 95 | 5 | 880037203 | . 23391 144 | 509 | 4 | 882181859 | . 90744 895 | 50 | 2 | 887159951 | . 3043 255 | 279 | 5 | 882151167 | . 8932 55 | 94 | 4 | 892683274 | . test.head() y_true = test.rating . Creating the model . import keras n_latent_factors_user = 8 n_latent_factors_movie = 10 n_latent_factors_mf = 3 n_users, n_movies = len(dataset.user_id.unique()), len(dataset.item_id.unique()) movie_input = keras.layers.Input(shape=[1],name=&#39;Item&#39;) movie_embedding_mlp = keras.layers.Embedding(n_movies + 1, n_latent_factors_movie, name=&#39;Movie-Embedding-MLP&#39;)(movie_input) movie_vec_mlp = keras.layers.Flatten(name=&#39;FlattenMovies-MLP&#39;)(movie_embedding_mlp) movie_vec_mlp = keras.layers.Dropout(0.2)(movie_vec_mlp) movie_embedding_mf = keras.layers.Embedding(n_movies + 1, n_latent_factors_mf, name=&#39;Movie-Embedding-MF&#39;)(movie_input) movie_vec_mf = keras.layers.Flatten(name=&#39;FlattenMovies-MF&#39;)(movie_embedding_mf) movie_vec_mf = keras.layers.Dropout(0.2)(movie_vec_mf) user_input = keras.layers.Input(shape=[1],name=&#39;User&#39;) user_vec_mlp = keras.layers.Flatten(name=&#39;FlattenUsers-MLP&#39;)(keras.layers.Embedding(n_users + 1, n_latent_factors_user,name=&#39;User-Embedding-MLP&#39;)(user_input)) user_vec_mlp = keras.layers.Dropout(0.2)(user_vec_mlp) user_vec_mf = keras.layers.Flatten(name=&#39;FlattenUsers-MF&#39;)(keras.layers.Embedding(n_users + 1, n_latent_factors_mf,name=&#39;User-Embedding-MF&#39;)(user_input)) user_vec_mf = keras.layers.Dropout(0.2)(user_vec_mf) concat = keras.layers.merge([movie_vec_mlp, user_vec_mlp], mode=&#39;concat&#39;,name=&#39;Concat&#39;) concat_dropout = keras.layers.Dropout(0.2)(concat) dense = keras.layers.Dense(200,name=&#39;FullyConnected&#39;)(concat_dropout) dense_batch = keras.layers.BatchNormalization(name=&#39;Batch&#39;)(dense) dropout_1 = keras.layers.Dropout(0.2,name=&#39;Dropout-1&#39;)(dense_batch) dense_2 = keras.layers.Dense(100,name=&#39;FullyConnected-1&#39;)(dropout_1) dense_batch_2 = keras.layers.BatchNormalization(name=&#39;Batch-2&#39;)(dense_2) dropout_2 = keras.layers.Dropout(0.2,name=&#39;Dropout-2&#39;)(dense_batch_2) dense_3 = keras.layers.Dense(50,name=&#39;FullyConnected-2&#39;)(dropout_2) dense_4 = keras.layers.Dense(20,name=&#39;FullyConnected-3&#39;, activation=&#39;relu&#39;)(dense_3) pred_mf = keras.layers.merge([movie_vec_mf, user_vec_mf], mode=&#39;dot&#39;,name=&#39;Dot&#39;) pred_mlp = keras.layers.Dense(1, activation=&#39;relu&#39;,name=&#39;Activation&#39;)(dense_4) combine_mlp_mf = keras.layers.merge([pred_mf, pred_mlp], mode=&#39;concat&#39;,name=&#39;Concat-MF-MLP&#39;) result_combine = keras.layers.Dense(100,name=&#39;Combine-MF-MLP&#39;)(combine_mlp_mf) deep_combine = keras.layers.Dense(100,name=&#39;FullyConnected-4&#39;)(result_combine) result = keras.layers.Dense(1,name=&#39;Prediction&#39;)(deep_combine) model = keras.Model([user_input, movie_input], result) opt = keras.optimizers.Adam(lr =0.01) model.compile(optimizer=&#39;adam&#39;,loss= &#39;mean_absolute_error&#39;) . Using TensorFlow backend. . Let&#39;s now see how our model looks like: . from IPython.display import SVG from keras.utils.vis_utils import model_to_dot SVG(model_to_dot(model, show_shapes=False, show_layer_names=True, rankdir=&#39;HB&#39;).create(prog=&#39;dot&#39;, format=&#39;svg&#39;)) . G 4515113056 Item: InputLayer 4515113392 Movie-Embedding-MLP: Embedding 4515113056-&gt;4515113392 112084624776 Movie-Embedding-MF: Embedding 4515113056-&gt;112084624776 4408641744 User: InputLayer 112085071184 User-Embedding-MLP: Embedding 4408641744-&gt;112085071184 112085982960 User-Embedding-MF: Embedding 4408641744-&gt;112085982960 112084623992 FlattenMovies-MLP: Flatten 4515113392-&gt;112084623992 4378375728 FlattenUsers-MLP: Flatten 112085071184-&gt;4378375728 4515113224 dropout_1: Dropout 112084623992-&gt;4515113224 112085499184 dropout_3: Dropout 4378375728-&gt;112085499184 112085917424 Concat: Merge 4515113224-&gt;112085917424 112085499184-&gt;112085917424 112085764920 dropout_5: Dropout 112085917424-&gt;112085764920 112086436832 FullyConnected: Dense 112085764920-&gt;112086436832 112086434816 Batch: BatchNormalization 112086436832-&gt;112086434816 112086597360 Dropout-1: Dropout 112086434816-&gt;112086597360 112086994000 FullyConnected-1: Dense 112086597360-&gt;112086994000 112086761144 Batch-2: BatchNormalization 112086994000-&gt;112086761144 112087744464 Dropout-2: Dropout 112086761144-&gt;112087744464 4399310888 FlattenMovies-MF: Flatten 112084624776-&gt;4399310888 4407942728 FlattenUsers-MF: Flatten 112085982960-&gt;4407942728 112087744128 FullyConnected-2: Dense 112087744464-&gt;112087744128 112084624160 dropout_2: Dropout 4399310888-&gt;112084624160 112085423664 dropout_4: Dropout 4407942728-&gt;112085423664 112087225176 FullyConnected-3: Dense 112087744128-&gt;112087225176 112088890336 Dot: Merge 112084624160-&gt;112088890336 112085423664-&gt;112088890336 112089386176 Activation: Dense 112087225176-&gt;112089386176 112089474160 Concat-MF-MLP: Merge 112088890336-&gt;112089474160 112089386176-&gt;112089474160 112089888696 Combine-MF-MLP: Dense 112089474160-&gt;112089888696 112089888752 FullyConnected-4: Dense 112089888696-&gt;112089888752 112089753920 Prediction: Dense 112089888752-&gt;112089753920 So, it wasn&#39;t very complicated to set up. Courtesy Keras, we can do even more complex stuff! . model.summary() . __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== Item (InputLayer) (None, 1) 0 __________________________________________________________________________________________________ User (InputLayer) (None, 1) 0 __________________________________________________________________________________________________ Movie-Embedding-MLP (Embedding) (None, 1, 10) 16830 Item[0][0] __________________________________________________________________________________________________ User-Embedding-MLP (Embedding) (None, 1, 8) 7552 User[0][0] __________________________________________________________________________________________________ FlattenMovies-MLP (Flatten) (None, 10) 0 Movie-Embedding-MLP[0][0] __________________________________________________________________________________________________ FlattenUsers-MLP (Flatten) (None, 8) 0 User-Embedding-MLP[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 10) 0 FlattenMovies-MLP[0][0] __________________________________________________________________________________________________ dropout_3 (Dropout) (None, 8) 0 FlattenUsers-MLP[0][0] __________________________________________________________________________________________________ Concat (Merge) (None, 18) 0 dropout_1[0][0] dropout_3[0][0] __________________________________________________________________________________________________ dropout_5 (Dropout) (None, 18) 0 Concat[0][0] __________________________________________________________________________________________________ FullyConnected (Dense) (None, 200) 3800 dropout_5[0][0] __________________________________________________________________________________________________ Batch (BatchNormalization) (None, 200) 800 FullyConnected[0][0] __________________________________________________________________________________________________ Dropout-1 (Dropout) (None, 200) 0 Batch[0][0] __________________________________________________________________________________________________ FullyConnected-1 (Dense) (None, 100) 20100 Dropout-1[0][0] __________________________________________________________________________________________________ Batch-2 (BatchNormalization) (None, 100) 400 FullyConnected-1[0][0] __________________________________________________________________________________________________ Movie-Embedding-MF (Embedding) (None, 1, 3) 5049 Item[0][0] __________________________________________________________________________________________________ User-Embedding-MF (Embedding) (None, 1, 3) 2832 User[0][0] __________________________________________________________________________________________________ Dropout-2 (Dropout) (None, 100) 0 Batch-2[0][0] __________________________________________________________________________________________________ FlattenMovies-MF (Flatten) (None, 3) 0 Movie-Embedding-MF[0][0] __________________________________________________________________________________________________ FlattenUsers-MF (Flatten) (None, 3) 0 User-Embedding-MF[0][0] __________________________________________________________________________________________________ FullyConnected-2 (Dense) (None, 50) 5050 Dropout-2[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 3) 0 FlattenMovies-MF[0][0] __________________________________________________________________________________________________ dropout_4 (Dropout) (None, 3) 0 FlattenUsers-MF[0][0] __________________________________________________________________________________________________ FullyConnected-3 (Dense) (None, 20) 1020 FullyConnected-2[0][0] __________________________________________________________________________________________________ Dot (Merge) (None, 1) 0 dropout_2[0][0] dropout_4[0][0] __________________________________________________________________________________________________ Activation (Dense) (None, 1) 21 FullyConnected-3[0][0] __________________________________________________________________________________________________ Concat-MF-MLP (Merge) (None, 2) 0 Dot[0][0] Activation[0][0] __________________________________________________________________________________________________ Combine-MF-MLP (Dense) (None, 100) 300 Concat-MF-MLP[0][0] __________________________________________________________________________________________________ FullyConnected-4 (Dense) (None, 100) 10100 Combine-MF-MLP[0][0] __________________________________________________________________________________________________ Prediction (Dense) (None, 1) 101 FullyConnected-4[0][0] ================================================================================================== Total params: 73,955 Trainable params: 73,355 Non-trainable params: 600 __________________________________________________________________________________________________ . We can see that the number of parameters is more than what we had in the Matrix Factorisation case. Let&#39;s see how this model works. I&#39;ll run it for more epochs given that we have more parameters. . history = model.fit([train.user_id, train.item_id], train.rating, epochs=25, verbose=0, validation_split=0.1) . Prediction performance of Neural Network based recommender system . from sklearn.metrics import mean_absolute_error y_hat_2 = np.round(model.predict([test.user_id, test.item_id]),0) print(mean_absolute_error(y_true, y_hat_2)) print(mean_absolute_error(y_true, model.predict([test.user_id, test.item_id]))) . 0.716 0.737380115688 . Pretty similar to the result we got using matrix factorisation. This isn&#39;t very optimised, and I am sure doing so, we can make this approach perform much better than GMF! . Thanks for reading. This post has been a good learning experience for me. Hope you enjoyed too! .",
            "url": "https://nipunbatra.github.io/blog/ml/2017/12/29/neural-collaborative-filtering.html",
            "relUrl": "/ml/2017/12/29/neural-collaborative-filtering.html",
            "date": " • Dec 29, 2017"
        }
        
    
  
    
        ,"post23": {
            "title": "Recommender Systems in Keras",
            "content": "I have written a few posts earlier about matrix factorisation using various Python libraries. The main application I had in mind for matrix factorisation was recommender systems. In this post, I&#39;ll write about using Keras for creating recommender systems. Various people have written excellent similar posts and code that I draw a lot of inspiration from, and give them their credit! I&#39;m assuming that a reader has some experience with Keras, as this post is not intended to be an introduction to Keras. . Specifically, in this post, I&#39;ll talk about: . Matrix Factorisation in Keras | Adding non-negativitiy constraints to solve non-negative matrix factorisation (NNMF) | Using neural networks for recommendations | I&#39;ll be using the Movielens-100k dataset for illustration. There are 943 users and 1682 movies. In total there are a 100k ratings in the dataset. It should be noted that the max. total number of rating for the &lt;users, movies&gt; would be 943*1682, which means that we have about 7% of the total ratings! All rating are on a scale of 1-5. . Task . Given this set of ratings, can we recommend the next set of movies to a user? This would translate to: for every user, estimating the ratings for all the movies that (s)he hasn&#39;t watched and maybe recommend the top-k movies by the esimtated ratings! . Peak into the dataset . import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&#39;ignore&#39;) %matplotlib inline . dataset = pd.read_csv(&quot;/Users/nipun/Downloads/ml-100k/u.data&quot;,sep=&#39; t&#39;,names=&quot;user_id,item_id,rating,timestamp&quot;.split(&quot;,&quot;)) . dataset.head() . user_id item_id rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . So, each record (row) shows the rating for a user, item (movie) pair. It should be noted that I use item and movie interchangeably in this post. . len(dataset.user_id.unique()), len(dataset.item_id.unique()) . (943, 1682) . We assign a unique number between (0, #users) to each user and do the same for movies. . dataset.user_id = dataset.user_id.astype(&#39;category&#39;).cat.codes.values dataset.item_id = dataset.item_id.astype(&#39;category&#39;).cat.codes.values . dataset.head() . user_id item_id rating timestamp . 0 195 | 241 | 3 | 881250949 | . 1 185 | 301 | 3 | 891717742 | . 2 21 | 376 | 1 | 878887116 | . 3 243 | 50 | 2 | 880606923 | . 4 165 | 345 | 1 | 886397596 | . Train test split . We&#39;ll now split our dataset of 100k ratings into train (containing 80k ratings) and test (containing 20k ratings). Given the train set, we&#39;d like to accurately estimate the ratings in the test set. . from sklearn.model_selection import train_test_split train, test = train_test_split(dataset, test_size=0.2) . train.head() . user_id item_id rating timestamp . 90092 832 | 12 | 2 | 875036139 | . 50879 94 | 132 | 3 | 888954341 | . 67994 436 | 12 | 4 | 880141129 | . 49769 710 | 344 | 4 | 884485683 | . 11032 121 | 736 | 4 | 879270874 | . test.head() . user_id item_id rating timestamp . 89284 907 | 493 | 3 | 879723046 | . 60499 550 | 25 | 4 | 892785056 | . 11090 373 | 222 | 5 | 880394520 | . 36096 199 | 140 | 4 | 884129346 | . 21633 71 | 317 | 5 | 880037702 | . Matrix factorisation . One popular recommender systems approach is called Matrix Factorisation. It works on the principle that we can learn a low-dimensional representation (embedding) of user and movie. For example, for each movie, we can have how much action it has, how long it is, and so on. For each user, we can encode how much they like action, or how much they like long movies, etc. Thus, we can combine the user and the movie embeddings to estimate the ratings on unseen movies. This approach can also be viewed as: given a matrix (A [M X N]) containing users and movies, we want to estimate low dimensional matrices (W [M X k] and H [M X k]), such that: $A approx W.H^T$ . Matrix factorisation in Keras . We&#39;ll now write some code to solve the recommendation problem by matrix factorisation in Keras. We&#39;re trying to learn two low-dimensional embeddings of users and items. . import keras from IPython.display import SVG from keras.optimizers import Adam from keras.utils.vis_utils import model_to_dot n_users, n_movies = len(dataset.user_id.unique()), len(dataset.item_id.unique()) n_latent_factors = 3 . Using TensorFlow backend. . The key thing is to learn an embedding for movies and users, and then combine them using the dot product! For estimating the rating, for each user, movie pair of interest, we&#39;d take the dot product of the respective user and item embedding. As an example, if we have 2 dimensions in our user and item embedding, which say correspond to [how much user likes action, how much user likes long movies], and the item embedding is [how much action is in the movie, how long is the movie]. Then, we can predict for a user u, and movie m as how much u likes action $ times$ how much action is there in m $+$ how much u likes long movies $ times$ how long is m. . Our model would optimise the emebedding such that we minimise the mean squared error on the ratings from the train set. . movie_input = keras.layers.Input(shape=[1],name=&#39;Item&#39;) movie_embedding = keras.layers.Embedding(n_movies + 1, n_latent_factors, name=&#39;Movie-Embedding&#39;)(movie_input) movie_vec = keras.layers.Flatten(name=&#39;FlattenMovies&#39;)(movie_embedding) user_input = keras.layers.Input(shape=[1],name=&#39;User&#39;) user_vec = keras.layers.Flatten(name=&#39;FlattenUsers&#39;)(keras.layers.Embedding(n_users + 1, n_latent_factors,name=&#39;User-Embedding&#39;)(user_input)) prod = keras.layers.merge([movie_vec, user_vec], mode=&#39;dot&#39;,name=&#39;DotProduct&#39;) model = keras.Model([user_input, movie_input], prod) model.compile(&#39;adam&#39;, &#39;mean_squared_error&#39;) . Here&#39;s a visualisation of our model for a better understanding. . SVG(model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir=&#39;HB&#39;).create(prog=&#39;dot&#39;, format=&#39;svg&#39;)) . G 4651743104 Item: InputLayer input: output: (None, 1) (None, 1) 4651743216 Movie-Embedding: Embedding input: output: (None, 1) (None, 1, 3) 4651743104-&gt;4651743216 4651744392 User: InputLayer input: output: (None, 1) (None, 1) 4651743888 User-Embedding: Embedding input: output: (None, 1) (None, 1, 3) 4651744392-&gt;4651743888 4651744000 FlattenMovies: Flatten input: output: (None, 1, 3) (None, 3) 4651743216-&gt;4651744000 4468062472 FlattenUsers: Flatten input: output: (None, 1, 3) (None, 3) 4651743888-&gt;4468062472 4651881696 DotProduct: Merge input: output: [(None, 3), (None, 3)] (None, 1) 4651744000-&gt;4651881696 4468062472-&gt;4651881696 We can see that in the Merge layer, we take the dot product of the user and the item embeddings to obtain the rating. . We can also summarise our model as follows: . model.summary() . __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== Item (InputLayer) (None, 1) 0 __________________________________________________________________________________________________ User (InputLayer) (None, 1) 0 __________________________________________________________________________________________________ Movie-Embedding (Embedding) (None, 1, 3) 5049 Item[0][0] __________________________________________________________________________________________________ User-Embedding (Embedding) (None, 1, 3) 2832 User[0][0] __________________________________________________________________________________________________ FlattenMovies (Flatten) (None, 3) 0 Movie-Embedding[0][0] __________________________________________________________________________________________________ FlattenUsers (Flatten) (None, 3) 0 User-Embedding[0][0] __________________________________________________________________________________________________ DotProduct (Merge) (None, 1) 0 FlattenMovies[0][0] FlattenUsers[0][0] ================================================================================================== Total params: 7,881 Trainable params: 7,881 Non-trainable params: 0 __________________________________________________________________________________________________ . So, we have 7881 parameters to learn! Let&#39;s train our model now! . history = model.fit([train.user_id, train.item_id], train.rating, epochs=100, verbose=0) . Train error v/s epoch number . Before we test how well our model does in the test setting, we can visualise the train loss with epoch number. . pd.Series(history.history[&#39;loss&#39;]).plot(logy=True) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Train Error&quot;) . &lt;matplotlib.text.Text at 0x1155a07b8&gt; . Prediction error . Let&#39;s now see how our model does! I&#39;ll do a small post-processing step to round off our prediction to the nearest integer. This is usually not done, and thus just a whimsical step, since the training ratings are all integers! There are better ways to encode this intger requirement (one-hot encoding!), but we won&#39;t discuss them in this post. . y_hat = np.round(model.predict([test.user_id, test.item_id]),0) y_true = test.rating . from sklearn.metrics import mean_absolute_error mean_absolute_error(y_true, y_hat) . 0.6915 . Not bad! We&#39;re able to get a $MAE$ of 0.69! I&#39;m sure with a bit of parameter/hyper-parameter optimisation, we may be able to improve the results. However, I won&#39;t talk about these optimisations in this post. . Extracting the learnt embeddings . We can extract the learnt movie and item embeddings as follows: . movie_embedding_learnt = model.get_layer(name=&#39;Movie-Embedding&#39;).get_weights()[0] pd.DataFrame(movie_embedding_learnt).describe() . 0 1 2 . count 1683.000000 | 1683.000000 | 1683.000000 | . mean -0.935420 | 0.857862 | 0.954169 | . std 0.517458 | 0.447439 | 0.458095 | . min -2.524487 | -0.459752 | -0.989537 | . 25% -1.323431 | 0.546364 | 0.642444 | . 50% -0.949188 | 0.851243 | 0.993619 | . 75% -0.550862 | 1.159588 | 1.283555 | . max 0.500618 | 2.140607 | 2.683658 | . user_embedding_learnt = model.get_layer(name=&#39;User-Embedding&#39;).get_weights()[0] pd.DataFrame(user_embedding_learnt).describe() . 0 1 2 . count 944.000000 | 944.000000 | 944.000000 | . mean -1.126231 | 1.171609 | 1.109131 | . std 0.517478 | 0.409016 | 0.548384 | . min -2.883226 | -0.500010 | -0.415373 | . 25% -1.458197 | 0.903574 | 0.735729 | . 50% -1.159480 | 1.199517 | 1.084089 | . 75% -0.836746 | 1.456610 | 1.468611 | . max 0.899436 | 2.605330 | 2.826109 | . We can see that both the user and the item embeddings have negative elements. There are some applications which require that the learnt embeddings be non-negative. This approach is also called non-negative matrix factorisation, which we&#39;ll workout now. . Non-negative Matrix factorisation (NNMF) in Keras . The code for NNMF remains exactly the same as the code for matrix factorisation. The only change is that we add non-negativity constraints on the learnt embeddings. This is done as follows: . from keras.constraints import non_neg movie_input = keras.layers.Input(shape=[1],name=&#39;Item&#39;) movie_embedding = keras.layers.Embedding(n_movies + 1, n_latent_factors, name=&#39;NonNegMovie-Embedding&#39;, embeddings_constraint=non_neg())(movie_input) movie_vec = keras.layers.Flatten(name=&#39;FlattenMovies&#39;)(movie_embedding) user_input = keras.layers.Input(shape=[1],name=&#39;User&#39;) user_vec = keras.layers.Flatten(name=&#39;FlattenUsers&#39;)(keras.layers.Embedding(n_users + 1, n_latent_factors,name=&#39;NonNegUser-Embedding&#39;,embeddings_constraint=non_neg())(user_input)) prod = keras.layers.merge([movie_vec, user_vec], mode=&#39;dot&#39;,name=&#39;DotProduct&#39;) model = keras.Model([user_input, movie_input], prod) model.compile(&#39;adam&#39;, &#39;mean_squared_error&#39;) . We now verify if we are indeed able to learn non-negative embeddings. I&#39;ll not compare the performance of NNMF on the test set, in the interest of space. . history_nonneg = model.fit([train.user_id, train.item_id], train.rating, epochs=10, verbose=0) . movie_embedding_learnt = model.get_layer(name=&#39;NonNegMovie-Embedding&#39;).get_weights()[0] pd.DataFrame(movie_embedding_learnt).describe() . 0 1 2 . count 1683.000000 | 1683.000000 | 1683.000000 | . mean 0.838450 | 0.840330 | 0.838066 | . std 0.301618 | 0.301529 | 0.301040 | . min -0.000000 | -0.000000 | -0.000000 | . 25% 0.657749 | 0.663951 | 0.656453 | . 50% 0.901495 | 0.904192 | 0.895934 | . 75% 1.072706 | 1.073591 | 1.072926 | . max 1.365719 | 1.379006 | 1.373672 | . Looks good! . Neural networks for recommendation . We&#39;ll now create a simple neural network for recommendation, or for estimating rating! This model is very similar to the earlier matrix factorisation models, but differs in the following ways: . Instead of taking a dot product of the user and the item embedding, we concatenate them and use them as features for our neural network. Thus, we are not constrained to the dot product way of combining the embeddings, and can learn complex non-linear relationships. | Due to #1, we can now have a different dimension of user and item embeddings. This can be useful if one dimension is larger than the other. | n_latent_factors_user = 5 n_latent_factors_movie = 8 movie_input = keras.layers.Input(shape=[1],name=&#39;Item&#39;) movie_embedding = keras.layers.Embedding(n_movies + 1, n_latent_factors_movie, name=&#39;Movie-Embedding&#39;)(movie_input) movie_vec = keras.layers.Flatten(name=&#39;FlattenMovies&#39;)(movie_embedding) movie_vec = keras.layers.Dropout(0.2)(movie_vec) user_input = keras.layers.Input(shape=[1],name=&#39;User&#39;) user_vec = keras.layers.Flatten(name=&#39;FlattenUsers&#39;)(keras.layers.Embedding(n_users + 1, n_latent_factors_user,name=&#39;User-Embedding&#39;)(user_input)) user_vec = keras.layers.Dropout(0.2)(user_vec) concat = keras.layers.merge([movie_vec, user_vec], mode=&#39;concat&#39;,name=&#39;Concat&#39;) concat_dropout = keras.layers.Dropout(0.2)(concat) dense = keras.layers.Dense(200,name=&#39;FullyConnected&#39;)(concat) dropout_1 = keras.layers.Dropout(0.2,name=&#39;Dropout&#39;)(dense) dense_2 = keras.layers.Dense(100,name=&#39;FullyConnected-1&#39;)(concat) dropout_2 = keras.layers.Dropout(0.2,name=&#39;Dropout&#39;)(dense_2) dense_3 = keras.layers.Dense(50,name=&#39;FullyConnected-2&#39;)(dense_2) dropout_3 = keras.layers.Dropout(0.2,name=&#39;Dropout&#39;)(dense_3) dense_4 = keras.layers.Dense(20,name=&#39;FullyConnected-3&#39;, activation=&#39;relu&#39;)(dense_3) result = keras.layers.Dense(1, activation=&#39;relu&#39;,name=&#39;Activation&#39;)(dense_4) adam = Adam(lr=0.005) model = keras.Model([user_input, movie_input], result) model.compile(optimizer=adam,loss= &#39;mean_absolute_error&#39;) . Let&#39;s now see how our model looks like: . SVG(model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir=&#39;HB&#39;).create(prog=&#39;dot&#39;, format=&#39;svg&#39;)) . G 112307868840 Item: InputLayer input: output: (None, 1) (None, 1) 112308383136 Movie-Embedding: Embedding input: output: (None, 1) (None, 1, 8) 112307868840-&gt;112308383136 4659651864 User: InputLayer input: output: (None, 1) (None, 1) 112310319536 User-Embedding: Embedding input: output: (None, 1) (None, 1, 5) 4659651864-&gt;112310319536 112308383416 FlattenMovies: Flatten input: output: (None, 1, 8) (None, 8) 112308383136-&gt;112308383416 112307982232 FlattenUsers: Flatten input: output: (None, 1, 5) (None, 5) 112310319536-&gt;112307982232 112308313840 dropout_1: Dropout input: output: (None, 8) (None, 8) 112308383416-&gt;112308313840 112310320768 dropout_2: Dropout input: output: (None, 5) (None, 5) 112307982232-&gt;112310320768 4659651360 Concat: Merge input: output: [(None, 8), (None, 5)] (None, 13) 112308313840-&gt;4659651360 112310320768-&gt;4659651360 112308749368 FullyConnected-1: Dense input: output: (None, 13) (None, 100) 4659651360-&gt;112308749368 112310118104 FullyConnected-2: Dense input: output: (None, 100) (None, 50) 112308749368-&gt;112310118104 4653345424 FullyConnected-3: Dense input: output: (None, 50) (None, 20) 112310118104-&gt;4653345424 4653179904 Activation: Dense input: output: (None, 20) (None, 1) 4653345424-&gt;4653179904 It should be noted that we use a different number of embeddings for user (3) and items (5)! These combine to form a vector of length (5+3 = 8), which is then fed into the neural network. We also add a dropout layer to prevent overfitting! . model.summary() . __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== Item (InputLayer) (None, 1) 0 __________________________________________________________________________________________________ User (InputLayer) (None, 1) 0 __________________________________________________________________________________________________ Movie-Embedding (Embedding) (None, 1, 8) 13464 Item[0][0] __________________________________________________________________________________________________ User-Embedding (Embedding) (None, 1, 5) 4720 User[0][0] __________________________________________________________________________________________________ FlattenMovies (Flatten) (None, 8) 0 Movie-Embedding[0][0] __________________________________________________________________________________________________ FlattenUsers (Flatten) (None, 5) 0 User-Embedding[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 8) 0 FlattenMovies[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 5) 0 FlattenUsers[0][0] __________________________________________________________________________________________________ Concat (Merge) (None, 13) 0 dropout_1[0][0] dropout_2[0][0] __________________________________________________________________________________________________ FullyConnected-1 (Dense) (None, 100) 1400 Concat[0][0] __________________________________________________________________________________________________ FullyConnected-2 (Dense) (None, 50) 5050 FullyConnected-1[0][0] __________________________________________________________________________________________________ FullyConnected-3 (Dense) (None, 20) 1020 FullyConnected-2[0][0] __________________________________________________________________________________________________ Activation (Dense) (None, 1) 21 FullyConnected-3[0][0] ================================================================================================== Total params: 25,675 Trainable params: 25,675 Non-trainable params: 0 __________________________________________________________________________________________________ . We can see that the number of parameters is more than what we had in the Matrix Factorisation case. Let&#39;s see how this model works. I&#39;ll run it for more epochs given that we have more parameters. . history = model.fit([train.user_id, train.item_id], train.rating, epochs=250, verbose=0) . Prediction performance of Neural Network based recommender system . y_hat_2 = np.round(model.predict([test.user_id, test.item_id]),0) print(mean_absolute_error(y_true, y_hat_2)) print(mean_absolute_error(y_true, model.predict([test.user_id, test.item_id]))) . 0.6957 0.708807692927 . Pretty similar to the result we got using matrix factorisation. Maybe, we need to tweak around a lot more with the neural network to get better results? . Thanks for reading. This post has been a good learning experience for me. Hope you enjoyed too! .",
            "url": "https://nipunbatra.github.io/blog/ml/2017/12/18/recommend-keras.html",
            "relUrl": "/ml/2017/12/18/recommend-keras.html",
            "date": " • Dec 18, 2017"
        }
        
    
  
    
        ,"post24": {
            "title": "Adagrad based matrix factorization",
            "content": "In a previous post, we had seen how to perfom non-negative matrix factorization (NNMF) using Tensorflow. In another previous post, I had shown how to use Adagrad for linear regression. This current post can be considered an extension of the linear regression using Adagrad post. Just for the purpose of education, I&#39;ll poorly initialise the estimate of one of the decomposed matrix, to see how well Adagrad can adjust weights! . Customary imports . import autograd.numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from matplotlib.animation import FuncAnimation from matplotlib import gridspec %matplotlib inline . Creating the matrix to be decomposed . A = np.array([[3, 4, 5, 2], [4, 4, 3, 3], [5, 5, 4, 3]], dtype=np.float32).T . Masking one entry . A[0, 0] = np.NAN . A . array([[ nan, 4., 5.], [ 4., 4., 5.], [ 5., 3., 4.], [ 2., 3., 3.]], dtype=float32) . Defining the cost function . def cost(param_list): W, H = param_list pred = np.dot(W, H) mask = ~np.isnan(A) return np.sqrt(((pred - A)[mask].flatten() ** 2).mean(axis=None)) . Decomposition params . rank = 2 learning_rate=0.01 n_steps = 10000 . Adagrad routine . def adagrad_gd(param_init, cost, niter=5, lr=1e-2, eps=1e-8, random_seed=0): &quot;&quot;&quot; param_init: List of initial values of parameters cost: cost function niter: Number of iterations to run lr: Learning rate eps: Fudge factor, to avoid division by zero &quot;&quot;&quot; from copy import deepcopy from autograd import grad # Fixing the random_seed np.random.seed(random_seed) # Function to compute the gradient of the cost function grad_cost = grad(cost) params = deepcopy(param_init) param_array, grad_array, lr_array, cost_array = [params], [], [[lr*np.ones_like(_) for _ in params]], [cost(params)] # Initialising sum of squares of gradients for each param as 0 sum_squares_gradients = [np.zeros_like(param) for param in params] for i in range(niter): out_params = [] gradients = grad_cost(params) # At each iteration, we add the square of the gradients to `sum_squares_gradients` sum_squares_gradients= [eps + sum_prev + np.square(g) for sum_prev, g in zip(sum_squares_gradients, gradients)] # Adapted learning rate for parameter list lrs = [np.divide(lr, np.sqrt(sg)) for sg in sum_squares_gradients] # Paramter update params = [param-(adapted_lr*grad_param) for param, adapted_lr, grad_param in zip(params, lrs, gradients)] param_array.append(params) lr_array.append(lrs) grad_array.append(gradients) cost_array.append(cost(params)) return params, param_array, grad_array, lr_array, cost_array . Running Adagrad . Fixing initial parameters . I&#39;m poorly initialising H here to see how the learning rates vary for W and H. . np.random.seed(0) shape = A.shape H_init = -5*np.abs(np.random.randn(rank, shape[1])) W_init = np.abs(np.random.randn(shape[0], rank)) param_init = [W_init, H_init] . H_init . array([[ -8.82026173, -2.00078604, -4.89368992], [-11.204466 , -9.33778995, -4.8863894 ]]) . W_init . array([[ 0.95008842, 0.15135721], [ 0.10321885, 0.4105985 ], [ 0.14404357, 1.45427351], [ 0.76103773, 0.12167502]]) . # Cost for initial set of parameters cost(param_init) . 11.651268820608442 . lr = 0.1 eps=1e-8 niter=2000 ada_params, ada_param_array, ada_grad_array, ada_lr_array, ada_cost_array = adagrad_gd(param_init, cost, niter=niter, lr=lr, eps=eps) . Cost v/s # iterations . pd.Series(ada_cost_array).plot(logy=True) plt.ylabel(&quot;Cost (log scale)&quot;) plt.xlabel(&quot;# Iterations&quot;) . &lt;matplotlib.text.Text at 0x10ece7610&gt; . Final set of parameters and recovered matrix . W_final, H_final = ada_params pred = np.dot(W_final, H_final) pred_df = pd.DataFrame(pred).round() pred_df . 0 1 2 . 0 5.0 | 4.0 | 5.0 | . 1 4.0 | 4.0 | 5.0 | . 2 5.0 | 3.0 | 4.0 | . 3 2.0 | 3.0 | 3.0 | . Learning rate evolution for W . W_lrs = np.array(ada_lr_array)[:, 0] . W_lrs = np.array(ada_lr_array)[:, 0] fig= plt.figure(figsize=(4, 2)) gs = gridspec.GridSpec(1, 2, width_ratios=[8, 1]) ax = plt.subplot(gs[0]), plt.subplot(gs[1]) max_W, min_W = np.max([np.max(x) for x in W_lrs]), np.min([np.min(x) for x in W_lrs]) def update(iteration): ax[0].cla() ax[1].cla() sns.heatmap(W_lrs[iteration], vmin=min_W, vmax=max_W, ax=ax[0], annot=True, fmt=&#39;.4f&#39;, cbar_ax=ax[1]) ax[0].set_title(&quot;Learning rate update for W nIteration: {}&quot;.format(iteration)) fig.tight_layout() anim = FuncAnimation(fig, update, frames=np.arange(0, 200, 10), interval=500) anim.save(&#39;W_update.gif&#39;, dpi=80, writer=&#39;imagemagick&#39;) plt.close() . . Learning rate evolution for H . H_lrs = np.array(ada_lr_array)[:, 1] fig= plt.figure(figsize=(4, 2)) gs = gridspec.GridSpec(1, 2, width_ratios=[10, 1]) ax = plt.subplot(gs[0]), plt.subplot(gs[1]) max_H, min_H = np.max([np.max(x) for x in H_lrs]), np.min([np.min(x) for x in H_lrs]) def update(iteration): ax[0].cla() ax[1].cla() sns.heatmap(H_lrs[iteration], vmin=min_H, vmax=max_H, ax=ax[0], annot=True, fmt=&#39;.2f&#39;, cbar_ax=ax[1]) ax[0].set_title(&quot;Learning rate update for H nIteration: {}&quot;.format(iteration)) fig.tight_layout() anim = FuncAnimation(fig, update, frames=np.arange(0, 200, 10), interval=500) anim.save(&#39;H_update.gif&#39;, dpi=80, writer=&#39;imagemagick&#39;) plt.close() . .",
            "url": "https://nipunbatra.github.io/blog/ml/2017/08/13/mf-autograd-adagrad.html",
            "relUrl": "/ml/2017/08/13/mf-autograd-adagrad.html",
            "date": " • Aug 13, 2017"
        }
        
    
  
    
        ,"post25": {
            "title": "Programatically understanding Adagrad",
            "content": "In this post, I&#39;ll be using Adagrad for solving linear regression. As usual, the purpose of this post is educational. This link gives a good overview of Adagrad alongwith other variants of Gradient Descent. To summarise from the link: . It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. . As I&#39;d done previously, I&#39;ll be using Autograd to compute the gradients. Please note Autograd and not Adagrad! . Formulation ([borrowed from here])((http://ruder.io/optimizing-gradient-descent/))) . In regular gradient descent, we would update the $i^{th}$ parameter in the $t+1^{th}$ iteration, given the learning rate $ eta$, where $g_{t, i}$ represents the gradient of the cost wrt $i^{th}$ param at time $t$. . $$ theta_{t+1, i} = theta_{t, i} - eta cdot g_{t, i} tag{Eq 1} $$ . In Adagrad, we update as follows: . $$ theta_{t+1, i} = theta_{t, i} - dfrac{ eta}{ sqrt{G_{t, ii} + epsilon}} cdot g_{t, i} tag{Eq 2}$$ . Here, . $G_{t} in mathbb{R}^{d times d}$ is a diagonal matrix where each diagonal element $i, i$ is the sum of the squares of the gradients w.r.t. $ theta_i$ up to time step $t$ , while $ epsilon$ is a smoothing term that avoids division by zero (usually on the order of 1e−8). . Customary imports . import autograd.numpy as np import matplotlib.pyplot as plt import pandas as pd %matplotlib inline . True model . $$Y = 10 X + 6$$ . Generating data . np.random.seed(0) n_samples = 50 X = np.linspace(1, 50, n_samples) Y = 10*X + 6 + 2*np.random.randn(n_samples) . plt.plot(X, Y, &#39;k.&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;); . Model to be learnt . We want to learn W and b such that: . $$Y = 10 W+ b$$ . Defining the cost function . We will now write a general cost function that accepts a list of parameters. . def cost(param_list): w, b = param_list pred = w*X+b return np.sqrt(((pred - Y) ** 2).mean(axis=None))/(2*len(Y)) . Dry run of cost and gradient functioning . # Cost of w=0, b=0 w, b = 0., 0. print(&quot;Cost at w={}, b={} is: {}&quot;.format(w, b, cost([w, b]))) # Cost of w=10, b=4. Should be lower than w=0, b=0 w, b = 10., 4. print(&quot;Cost at w={}, b={} is: {}&quot;.format(w, b, cost([w, b]))) # Computing the gradient at w=0, b=0 from autograd import grad grad_cost =grad(cost) w, b = 0., 0. print(&quot;Gradient at w={}, b={} is: {}&quot;.format(w, b, grad_cost([w, b]))) # Computing the gradient at w=10, b=4. We would expect it to be smaller than at 0, 0 w, b = 10., 4. print(&quot;Gradient at w={}, b={} is: {}&quot;.format(w, b, grad_cost([w, b]))) . Cost at w=0.0, b=0.0 is: 2.98090446495 Cost at w=10.0, b=4.0 is: 0.0320479471939 Gradient at w=0.0, b=0.0 is: [array(-0.29297046699711365), array(-0.008765162440358071)] Gradient at w=10.0, b=4.0 is: [array(-0.14406455246023858), array(-0.007117830452061141)] . Adagrad algorithm (applied on whole data batch) . def adagrad_gd(param_init, cost, niter=5, lr=1e-2, eps=1e-8, random_seed=0): &quot;&quot;&quot; param_init: List of initial values of parameters cost: cost function niter: Number of iterations to run lr: Learning rate eps: Fudge factor, to avoid division by zero &quot;&quot;&quot; from copy import deepcopy import math # Fixing the random_seed np.random.seed(random_seed) # Function to compute the gradient of the cost function grad_cost = grad(cost) params = deepcopy(param_init) param_array, grad_array, lr_array, cost_array = [params], [], [[lr for _ in params]], [cost(params)] # Initialising sum of squares of gradients for each param as 0 sum_squares_gradients = [np.zeros_like(param) for param in params] for i in range(niter): out_params = [] gradients = grad_cost(params) # At each iteration, we add the square of the gradients to `sum_squares_gradients` sum_squares_gradients= [eps + sum_prev + np.square(g) for sum_prev, g in zip(sum_squares_gradients, gradients)] # Adapted learning rate for parameter list lrs = [np.divide(lr, np.sqrt(sg)) for sg in sum_squares_gradients] # Paramter update params = [param-(adapted_lr*grad_param) for param, adapted_lr, grad_param in zip(params, lrs, gradients)] param_array.append(params) lr_array.append(lrs) grad_array.append(gradients) cost_array.append(cost(params)) return params, param_array, grad_array, lr_array, cost_array . Experiment time! . Evolution of learning rates for W and b . Let us see how the learning rate for W and b will evolve over time. I will fix the initial learning rate to 0.01 as mot of the Adagrad literature out there seems to suggest. . # Fixing the random seed for reproducible init params for `W` and `b` np.random.seed(0) param_init = [np.random.randn(), np.random.randn()] lr = 0.01 eps=1e-8 niter=1000 ada_params, ada_param_array, ada_grad_array, ada_lr_array, ada_cost_array = adagrad_gd(param_init, cost, niter=niter, lr=lr, eps=eps) . Let us first see the evolution of cost wrt time . pd.Series(ada_cost_array, name=&#39;Cost&#39;).plot(title=&#39;Adagrad: Cost v/s # Iterations&#39;) plt.ylabel(&quot;Cost&quot;) plt.xlabel(&quot;# Iterations&quot;); . Ok. While There seems to be a drop in the cost, the converegence will be very slow. Remember that we had earlier found . Cost at w=10.0, b=4.0 is:0.0320479471939 I&#39;m sure this means that our parameter estimates are similar to the initial parameters and far from the true parameters. Let&#39;s just confirm the same. . print(&quot;After {} iterations, learnt `W` = {} and learnt `b` = {}&quot;.format(niter, *ada_params)) . After 1000 iterations, learnt `W` = 2.38206194526 and learnt `b` = 1.01811878873 . I would suspect that the learning rate, courtesy of the adaptive nature is falling very rapidly! How would the vanilla gradient descent have done starting with the same learning rate and initial values? My hunch is it would do better. Let&#39;s confirm! . GD vs Adagrad! . def gd(param_init, cost, niter=5, lr=0.01, random_seed=0): np.random.seed(random_seed) from copy import deepcopy grad_cost = grad(cost) params = deepcopy(param_init) param_array, grad_array, cost_array = [params], [], [cost(params)] for i in range(niter): out_params = [] gradients = grad_cost(params) params = [param-lr*grad_param for param, grad_param in zip(params, gradients)] param_array.append(params) grad_array.append(gradients) cost_array.append(cost(params)) return params, param_array, grad_array, cost_array . # Fixing the random seed for reproducible init params for `W` and `b` np.random.seed(0) param_init = [np.random.randn(), np.random.randn()] lr = 0.01 niter=1000 gd_params, gd_param_array, gd_grad_array, gd_cost = gd(param_init, cost, niter=niter, lr=lr) . pd.Series(ada_cost_array, name=&#39;Cost&#39;).plot(label=&#39;Adagrad&#39;) pd.Series(gd_cost, name=&#39;Cost&#39;).plot(label=&#39;GD&#39;) plt.ylabel(&quot;Cost&quot;) plt.xlabel(&quot;# Iterations&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1153b4ad0&gt; . Ok. So, indeed with learning rate of 0.01, gradient descent fares better. Let&#39;s just confirm that for Adagrad, the learning rates diminish rapidly leading to little reduction in cost! . pd.DataFrame(np.array(ada_lr_array), columns=[&#39;LR for W&#39;, &#39;LR for b&#39;])[::50].plot(subplots=True, marker=&#39;o&#39;) plt.xlabel(&quot;# Iterations&quot;) . &lt;matplotlib.text.Text at 0x11569c4d0&gt; . There are a couple of interesting observations: . The learning rate for b actually increases from its initial value of 0.01. Even after 1000 iterations, it remains more than its initial value. This can be explained by the fact that the suim of squares gradients wrt b would be less than 1. Thus, the denominator term by which the learning rate gets divided will be less than 1. Thus, increasing the learning rate wrt b. This can however be fixed by choosing $ epsilon=1.0$ | The learning rate for W falls very rapidly. Learning would be negligble for W after the initial few iterations. This can be fixed by choosing a larger initial learning rate $ eta$. | Evolution of W and b, wrt $ eta$ and $ epsilon$ . # Fixing the random seed for reproducible init params for `W` and `b` out = {} for lr in [0.01, 0.1, 1, 10]: out[lr] = {} for eps in [1e-8, 1e-1, 1]: print(lr, eps) np.random.seed(0) param_init = [np.random.randn(), np.random.randn()] niter=10000 ada_params, ada_param_array, ada_grad_array, ada_lr_array, ada_cost_array = adagrad_gd(param_init, cost, niter=niter, lr=lr, eps=eps) out[lr][eps] = {&#39;Final-params&#39;:ada_params, &#39;Param-array&#39;:ada_param_array, &#39;Cost-array&#39;:ada_cost_array} . (0.01, 1e-08) (0.01, 0.1) (0.01, 1) (0.1, 1e-08) (0.1, 0.1) (0.1, 1) (1, 1e-08) (1, 0.1) (1, 1) (10, 1e-08) (10, 0.1) (10, 1) . Plotting cost v/s # Iterations . fig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, figsize=(8, 6), sharey=True) for row, eps in enumerate([1e-8, 1e-1, 1]): for column, lr in enumerate([0.01, 0.1, 1, 10]): pd.Series(out[lr][eps][&#39;Cost-array&#39;]).plot(ax=ax[row, column]) ax[0, column].set_title(&quot;Eta={}&quot;.format(lr)) ax[row, 0].set_ylabel(&quot;Eps={}&quot;.format(eps)) fig.text(0.5, 0.0, &#39;# Iterations&#39;) plt.suptitle(&quot;Cost v/s # Iterations&quot;); . It seems that choosing $ eta=1$ or above the cost usually converges quickly. This seems to be different from most literature recommending $ eta=0.01$. Aside: I confirmed that even using Tensorflow on the same dataset with Adagrad optimizer, the optimal learning rates are similar to the ones we found here! . W v/s # Iterations . fig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, figsize=(8, 6), sharey=True) for row, eps in enumerate([1e-8, 1e-1, 1]): for column, lr in enumerate([0.01, 0.1, 1, 10]): pd.DataFrame(out[lr][eps][&#39;Param-array&#39;])[0].plot(ax=ax[row, column]) ax[0, column].set_title(&quot;Eta={}&quot;.format(lr)) ax[row, 0].set_ylabel(&quot;Eps={}&quot;.format(eps)) fig.text(0.5, 0.0, &#39;# Iterations&#39;) plt.suptitle(&quot;W v/s # Iterations&quot;); . b v/s # Iterations . fig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, figsize=(8, 6), sharey=True) for row, eps in enumerate([1e-8, 1e-1, 1]): for column, lr in enumerate([0.01, 0.1, 1, 10]): pd.DataFrame(out[lr][eps][&#39;Param-array&#39;])[1].plot(ax=ax[row, column]) ax[0, column].set_title(&quot;Eta={}&quot;.format(lr)) ax[row, 0].set_ylabel(&quot;Eps={}&quot;.format(eps)) fig.text(0.5, 0.0, &#39;# Iterations&#39;) plt.suptitle(&quot;b v/s # Iterations&quot;); . Across the above two plots, we can see that at high $ eta$, there are oscillations! In general, $ eta=1$ and $ epsilon=1e-8$ seem to give the best set of results. . Visualising the model learning . from matplotlib.animation import FuncAnimation fig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, figsize=(8, 6), sharey=True) def update(i): #fig.clf() for row, eps in enumerate([1e-8, 1e-1, 1]): for column, lr in enumerate([0.01, 0.1, 1, 10]): params_i = out[lr][eps][&#39;Param-array&#39;][i] ax[row, column].cla() w_i, b_i = params_i ax[row, column].plot(X, Y, &#39;k.&#39;, ms=1) ax[row, column].plot(X, w_i*X+b_i, &#39;r&#39;) ax[row, column].tick_params( #https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot axis=&#39;both&#39;, which=&#39;both&#39;, bottom=&#39;off&#39;, left=&#39;off&#39;, top=&#39;off&#39;, labelbottom=&#39;off&#39;, labelleft=&#39;off&#39;) ax[0, column].set_title(&quot;Eta={}&quot;.format(lr)) ax[row, 0].set_ylabel(&quot;Eps={}&quot;.format(eps)) fig.suptitle(&quot;Iteration number: {}&quot;.format(i)) anim = FuncAnimation(fig, update, frames=np.arange(0, 5000, 200), interval=500) anim.save(&#39;adagrad.gif&#39;, dpi=80, writer=&#39;imagemagick&#39;) plt.close() . . So, there you go. Implementing Adagrad and running this experiment was a lot of fun and learning. Feel free to comment! .",
            "url": "https://nipunbatra.github.io/blog/ml/2017/08/12/linear-regression-adagrad-vs-gd.html",
            "relUrl": "/ml/2017/08/12/linear-regression-adagrad-vs-gd.html",
            "date": " • Aug 12, 2017"
        }
        
    
  
    
        ,"post26": {
            "title": "Top 50 ggplot2 Visualizations in Python - Part 1",
            "content": "A while back, I read this wonderful article called &quot;Top 50 ggplot2 Visualizations - The Master List (With Full R Code)&quot;. Many of the plots looked very useful. In this post, I&#39;ll look at creating the first of the plot in Python (with the help of Stack Overflow). . Here&#39;s how the end result should look like. . How the final plot should look like . . Attributes of above plot . X-Y scatter for area vs population | Color by state | Marker-size by population | I&#39;ll first use Pandas to create the plot. Pandas plotting capabilites are almost the first thing I use to create plots. Next, I&#39;ll show how to use Seaborn to reduce some complexity. Lastly, I&#39;ll use Altair, ggplot and Plotnine to show how it focuses on getting directly to the point, i.e. expressing the 3 required attributes! . TLDR: Declarative visualisatio) is super useful! . Original R code . # install.packages(&quot;ggplot2&quot;) # load package and data options(scipen=999) # turn-off scientific notation like 1e+48 library(ggplot2) theme_set(theme_bw()) # pre-set the bw theme. data(&quot;midwest&quot;, package = &quot;ggplot2&quot;) # midwest &lt;- read.csv(&quot;http://goo.gl/G1K41K&quot;) # bkup data source # Scatterplot gg &lt;- ggplot(midwest, aes(x=area, y=poptotal)) + geom_point(aes(col=state, size=popdensity)) + geom_smooth(method=&quot;loess&quot;, se=F) + xlim(c(0, 0.1)) + ylim(c(0, 500000)) + labs(subtitle=&quot;Area Vs Population&quot;, y=&quot;Population&quot;, x=&quot;Area&quot;, title=&quot;Scatterplot&quot;, caption = &quot;Source: midwest&quot;) plot(gg) . %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns import pandas as pd . Color scheme (borrowed from Randy Olson&#39;s website) . # Tableau 20 Colors tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120), (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150), (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148), (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199), (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)] # Rescale to values between 0 and 1 for i in range(len(tableau20)): r, g, b = tableau20[i] tableau20[i] = (r / 255., g / 255., b / 255.) . Getting the data . midwest= pd.read_csv(&quot;http://goo.gl/G1K41K&quot;) # Filtering midwest= midwest[midwest.poptotal&lt;50000] . midwest.head().loc[:, [&#39;area&#39;] ] . area . 1 0.014 | . 2 0.022 | . 3 0.017 | . 4 0.018 | . 5 0.050 | . Default Pandas scatter plot with marker size by population density . midwest.plot(kind=&#39;scatter&#39;, x=&#39;area&#39;, y=&#39;poptotal&#39;, ylim=((0, 50000)), xlim=((0., 0.1)), s=midwest[&#39;popdensity&#39;]*0.1) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x10e5714d0&gt; . If we just use the default Pandas scatter, we won&#39;t get the colour by state. For that we wil group the dataframe by states and then scatter plot each group individually. . Complete Pandas&#39; solution (hand-wavy at times!) . fig, ax = plt.subplots() groups = midwest.groupby(&#39;state&#39;) colors = tableau20[::2] # Plotting each group for i, (name, group) in enumerate(groups): group.plot(kind=&#39;scatter&#39;, x=&#39;area&#39;, y=&#39;poptotal&#39;, ylim=((0, 50000)), xlim=((0., 0.1)), s=10+group[&#39;popdensity&#39;]*0.1, # hand-wavy :( label=name, ax=ax, color=colors[i]) # Legend for State colours lgd = ax.legend(numpoints=1, loc=1, borderpad=1, frameon=True, framealpha=0.9, title=&quot;state&quot;) for handle in lgd.legendHandles: handle.set_sizes([100.0]) # Make a legend for popdensity. Hand-wavy. Error prone! pws = (pd.cut(midwest[&#39;popdensity&#39;], bins=4, retbins=True)[1]).round(0) for pw in pws: plt.scatter([], [], s=(pw**2)/2e4, c=&quot;k&quot;,label=str(pw)) h, l = plt.gca().get_legend_handles_labels() plt.legend(h[5:], l[5:], labelspacing=1.2, title=&quot;popdensity&quot;, borderpad=1, frameon=True, framealpha=0.9, loc=4, numpoints=1) plt.gca().add_artist(lgd) . &lt;matplotlib.legend.Legend at 0x110a3e790&gt; . Using Seaborn . The solution using Seaborn is slightly less complicated as we won&#39;t need to write the code for plotting different states on different colours. However, the legend jugglery for markersize would still be required! . sizes = [10, 40, 70, 100] marker_size = pd.cut(midwest[&#39;popdensity&#39;], range(0, 2500, 500), labels=sizes) sns.lmplot(&#39;area&#39;, &#39;poptotal&#39;, data=midwest, hue=&#39;state&#39;, fit_reg=False, scatter_kws={&#39;s&#39;:marker_size}) plt.ylim((0, 50000)) . (0, 50000) . Altair (could not get simpler!) . from altair import Chart chart = Chart(midwest) chart.mark_circle().encode( x=&#39;area&#39;, y=&#39;poptotal&#39;, color=&#39;state&#39;, size=&#39;popdensity&#39;, ) . ggplot . from ggplot import * ggplot(aes(x=&#39;area&#39;, y=&#39;poptotal&#39;, color=&#39;state&#39;, size=&#39;popdensity&#39;), data=midwest) + geom_point() + theme_bw() + xlab(&quot;Area&quot;) + ylab(&quot;Population&quot;) + ggtitle(&quot;Area vs Population&quot;) . &lt;ggplot: (295628405)&gt; . It was great fun (and frustration) trying to make this plot. Still some bits like LOESS are not included in the visualisation I made. The best thing about this exercise was discovering Altair! Declarative visualisation looks so natural. Way to go declarative visualisation! .",
            "url": "https://nipunbatra.github.io/blog/visualisation/2017/08/02/fifty-ggplot-python-1.html",
            "relUrl": "/visualisation/2017/08/02/fifty-ggplot-python-1.html",
            "date": " • Aug 2, 2017"
        }
        
    
  
    
        ,"post27": {
            "title": "Linear regression with prior (using gradient descent)",
            "content": "Let&#39;s say we have a prior on the linear model, i.e. we start with a known W (W_prior) and b (b_prior). Further, we say that the learnt function can be such that: . $$W = alpha times W_{prior} + delta$$ $$b = beta + b_{prior} + eta$$ . Our task reduces to learn $ alpha$, $ beta$, $ delta$ and $ eta$. This can be solved as we would usually do using Gradient descent, the only difference being that we will compute the gradient wrt $ alpha$ , $ beta$, $ delta$, $ eta$. I will use autograd to compute the gradients. . In a typical model we might have 2 parameters (w and b). In our refined one, we have four- $ alpha$ , $ beta$, $ delta$, $ eta$. . Customary imports . import autograd.numpy as np import matplotlib.pyplot as plt %matplotlib inline . True model . $$Y = 10 X + 6$$ . Generating data . np.random.seed(0) n_samples = 50 X = np.linspace(1, 50, n_samples) Y = 10*X + 6 + 3*np.random.randn(n_samples) . plt.plot(X, Y, &#39;k.&#39;) plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;); . Defining priors (bad ones!) . w_prior = -2 b_prior = -2 . Defining the cost function in terms of alpha and beta . def cost(alpha, beta, delta, eta): pred = np.dot(X, alpha*w_prior+delta) + b_prior + beta + eta return np.sqrt(((pred - Y) ** 2).mean(axis=None)) from autograd import grad, multigrad grad_cost= multigrad(cost, argnums=[0, 1, 2, 3]) . Gradient descent . alpha = np.random.randn() beta = np.random.randn() eta = np.random.randn() delta = np.random.randn() lr = 0.001 # We will also save the values for plotting later w_s = [alpha*w_prior+delta] b_s = [alpha*w_prior+delta] for i in range(10001): del_alpha, del_beta, del_delta, del_eta = grad_cost(alpha, beta, delta, eta) alpha = alpha - del_alpha*lr beta = beta - del_beta*lr delta = delta - del_delta*lr eta = eta - del_eta*lr w_s.append(alpha*w_prior+delta) b_s.append(alpha*w_prior+delta) if i%500==0: print &quot;*&quot;*20 print i print &quot;*&quot;*20 print cost(alpha, beta, delta, eta), alpha*w_prior+delta, alpha*w_prior+delta . ******************** 0 ******************** 277.717926153 0.756766902473 0.756766902473 ******************** 500 ******************** 5.95005440573 10.218493676 10.218493676 ******************** 1000 ******************** 5.77702829051 10.2061390906 10.2061390906 ******************** 1500 ******************** 5.60823669668 10.1939366275 10.1939366275 ******************** 2000 ******************** 5.44395500928 10.1818982949 10.1818982949 ******************** 2500 ******************** 5.28446602486 10.1700368748 10.1700368748 ******************** 3000 ******************** 5.1300568557 10.158365894 10.158365894 ******************** 3500 ******************** 4.98101499128 10.1468995681 10.1468995681 ******************** 4000 ******************** 4.83762347034 10.1356527141 10.1356527141 ******************** 4500 ******************** 4.70015516667 10.1246406278 10.1246406278 ******************** 5000 ******************** 4.56886626032 10.1138789219 10.1138789219 ******************** 5500 ******************** 4.44398905185 10.1033833225 10.1033833225 ******************** 6000 ******************** 4.32572437603 10.0931694258 10.0931694258 ******************** 6500 ******************** 4.21423397192 10.0832524173 10.0832524173 ******************** 7000 ******************** 4.10963325557 10.0736467626 10.0736467626 ******************** 7500 ******************** 4.01198500112 10.0643658801 10.0643658801 ******************** 8000 ******************** 3.92129444852 10.0554218111 10.0554218111 ******************** 8500 ******************** 3.83750630808 10.046824905 10.046824905 ******************** 9000 ******************** 3.7605040187 10.0385835381 10.0385835381 ******************** 9500 ******************** 3.69011144573 10.0307038843 10.0307038843 ******************** 10000 ******************** 3.6260969956 10.023189752 10.023189752 . We are able to learn a reasonably accurate W=10.07 and b=2.7. . Bonus: Animation . Making the plots look nicer. . def format_axes(ax): for spine in [&#39;top&#39;, &#39;right&#39;]: ax.spines[spine].set_visible(False) for spine in [&#39;left&#39;, &#39;bottom&#39;]: ax.spines[spine].set_color(&#39;grey&#39;) ax.spines[spine].set_linewidth(0.5) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.yaxis.set_ticks_position(&#39;left&#39;) for axis in [ax.xaxis, ax.yaxis]: axis.set_tick_params(direction=&#39;out&#39;, color=&#39;grey&#39;) return ax . # Code courtesy: http://eli.thegreenplace.net/2016/drawing-animated-gifs-with-matplotlib/ from matplotlib.animation import FuncAnimation fig, ax = plt.subplots(figsize=(4, 3)) fig.set_tight_layout(True) # Query the figure&#39;s on-screen size and DPI. Note that when saving the figure to # a file, we need to provide a DPI for that separately. print(&#39;fig size: {0} DPI, size in inches {1}&#39;.format( fig.get_dpi(), fig.get_size_inches())) # Plot a scatter that persists (isn&#39;t redrawn) and the initial line. ax.scatter(X, Y, color=&#39;grey&#39;, alpha=0.8, s=1) # Initial line line, = ax.plot(X, X*w_prior+b_prior, &#39;r-&#39;, linewidth=1) def update(i): label = &#39;Iteration {0}&#39;.format(i) line.set_ydata(X*w_s[i]+b_s[i]) ax.set_xlabel(label) format_axes(ax) return line, ax anim = FuncAnimation(fig, update, frames=np.arange(0, 100), interval=1) anim.save(&#39;line_prior.gif&#39;, dpi=80, writer=&#39;imagemagick&#39;) plt.close() . fig size: 72.0 DPI, size in inches [ 4. 3.] . .",
            "url": "https://nipunbatra.github.io/blog/ml/2017/06/15/linear-regression-prior.html",
            "relUrl": "/ml/2017/06/15/linear-regression-prior.html",
            "date": " • Jun 15, 2017"
        }
        
    
  
    
        ,"post28": {
            "title": "Data exploration using widgets in Matplotlib",
            "content": "Imagine that you have to do data cleaning on 10s or 100s of sample points (akin to a row in a 2d matrix). For the purposes of data cleaning, you&#39;d also need to zoom/pan at the data correpsonding to each sample point. Would you create 100s of static plots? We lose the zoom/pan ability there. How about we write a simple function and manually change the argument to reflect the sample #. . In this post, I&#39;ll be looking at a simple Matplotlib widget to sift through the samples and retain the ability to pan and zoom. This post is heavily inspired by Jake Vanderplas&#39; PyData 2013 Matplotlib tutorial. I would be creating 15 timeseries having recorded daily for an year for illustration purposes. . Setting the backend to TK. . For some reasons, it works better than the default OSX one. . %matplotlib tk . Customary imports . import matplotlib.pyplot as plt import numpy as np import pandas as pd import sys . Creating the data . # Fixing the seed for reproducibility np.random.seed(0) df = pd.DataFrame(np.random.randn(365, 15), index=pd.DatetimeIndex(start=&#39;2017&#39;,freq=&#39;D&#39;, periods=365)) . df.head()[range(5)] . 0 1 2 3 4 . 2017-01-01 1.764052 | 0.400157 | 0.978738 | 2.240893 | 1.867558 | . 2017-01-02 0.333674 | 1.494079 | -0.205158 | 0.313068 | -0.854096 | . 2017-01-03 0.154947 | 0.378163 | -0.887786 | -1.980796 | -0.347912 | . 2017-01-04 -0.438074 | -1.252795 | 0.777490 | -1.613898 | -0.212740 | . 2017-01-05 -0.672460 | -0.359553 | -0.813146 | -1.726283 | 0.177426 | . fig, ax = plt.subplots() df.plot(ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x10afff890&gt; . Notice, that since I used %matplotlib TK backend, I don&#39;t see the plot embedded in the notebook. Thus I&#39;ll save the current figure as an image and then link it here. . plt.savefig(&quot;all_data.png&quot;) . . This sure does not look pretty. . Proposed solution . . Great. It seems to do the intended job. Let us now look at the individual pieces and how we can tie them up. . Creating the initial frame . In the first frame we would like to plot the data for the first sample. . fig, ax = plt.subplots() df[0].plot(ax=ax, title=&quot;Sample number: 0&quot;) . Creating the buttons at the bottom . First, we&#39;d want to make space for the button at the bottom and place them there. We can do this as follows: . from matplotlib.widgets import Button fig.subplots_adjust(bottom=0.2) axprev = plt.axes([0.7, 0.05, 0.1, 0.075]) axnext = plt.axes([0.81, 0.05, 0.1, 0.075]) bnext = Button(axnext, &#39;&gt;&#39;) bprev = Button(axprev, &#39;&lt;&#39;) . Linking the buttons to functions . We&#39;d next want to call some function each time either of the two buttons are pressed. We would also need a notion of currently selected data point. The idea would be that each time, &gt; is pressed, we advance the currently selected point and plot correspondingly. . We&#39;d have to define next() and prev() where we increment and decrement the selected data point. . class Index: data = df selected = 0 def next(self, event): self.selected += 1 ax.cla() df[self.selected].plot(ax=ax) ax.set_title(&quot;Sample number: %d&quot; %self.selected) def prev(self, event): self.selected -= 1 ax.cla() df[self.selected].plot(ax=ax) ax.set_title(&quot;Sample number: %d&quot; %self.selected) . Here, ax.cla() clears the data for the current data point before drawing for the next one. df[self.selected].plot(ax=ax) plots for the newly selected data. ax.set_title(&quot;Sample number: %d&quot; %self.selected) would change the title to reflect the currently used data point. . We can link to callback as follows: . callback = Index() bnext.on_clicked(callback.next) bprev.on_clicked(callback.prev) . Ensuring we do not select data point out of range . If you notice, we simply incremented and decremented the selected data point without considering going beyond (0, number of data points). So, we need to change the call back functions to check that we do not go beyond the range. This would require the following changes to next() with the changes to prev() being similar. . data_min = 0 data_max = data.shape[1]-1 selected = 0 def next(self, event): if self.selected &gt;=self.data_max: self.selected = self.data_max ax.set_title(&#39;Last sample reached. Cannot go forwards&#39;) else: self.selected += 1 ax.cla() df[self.selected].plot(ax=ax) ax.set_title(&quot;Sample number: %d&quot; %self.selected) . There you go. This was fairly simple and fun to do, and yet can be very helpful! . Complete code . from matplotlib.widgets import Button fig, ax = plt.subplots() fig.subplots_adjust(bottom=0.2) df[0].plot(ax=ax, title=&quot;Sample number: 0&quot;) class Index: data = df data_min = 0 data_max = data.shape[1]-1 selected = 0 def next(self, event): if self.selected &gt;=self.data_max: self.selected = self.data_max ax.set_title(&#39;Last sample reached. Cannot go forwards&#39;) else: self.selected += 1 ax.cla() df[self.selected].plot(ax=ax) ax.set_title(&quot;Sample number: %d&quot; %self.selected) def prev(self, event): if self.selected &lt;=self.data_min: self.selected = 0 ax.set_title(&#39;First sample reached. Cannot go backwards&#39;) else: self.selected -= 1 ax.cla() df[self.selected].plot(ax=ax) ax.set_title(&quot;Sample number: %d&quot; %self.selected) callback = Index() axprev = plt.axes([0.7, 0.05, 0.1, 0.075]) axnext = plt.axes([0.81, 0.05, 0.1, 0.075]) bnext = Button(axnext, &#39;&gt;&#39;) bnext.on_clicked(callback.next) bprev = Button(axprev, &#39;&lt;&#39;) bprev.on_clicked(callback.prev) . 0 . Advanced example . Here is another slightly more advanced wideget use case in action. . . I will just put the code up here and leave the understanding upto the reader as an exercise. . with pd.HDFStore(&#39;temp-store.h5&#39;, mode=&#39;w&#39;) as st: # 15 home-&gt; 2 columns, 365 rows (daily one reading) for home in range(15): df = pd.DataFrame(np.random.randn(365, 2), columns=[&#39;fridge&#39;,&#39;microwave&#39;], index=pd.DatetimeIndex(start=&#39;2017&#39;,freq=&#39;D&#39;, periods=365)) df = df.abs() st[&#39;/home_%d&#39; %home] = df . st = pd.HDFStore(&#39;temp-store.h5&#39;, mode=&#39;r&#39;) . from matplotlib.widgets import Button, CheckButtons fig, ax = plt.subplots() fig.subplots_adjust(bottom=0.2) fig.subplots_adjust(left=0.2) home_0 = st[&#39;/home_0&#39;] rax = plt.axes([0.02, 0.4, 0.13, 0.2], aspect=&#39;equal&#39;) labels = tuple(home_0.columns) states = tuple([True]*len(labels)) check = CheckButtons(rax, labels, states) st[&#39;/home_0&#39;].plot(ax=ax, title=&quot;Sample number: 0&quot;).legend(loc=2) lines = ax.get_lines() class Index: store = st data_min = 0 data_max = len(store.keys())-1 selected = 0 st, la = states, labels states_dict = dict(zip(la, st)) def selected_column(self, label): self.states_dict[label] = not self.states_dict[label] self.plot() def plot(self): ax.cla() st[&#39;/home_%d&#39; %self.selected].plot(ax=ax, title=&quot;Sample number: %d&quot; %self.selected).legend(loc=2) lines = ax.get_lines() for i ,(l, s) in enumerate(self.states_dict.items()): lines[i].set_visible(s) plt.legend(loc=1) def next(self, event): if self.selected &gt;=self.data_max: self.selected = self.data_max ax.set_title(&#39;Last sample reached. Cannot go forwards&#39;) else: self.selected += 1 self.plot() def prev(self, event): if self.selected &lt;=self.data_min: self.selected = 0 ax.set_title(&#39;First sample reached. Cannot go backwards&#39;) else: self.selected -= 1 self.plot() callback = Index() axprev = plt.axes([0.7, 0.05, 0.1, 0.075]) axnext = plt.axes([0.81, 0.05, 0.1, 0.075]) bnext = Button(axnext, &#39;&gt;&#39;) bnext.on_clicked(callback.next) bprev = Button(axprev, &#39;&lt;&#39;) bprev.on_clicked(callback.prev) check.on_clicked(callback.selected_column); .",
            "url": "https://nipunbatra.github.io/blog/visualisation/2017/06/14/widgets-matplotlib.html",
            "relUrl": "/visualisation/2017/06/14/widgets-matplotlib.html",
            "date": " • Jun 14, 2017"
        }
        
    
  
    
        ,"post29": {
            "title": "Constrained Non-negative matrix factorisation using CVXPY",
            "content": "In a previous post, we saw how we can use CVXPY to perform non-negative matrix factorisation. In this post, I&#39;ll show how to add additional constraints that may arise from the problem domain. As a trivial example, I&#39;ll take constraints of the form when there is a less-than relationship among members of the matrix. For example, we may want to enforce certain movies to be always rated more than others? We&#39;ll create a matrix of 30 users and 12 items. We will enforce the contraint that the rating of the first 6 items be atleast twice that of the last 6 items. . Creating a ratings matrix . We will now create a matrix where the relationship among items exists. . import numpy as np import pandas as pd . K, N, M = 2, 12, 30 Y_gen = np.random.rand(M, K) X_1 = np.random.rand(K, N/2) # So that atleast twice as much X_2 = 2* X_1 + np.random.rand(K, N/2) X_gen = np.hstack([X_2, X_1]) # Normalizing X_gen = X_gen/np.max(X_gen) # Creating A (ratings matrix of size M, N) A = np.dot(Y_gen, X_gen) pd.DataFrame(A).head() . 0 1 2 3 4 5 6 7 8 9 10 11 . 0 0.732046 | 0.613565 | 0.961128 | 0.920089 | 0.244323 | 0.506472 | 0.280477 | 0.251049 | 0.324418 | 0.378219 | 0.075556 | 0.131750 | . 1 0.903630 | 0.340956 | 0.784109 | 0.919741 | 0.190856 | 0.433635 | 0.321932 | 0.135134 | 0.290862 | 0.394680 | 0.052976 | 0.081148 | . 2 0.972145 | 0.576558 | 1.046197 | 1.098279 | 0.261103 | 0.562996 | 0.358574 | 0.233405 | 0.368118 | 0.460967 | 0.077286 | 0.128344 | . 3 0.292231 | 0.263864 | 0.401968 | 0.377116 | 0.102567 | 0.210890 | 0.113070 | 0.108163 | 0.134489 | 0.154266 | 0.031993 | 0.056299 | . 4 0.694038 | 0.803459 | 1.125454 | 0.987344 | 0.290605 | 0.582178 | 0.278848 | 0.331075 | 0.365935 | 0.397023 | 0.093088 | 0.168300 | . We can see that for each user, the 0th item has higher rating compared to the 5th, 1st more than the 6th and so on. Now, in our alternating least squares implementation, we break down A as Y.X. Here X has dimensions of K, N. To ensure the relationship among the items, we will put contraints on X of the form: X[:, 0] &gt; 2 x X[:, 5] and so on. We will create a simple for loop for the same. . e = &quot;[&quot; for a in range(N/2): e+=&quot;X[:,%d] &gt; 2 * X[:,%d],&quot; %(a, a+N/2) e = e[:-1] + &quot;]&quot; e . &#39;[X[:,0] &gt; 2 * X[:,6],X[:,1] &gt; 2 * X[:,7],X[:,2] &gt; 2 * X[:,8],X[:,3] &gt; 2 * X[:,9],X[:,4] &gt; 2 * X[:,10],X[:,5] &gt; 2 * X[:,11]]&#39; . As we can see, we now have 6 constraints that we can feed into the optimisation routine. Whenever we learn X in the ALS, we apply these constraint. . CVX routine for handling input constraints . def nmf_features(A, k, MAX_ITERS=30, input_constraints_X=None, input_constraints_Y = None): import cvxpy as cvx np.random.seed(0) # Generate random data matrix A. m, n = A.shape mask = ~np.isnan(A) # Initialize Y randomly. Y_init = np.random.rand(m, k) Y = Y_init # Perform alternating minimization. residual = np.zeros(MAX_ITERS) for iter_num in xrange(1, 1 + MAX_ITERS): # For odd iterations, treat Y constant, optimize over X. if iter_num % 2 == 1: X = cvx.Variable(k, n) constraint = [X &gt;= 0] if input_constraints_X: constraint.extend(eval(input_constraints_X)) # For even iterations, treat X constant, optimize over Y. else: Y = cvx.Variable(m, k) constraint = [Y &gt;= 0] Temp = Y * X error = A[mask] - (Y * X)[mask] obj = cvx.Minimize(cvx.norm(error, &#39;fro&#39;)) prob = cvx.Problem(obj, constraint) prob.solve(solver=cvx.SCS) if prob.status != cvx.OPTIMAL: pass residual[iter_num - 1] = prob.value if iter_num % 2 == 1: X = X.value else: Y = Y.value return X, Y, residual . # Without constraints X, Y, r = nmf_features(A, 3, MAX_ITERS=20) # With contstraints X_c, Y_c, r_c = nmf_features(A, 3, MAX_ITERS=20, input_constraints_X=e) . pd.DataFrame(X) . 0 1 2 3 4 5 6 7 8 9 10 11 . 0 0.749994 | 0.112355 | 0.485850 | 0.674801 | 0.113004 | 0.281371 | 0.257239 | 0.04056 | 0.196474 | 0.297978 | 0.02745 | 0.033952 | . 1 0.102384 | 0.222149 | 0.266055 | 0.199361 | 0.070403 | 0.133510 | 0.047174 | 0.09233 | 0.081233 | 0.076518 | 0.02375 | 0.045097 | . 2 0.567213 | 0.558638 | 0.825066 | 0.756059 | 0.211427 | 0.430690 | 0.222174 | 0.22944 | 0.273260 | 0.307475 | 0.06659 | 0.118371 | . pd.DataFrame(X_c) . 0 1 2 3 4 5 6 7 8 9 10 11 . 0 0.749882 | 0.112384 | 0.485923 | 0.674778 | 0.113027 | 0.281399 | 0.257206 | 0.040566 | 0.196489 | 0.297960 | 0.027461 | 0.033971 | . 1 0.102366 | 0.222080 | 0.266058 | 0.199353 | 0.070404 | 0.133511 | 0.047168 | 0.092298 | 0.081233 | 0.076513 | 0.023751 | 0.045091 | . 2 0.567363 | 0.558700 | 0.825253 | 0.756242 | 0.211473 | 0.430789 | 0.222234 | 0.229470 | 0.273319 | 0.307549 | 0.066604 | 0.118382 | . Ok. The obtained X matrix looks fairly similar. How about we reverse the constraints. . e_rev = &quot;[&quot; for a in range(N/2): e_rev+=&quot; 2* X[:,%d] &lt; X[:,%d],&quot; %(a, a+N/2) e_rev = e_rev[:-1] + &quot;]&quot; e_rev . &#39;[ 2* X[:,0] &lt; X[:,6], 2* X[:,1] &lt; X[:,7], 2* X[:,2] &lt; X[:,8], 2* X[:,3] &lt; X[:,9], 2* X[:,4] &lt; X[:,10], 2* X[:,5] &lt; X[:,11]]&#39; . X_c_rev, Y_c_rev, r_c_rev = nmf_features(A, 3, MAX_ITERS=20, input_constraints_X=e_rev) . pd.DataFrame(X_c_rev) . 0 1 2 3 4 5 6 7 8 9 10 11 . 0 0.250945 | 0.038070 | 0.174189 | 0.252085 | 0.033251 | 0.069176 | 0.502026 | 0.076147 | 0.348450 | 0.504277 | 0.066521 | 0.138405 | . 1 0.030757 | 0.088033 | 0.085947 | 0.065135 | 0.024395 | 0.045976 | 0.061398 | 0.176002 | 0.171773 | 0.130146 | 0.048760 | 0.091882 | . 2 0.220256 | 0.183292 | 0.269014 | 0.282814 | 0.065713 | 0.128120 | 0.440553 | 0.366600 | 0.538065 | 0.565669 | 0.131436 | 0.256263 | . There you go! We now have learnt latent factors that conform to our constraints. .",
            "url": "https://nipunbatra.github.io/blog/ml/2017/04/21/constrained-nmf-cvx.html",
            "relUrl": "/ml/2017/04/21/constrained-nmf-cvx.html",
            "date": " • Apr 21, 2017"
        }
        
    
  
    
        ,"post30": {
            "title": "Out of Tensor factorisation",
            "content": "In a previous post, we had looked at predicting for users who weren&#39;t a part of the original matrix factorisation. In this post, we&#39;ll look at the same for 3-d tensors. In case you want to learn more about tensor factorisations, look at my earlier post. . General Tensor Factorisation . . General tensor factorisation for a 3d tensor A (M X N X O) would produce 3 factors- X (M X K), Y (N X K) and Z (O X K). The $A_{ijl}$ entry can be found as (Khatri product) : . $$ A_{ijl} = sum_k{X_{ik}Y_{jk}Z_{lk}}$$ . Learning $X_M$ factor . However, we&#39;d assume that the $M^{th}$ entry isn&#39;t a part of this decomposition. So, how do we obtain the X factors correspondonding to $M^{th}$ entry? We learn the Y and Z factors from the tensor A (excluding the $M^{th}$ row entries). We assume the Y and Z learnt to be shared across the entries across rows of A (1 through M). . . The above figure shows the latent factor for X ($X_{M}$) corresponding to the $M^{th}$ entry of X that we wish to learn. On the LHS, we see the matrix corresponding to $A_{M}$. The highlighted entry of $A_{M}$ is created by element-wise multiplication of $X_M, Y_0, Z_0$ and then summing. Thus, each of the N X O entries of $A_M$ are created by multiplying $X_M$ with a row from Y and a row from Z. In general, . $$A_{M, n, o} = sum_k{X_{M, k} times Y_{n, k} times Z_{o, k}}$$ . Now, to learn $X_M$, we plan to use least squares. For that, we need to reduce the problem into $ alpha x = beta$ We do this as follows: . We flatten out the A_M matrix into a vector containing N X O entries and call it $ beta$ | We create a matrix by element-wise multiplication of each row of Y with each row of Z to create $ alpha$ of shape (N X O, K) | We can now write, . $$ alpha X_M^T approx beta $$ Thus, X_M^T = Least Squares ($ alpha, beta$) . Ofcourse, $ beta$ can have missing entries, which we mask out. Thus, we can write: . $X_M^T$ = Least Squares ($ alpha [Mask], beta [Mask]$) . In case we&#39;re doing a non-negative tensor factorisation, we can instead learn $X_M^T$ as follows: $X_M^T$ = Non-negative Least Squares ($ alpha [Mask], beta [Mask]$) . Code example . import tensorly from tensorly.decomposition import parafac, non_negative_parafac import numpy as np . Creating the tensor to be decomposed . M, N, O = 10, 4, 3 #user, movie, feature t = np.arange(M*N*O).reshape(M, N, O).astype(&#39;float32&#39;) t[0] #First entry . array([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]], dtype=float32) . Setting a few entries of the last user to be unavailable/unknown . t_orig = t.copy() # creating a copy t[-1,:,:][0, 0] = np.NAN t[-1,:,:][2, 2] = np.NAN t[-1,:,:] . array([[ nan, 109., 110.], [ 111., 112., 113.], [ 114., 115., nan], [ 117., 118., 119.]], dtype=float32) . Standard Non-negative PARAFAC decomposition . K = 2 # Notice, we factorise a tensor with one less user. thus, t[:-1, :, :] X, Y, Z = non_negative_parafac(t[:-1,:,:], rank=K) . X.shape, Y.shape, Z.shape . ((9, 2), (4, 2), (3, 2)) . Y . array([[ 0.48012616, 1.13542261], [ 0.49409014, 2.98947262], [ 0.5072998 , 5.03375154], [ 0.52051081, 7.07682331]]) . Z . array([[ 0.57589198, 1.55655956], [ 0.58183329, 1.7695134 ], [ 0.58778163, 1.98182137]]) . Creating $ alpha$ by element-wise multiplication of Y, Z and reshaping . alpha = np.einsum(&#39;nk, ok -&gt; nok&#39;, Y, Z).reshape((N*O, K)) print alpha print &quot; nShape of alpha = &quot;, alpha.shape . [[ 0.27650081 1.76735291] [ 0.27935339 2.00914552] [ 0.28220934 2.25020479] [ 0.28454255 4.65329218] [ 0.28747809 5.28991186] [ 0.29041711 5.92460074] [ 0.29214989 7.83533407] [ 0.29516391 8.9072908 ] [ 0.29818151 9.9759964 ] [ 0.299758 11.01549695] [ 0.30285052 12.52253365] [ 0.30594669 14.02499969]] Shape of alpha = (12, 2) . from scipy.optimize import nnls . Creating $ beta$ . beta = t[-1,:,:].reshape(N*O, 1) mask = ~np.isnan(beta).flatten() beta[mask].reshape(-1, 1) . array([[ 109.], [ 110.], [ 111.], [ 112.], [ 113.], [ 114.], [ 115.], [ 117.], [ 118.], [ 119.]], dtype=float32) . Learning $X_M$ . X_M = nnls(alpha[mask], beta[mask].reshape(-1, ))[0].reshape((1, K)) X_M . array([[ 389.73825036, 0. ]]) . Comparing X_M with other entries from X . X . array([[ 7.40340055e-01, 7.62705972e-01], [ 4.14288653e+01, 7.57249713e-01], [ 8.51282259e+01, 6.56239315e-01], [ 1.29063811e+02, 5.46019997e-01], [ 1.73739412e+02, 4.06496594e-01], [ 2.19798887e+02, 2.11453297e-01], [ 2.64609697e+02, 6.54705290e-02], [ 3.01392149e+02, 2.39700484e-01], [ 3.39963876e+02, 3.41824756e-01]]) . It seems that the first column captures the increasing trend of values in the tensor . Predicting missing entries using tensor multiplication . np.round(np.einsum(&#39;ir, jr, kr -&gt; ijk&#39;, X_M, Y, Z)) . array([[[ 108., 109., 110.], [ 111., 112., 113.], [ 114., 115., 116.], [ 117., 118., 119.]]]) . Actual entries . t_orig[-1, :, :] . array([[ 108., 109., 110.], [ 111., 112., 113.], [ 114., 115., 116.], [ 117., 118., 119.]], dtype=float32) . Not bad! We&#39;re exactly there! .",
            "url": "https://nipunbatra.github.io/blog/ml/2017/04/20/parafac-out-tensor.html",
            "relUrl": "/ml/2017/04/20/parafac-out-tensor.html",
            "date": " • Apr 20, 2017"
        }
        
    
  
    
        ,"post31": {
            "title": "Out of matrix non-negative matrix factorisation",
            "content": "I have written a bunch of posts on this blog about non-negative matrix factorisation (NNMF). However, all of them involved the test user to be a part of the matrix that we factorise to learn the latent factors. Is that always the case? Read on to find more! . Standard Problem . Our goal is given a matrix A, decompose it into two non-negative factors, as follows: . $ A_{M times N} approx W_{M times K} times H_{K times N} $, such that $ W_{M times K} ge 0$ and $ H_{K times N} ge 0$ . . Our Problem- Out of matrix factorisation . Imagine that we have trained the model for M-1 users on N movies. Now, the $M^{th}$ user has rated some movies. Do we retrain the model from scratch to consider $M^{th}$ user? This can be a very expensive operation! . . Instead, as shown in above figure, we will learn the user factor for the $M^{th}$ user. We can do this the shared movie factor (H) has already been learnt. . We can formulate as follows: . $$ A[M,:] = W[M,:]H $$Taking transpose both sides . $$ A[M,:]^T = H^T W[M,:]^T $$ However, $A[M,:]^T$ will have missing entries. Thus, we can mask those entries from the calculation as shown below. . . Thus, we can write . $$ W[M,:]^T = mathrm{Least Squares} (H^T[Mask], A[M,:]^T[Mask]) $$If instead we want the factors to be non-negative, we can use non-negative least squares instead of usual least squares for this estimation. . Code example . I&#39;ll now present a simple code example to illustrate the procedure. . Defining matrix A . import numpy as np import pandas as pd M, N = 20, 10 np.random.seed(0) A_orig = np.abs(np.random.uniform(low=0.0, high=1.0, size=(M,N))) pd.DataFrame(A_orig).head() . 0 1 2 3 4 5 6 7 8 9 . 0 0.548814 | 0.715189 | 0.602763 | 0.544883 | 0.423655 | 0.645894 | 0.437587 | 0.891773 | 0.963663 | 0.383442 | . 1 0.791725 | 0.528895 | 0.568045 | 0.925597 | 0.071036 | 0.087129 | 0.020218 | 0.832620 | 0.778157 | 0.870012 | . 2 0.978618 | 0.799159 | 0.461479 | 0.780529 | 0.118274 | 0.639921 | 0.143353 | 0.944669 | 0.521848 | 0.414662 | . 3 0.264556 | 0.774234 | 0.456150 | 0.568434 | 0.018790 | 0.617635 | 0.612096 | 0.616934 | 0.943748 | 0.681820 | . 4 0.359508 | 0.437032 | 0.697631 | 0.060225 | 0.666767 | 0.670638 | 0.210383 | 0.128926 | 0.315428 | 0.363711 | . Masking a few entries . A = A_orig.copy() A[0, 0] = np.NAN A[3, 1] = np.NAN A[6, 3] = np.NAN # Masking for last user. A[19, 2] = np.NAN A[19, 7] = np.NAN . We will be using A2 (first 19 users) matrix for learning the movie factors and the user factors for the 19 users . A2 = A[:-1,:] A2.shape . (19, 10) . A_df = pd.DataFrame(A) A_df.head() . 0 1 2 3 4 5 6 7 8 9 . 0 NaN | 0.715189 | 0.602763 | 0.544883 | 0.423655 | 0.645894 | 0.437587 | 0.891773 | 0.963663 | 0.383442 | . 1 0.791725 | 0.528895 | 0.568045 | 0.925597 | 0.071036 | 0.087129 | 0.020218 | 0.832620 | 0.778157 | 0.870012 | . 2 0.978618 | 0.799159 | 0.461479 | 0.780529 | 0.118274 | 0.639921 | 0.143353 | 0.944669 | 0.521848 | 0.414662 | . 3 0.264556 | NaN | 0.456150 | 0.568434 | 0.018790 | 0.617635 | 0.612096 | 0.616934 | 0.943748 | 0.681820 | . 4 0.359508 | 0.437032 | 0.697631 | 0.060225 | 0.666767 | 0.670638 | 0.210383 | 0.128926 | 0.315428 | 0.363711 | . Defining matrices W and H (learning on M-1 users and N movies) . K = 4 W = np.abs(np.random.uniform(low=0, high=1, size=(M-1, K))) H = np.abs(np.random.uniform(low=0, high=1, size=(K, N))) W = np.divide(W, K*W.max()) H = np.divide(H, K*H.max()) . pd.DataFrame(W).head() . 0 1 2 3 . 0 0.078709 | 0.175784 | 0.095359 | 0.045339 | . 1 0.006230 | 0.016976 | 0.171505 | 0.114531 | . 2 0.135453 | 0.226355 | 0.250000 | 0.054753 | . 3 0.167387 | 0.066473 | 0.005213 | 0.191444 | . 4 0.080785 | 0.096801 | 0.148514 | 0.209789 | . pd.DataFrame(H).head() . 0 1 2 3 4 5 6 7 8 9 . 0 0.239700 | 0.203498 | 0.160529 | 0.222617 | 0.074611 | 0.216164 | 0.157328 | 0.003370 | 0.088415 | 0.037721 | . 1 0.250000 | 0.121806 | 0.126649 | 0.162827 | 0.093851 | 0.034858 | 0.209333 | 0.048340 | 0.130195 | 0.057117 | . 2 0.024914 | 0.219537 | 0.247731 | 0.244654 | 0.230833 | 0.197093 | 0.084828 | 0.020651 | 0.103694 | 0.059133 | . 3 0.033735 | 0.013604 | 0.184756 | 0.002910 | 0.196210 | 0.037417 | 0.020248 | 0.022815 | 0.171121 | 0.062477 | . Defining the cost that we want to minimise . def cost(A, W, H): from numpy import linalg WH = np.dot(W, H) A_WH = A-WH return linalg.norm(A_WH, &#39;fro&#39;) . However, since A has missing entries, we have to define the cost in terms of the entries present in A . def cost(A, W, H): from numpy import linalg mask = pd.DataFrame(A).notnull().values WH = np.dot(W, H) WH_mask = WH[mask] A_mask = A[mask] A_WH_mask = A_mask-WH_mask # Since now A_WH_mask is a vector, we use L2 instead of Frobenius norm for matrix return linalg.norm(A_WH_mask, 2) . Let us just try to see the cost of the initial set of values of W and H we randomly assigned. Notice, we pass A2! . cost(A2, W, H) . 7.2333001567031294 . Alternating NNLS procedure . num_iter = 1000 num_display_cost = max(int(num_iter/10), 1) from scipy.optimize import nnls for i in range(num_iter): if i%2 ==0: # Learn H, given A and W for j in range(N): mask_rows = pd.Series(A2[:,j]).notnull() H[:,j] = nnls(W[mask_rows], A2[:,j][mask_rows])[0] else: for j in range(M-1): mask_rows = pd.Series(A2[j,:]).notnull() W[j,:] = nnls(H.transpose()[mask_rows], A2[j,:][mask_rows])[0] WH = np.dot(W, H) c = cost(A2, W, H) if i%num_display_cost==0: print i, c . 0 3.74162948918 100 2.25416363991 200 2.25258698617 300 2.25229707846 400 2.25131714233 500 2.24968386447 600 2.24967129897 700 2.24965023589 800 2.24961410381 900 2.24955008837 . A_pred = pd.DataFrame(np.dot(W, H)) A_pred.head() . 0 1 2 3 4 5 6 7 8 9 . 0 0.590301 | 0.653038 | 0.531940 | 0.623272 | 0.584763 | 0.630835 | 0.574041 | 0.700139 | 0.841706 | 0.565808 | . 1 0.802724 | 0.532299 | 0.482430 | 1.017968 | 0.149923 | 0.449312 | 0.097775 | 0.708727 | 0.506595 | 0.846219 | . 2 0.764296 | 0.563711 | 0.527292 | 0.905236 | 0.306275 | 0.505674 | 0.223192 | 0.705882 | 0.604356 | 0.757878 | . 3 0.373539 | 0.745239 | 0.334948 | 0.663219 | 0.132686 | 0.551844 | 0.760420 | 0.598546 | 0.808108 | 0.627732 | . 4 0.467623 | 0.331457 | 0.617263 | 0.239449 | 0.634455 | 0.370041 | 0.294412 | 0.288539 | 0.484822 | 0.126945 | . Learning home factors for $M^{th}$ home . A_m = A[-1,:] A_m_transpose = A_m.T mask = ~np.isnan(A_m_transpose) W_m = nnls(H.T[mask], A_m_transpose[mask])[0] . W_m . array([ 0.12248095, 0.20778687, 0.15185613, 0. ]) . Predicting for $M^{th}$ home . ratings_m_home = np.dot(H.T, W_m) . ratings_m_home[~mask] . array([ 0.4245947 , 0.57447552]) . A_orig[-1,:][~mask] . array([ 0.18619301, 0.25435648]) . There you go, we are able to get ratings for the $M^{th}$ user for the movies that they have not seen. We only trained the model on the other users! Ofcourse, these numbers might not look so impressive. However, this was just a toy example based on random data. In reality, we could expect better results! .",
            "url": "https://nipunbatra.github.io/blog/ml/2017/04/19/nmf-out-matrix.html",
            "relUrl": "/ml/2017/04/19/nmf-out-matrix.html",
            "date": " • Apr 19, 2017"
        }
        
    
  
    
        ,"post32": {
            "title": "Coin tosses and MCMC",
            "content": "In the previous post, I had discussed regarding the EM algorithm from a programmer&#39;s perspective. That blog post sparked an interesting discussion on Twitter which has led to the current post. @twiecki Do you think it would be interesting to solve this problem using MCMC methods? . &mdash; Nipun Batra (@nipun_batra) April 27, 2014 . Thus, I started to investiage how good would MCMC methods perform in comparison to EM. . Problem statement . For a detailed understanding, refer to the previous post. In short, the problem is as follows. . You have two coins - A and B. Both of them have their own biases (probability of obtaining a head (or a tail )). We pick a coin at random and toss it up 10 times. We repeat this procedure 5 times, totaling in 5 observation sets of 10 observations each. However, we are not told which coin was tossed. So, looking at the data and some rough initial guess about the respective coin biases, can we tell something about the likely biases? Let us work it up using PyMC. . Customary imports . import pymc as pm import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats import seaborn as sns %matplotlib inline . Observations . We use 1 for Heads and 0 for tails. . # 5 observation sets of 10 observations each observations = np.array([[1,0,0,0,1,1,0,1,0,1], [1,1,1,1,0,1,1,1,1,1], [1,0,1,1,1,1,1,0,1,1], [1,0,1,0,0,0,1,1,0,0], [0,1,1,1,0,1,1,1,0,1]]) observations_flattened = observations.flatten() . Number of heads in each obervation set . n_heads_array = np.sum(observations, axis=1) plt.bar(range(len(n_heads_array)),n_heads_array.tolist()); plt.title(&quot;Number of heads vs Observation set&quot;) plt.xlabel(&quot;Observation set&quot;) plt.ylabel(&quot;#Heads observed&quot;); . Ground truth . The true sequence of coin tosses which was hidden from us. True indicated coin A and False indicates coin B. . # Ground truth coins_id = np.array([False,True,True,False,True]) . Number of observation sets and number of observations in each set. This allows us to modify data easily (as opposed to hard coding stuff). . n_observation_sets = observations.shape[0] n_observation_per_set = observations.shape[1] . Model for the problem statement . We pick up a simple prior on the bias of coin A. . $ theta_a$ ~ $ beta(h_a,t_a)$ . Similarly, for coin B. . $ theta_b$ ~ $ beta(h_b,t_b)$ . For any given observation set, we assign equal probability to it coming from coin A or B. . $coin choice$ ~ DiscreteUniform(0,1) . Thus, if coin choice is known, then the associated bias term is fixed. . $ theta$ = $ theta_a$ if $coin choice$ =1 else $ theta_b$ . Like, we did in the previous post, we use Binomial distribution with the bias as $ theta$. For each observation set, we calculate the number of heads observed and model it as our observed variable obs. . $obs$ = Binomial(n_tosses_per_observation_set, p = $ theta$ ) . Let us draw this model using daft. . import daft pgm = daft.PGM([3.6, 2.7], origin=[1, 0.65]) pgm.add_node(daft.Node(&quot;theta_a&quot;, r&quot;$ theta_a$&quot;, 4, 3, aspect=1.8)) pgm.add_node(daft.Node(&quot;theta_b&quot;, r&quot;$ theta_b$&quot;, 3, 3, aspect=1.2)) pgm.add_node(daft.Node(&quot;theta&quot;, r&quot;$ theta$&quot;, 3.5, 2, aspect=1.8)) pgm.add_node(daft.Node(&quot;coin_choice&quot;, r&quot;coin_choice&quot;, 2, 2, aspect=1.8)) pgm.add_node(daft.Node(&quot;obs&quot;, r&quot;obs&quot;, 3.5, 1, aspect=1.2, observed=True)) pgm.add_edge(&quot;theta_a&quot;, &quot;theta&quot;) pgm.add_edge(&quot;theta_b&quot;, &quot;theta&quot;) pgm.add_edge(&quot;coin_choice&quot;, &quot;theta&quot;) pgm.add_edge(&quot;theta&quot;, &quot;obs&quot;) pgm.render(); . The following segment codes the above model. . # Prior on coin A (For now we choose 2 H, 2 T) theta_a = pm.Beta(&#39;theta_a&#39;,2,2) # Prior on coin B (For now we choose 2 H, 2 T) theta_b = pm.Beta(&#39;theta_b&#39;,2,2) # Choosing either A or B coin_choice_array = pm.DiscreteUniform(&#39;coin_choice&#39;,0,1, size = n_observation_sets) # Creating a theta (theta_a if A is tossed or theta_b if B is tossed) @pm.deterministic def theta(theta_a = theta_a, theta_b=theta_b, coin_choice_array=coin_choice_array): #print coin_choice_array out = np.zeros(n_observation_sets) for i, coin_choice in enumerate(coin_choice_array): if coin_choice: out[i] = theta_a else: out[i] = theta_b return out . Let us examine how theta would be related to coin choice and other variables we have defined. . theta_a.value . array(0.46192564399429575) . theta_b.value . array(0.5918506713420255) . coin_choice_array.value . array([1, 1, 0, 0, 1]) . theta.value . array([ 0.46192564, 0.46192564, 0.59185067, 0.59185067, 0.46192564]) . So, whenever we see coin A, we put it&#39;s bias in theta and likewise if we observe coin B. Now, let us create a model for our observations which is binomial as discussed above. . observation_model = pm.Binomial(&quot;obs&quot;, n=n_observation_per_set, p=theta, value = n_heads_array, observed=True) model = pm.Model([observation_model, theta_a, theta_b, coin_choice_array]) . Let us view a few samples from the observation_model which returns the number of heads in 5 sets of 10 tosses. . observation_model.random() . array([4, 4, 7, 2, 6]) . observation_model.random() . array([4, 4, 7, 5, 2]) . mcmc = pm.MCMC(model) mcmc.sample(40000, 10000, 1) . [--100%--] 40000 of 40000 complete in 7.8 sec . Let us have a look at our posteriors for $ theta_a$ and $ theta_b$ . plt.hist(mcmc.trace(&#39;theta_a&#39;)[:], bins=20,alpha=0.7,label = r&quot;$ theta_a$&quot;); plt.hist(mcmc.trace(&#39;theta_b&#39;)[:], bins=20,alpha=0.4,label= r&quot;$ theta_b$&quot;); plt.legend(); . Looks like both $ theta_a$ and $ theta_b$ peak around the same value. But, wasn&#39;t it expected? We gave both of them the same priors. This was also the case when we initialed EM with same values for both $ theta_a$ and $ theta_b$. So, let us add some informative priors an see how we do. Like in EM experiment, we knew that one of the coin was more biased than the other. So, let us make that the case and rerun the experiment. . More informative priors . # Prior on coin A (more likely to have heads) theta_a = pm.Beta(&#39;theta_a&#39;,4,2) # Prior on coin B (more likely to have tails) theta_b = pm.Beta(&#39;theta_b&#39;,2,4) # Choosing either A or B (for 5 observations) coin_choice_array = pm.DiscreteUniform(&#39;coin_choice&#39;,0,1, size = 5) # Creating a theta (theta_a if A is tossed or theta_b if B is tossed) @pm.deterministic def theta(theta_a = theta_a, theta_b=theta_b, coin_choice_array=coin_choice_array): #print coin_choice_array out = np.zeros(n_observation_sets) for i, coin_choice in enumerate(coin_choice_array): if coin_choice: out[i] = theta_a else: out[i] = theta_b return out . observation_model = pm.Binomial(&quot;obs&quot;, n=10, p=theta, value = n_heads_array, observed=True) model = pm.Model([observation_model, theta_a, theta_b, coin_choice_array]) . mcmc = pm.MCMC(model) mcmc.sample(40000, 10000, 1) . [--100%--] 40000 of 40000 complete in 8.0 sec . plt.hist(mcmc.trace(&#39;theta_a&#39;)[:], bins=20,alpha=0.7,label = r&quot;$ theta_a$&quot;); plt.hist(mcmc.trace(&#39;theta_b&#39;)[:], bins=20,alpha=0.4,label= r&quot;$ theta_b$&quot;); plt.legend(); . Quiet a clear distinction now! $ theta_a$ seems to peak around 0.8 and $ theta_b$ around 0.5. This matches our results from EM. However, unlike EM, we have much more than point estimates. . Feel free to leave your comments and suggestions by. .",
            "url": "https://nipunbatra.github.io/blog/ml/2014/07/01/mcmc_coins.html",
            "relUrl": "/ml/2014/07/01/mcmc_coins.html",
            "date": " • Jul 1, 2014"
        }
        
    
  
    
        ,"post33": {
            "title": "Latexify Matplotlib",
            "content": "Every time I would prepare a matplotlib graph for a paper, I would iteratively adjust the figure size, the font size, scaling in LaTeX. This turns to be a tedious process. Fortunately, when along with Jack and Oliver, I was writing our nilmtk paper, Jack demonstrated a function to &quot;latexify&quot; plots. This function would take care of font sizes and scaling, so that in one go one could generate a plot and stick in LaTeX. In this post I&#39;ll illustrate this technique to save all that iterative effort and make plots look nicer. . import matplotlib.pyplot as plt import numpy as np import pandas as pd import matplotlib . from math import sqrt SPINE_COLOR = &#39;gray&#39; . The following is the latexify function. It allows you to create 2 column or 1 column figures. You may also wish to alter the height or width of the figure. The default settings are good for most cases. You may also change the parameters such as labelsize and fontsize based on your classfile. For this post, I&#39;ll use the following ACM classfile. . def latexify(fig_width=None, fig_height=None, columns=1): &quot;&quot;&quot;Set up matplotlib&#39;s RC params for LaTeX plotting. Call this before plotting a figure. Parameters - fig_width : float, optional, inches fig_height : float, optional, inches columns : {1, 2} &quot;&quot;&quot; # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples # Width and max height in inches for IEEE journals taken from # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf assert(columns in [1,2]) if fig_width is None: fig_width = 3.39 if columns==1 else 6.9 # width in inches if fig_height is None: golden_mean = (sqrt(5)-1.0)/2.0 # Aesthetic ratio fig_height = fig_width*golden_mean # height in inches MAX_HEIGHT_INCHES = 8.0 if fig_height &gt; MAX_HEIGHT_INCHES: print(&quot;WARNING: fig_height too large:&quot; + fig_height + &quot;so will reduce to&quot; + MAX_HEIGHT_INCHES + &quot;inches.&quot;) fig_height = MAX_HEIGHT_INCHES params = {&#39;backend&#39;: &#39;ps&#39;, &#39;text.latex.preamble&#39;: [r&#39; usepackage{gensymb}&#39;], &#39;axes.labelsize&#39;: 8, # fontsize for x and y labels (was 10) &#39;axes.titlesize&#39;: 8, &#39;font.size&#39;: 8, # was 10 &#39;legend.fontsize&#39;: 8, # was 10 &#39;xtick.labelsize&#39;: 8, &#39;ytick.labelsize&#39;: 8, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: [fig_width,fig_height], &#39;font.family&#39;: &#39;serif&#39; } matplotlib.rcParams.update(params) def format_axes(ax): for spine in [&#39;top&#39;, &#39;right&#39;]: ax.spines[spine].set_visible(False) for spine in [&#39;left&#39;, &#39;bottom&#39;]: ax.spines[spine].set_color(SPINE_COLOR) ax.spines[spine].set_linewidth(0.5) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.yaxis.set_ticks_position(&#39;left&#39;) for axis in [ax.xaxis, ax.yaxis]: axis.set_tick_params(direction=&#39;out&#39;, color=SPINE_COLOR) return ax . %matplotlib inline . Let us create a dummy data frame . df = pd.DataFrame(np.random.randn(10,2)) df.columns = [&#39;Column 1&#39;, &#39;Column 2&#39;] . ax = df.plot() ax.set_xlabel(&quot;X label&quot;) ax.set_ylabel(&quot;Y label&quot;) ax.set_title(&quot;Title&quot;) plt.tight_layout() plt.savefig(&quot;image1.pdf&quot;) . Now, let us call the latexify function to alter matplotlib parameters suited to our LaTeX classfile. . latexify() . ax = df.plot() ax.set_xlabel(&quot;X label&quot;) ax.set_ylabel(&quot;Y label&quot;) ax.set_title(&quot;Title&quot;) plt.tight_layout() format_axes(ax) plt.savefig(&quot;image2.pdf&quot;) . Let us have a quick look at our latex source file. I have scaled down the plot generated by default matploltib settings by 50%. The next plot which is generated using latexified settings doesn&#39;t need any scaling. . ! cat 1.tex . documentclass{sig-alternate} title{Python plots for LaTeX} begin{document} maketitle section{Introduction} Some random text out here! noindent begin{figure} centering includegraphics[scale=0.5]{image1.pdf} caption{Scaled down of the originial matplotlib plot. Now, the text looks very small.} label{image1} end{figure} begin{figure} centering includegraphics{image2.pdf} caption{LaTeXified plot :)} label{image2} end{figure} end{document} . Finally, let us look at the &quot;png&quot; version of our generated pdf. . . Clearly, the LaTeXified version is likely to save you a lot of figure tweaking! You don&#39;t need to play with different scaling settings. Nor, do you have to play with font sizes and ofcourse not with a combination of these two which can be pretty hard. .",
            "url": "https://nipunbatra.github.io/blog/visualisation/2014/06/02/latexify.html",
            "relUrl": "/visualisation/2014/06/02/latexify.html",
            "date": " • Jun 2, 2014"
        }
        
    
  
    
        ,"post34": {
            "title": "Programatically understanding Expectation Maximization",
            "content": "I was recently studying the Expectation Maximization algorithm from this well cited Nature article. To be honest, I found it hard to get all the maths right initially. I had to infact resort to looking up a few forums to get a clear understanding of this algorithm. I decided to take a programming approach to clear up some concepts in this problem. Another reason to do this is the amount of calculations needed in this algorithm (though not difficult, can be time consuming). . Maximum Likelihood . We are given two coins- A and B. Both these coins have a certain probability of getting heads. We choose one of the coin at random (with equal probability) and toss it 10 times noting down the heads-tails pattern. We also carefully account which coin was thrown. We repeat this procedure 5 times. The coin tosses observed in this case are show in the figure below in case A. . Our aim is to determine the probability of getting a head on coin A and likewise for coin B. Intuitively, if we add up the number of heads observed when A was thrown and divide it by the total number of times A was tossed, we woud get this number. This comes from the well known principle of Maximum Likelihood. . . This procedure of tossing a coin which may land as either heads or tails is an example of a Bernoulli trial. As per this Wiki page, its definition is as follows: . In the theory of probability and statistics, a Bernoulli trial (or binomial trial) is a random experiment with exactly two possible outcomes, &quot;success&quot; and &quot;failure&quot;, in which the probability of success is the same every time the experiment is conducted . When $n$ such trials are performed, it is called a binomial experiment. In the case of the coin toss experiment, if we have: $n$ coin toss $p$ probability of head in each trial -&gt; $1-p$ probability of head in each throw . then we observe $k$ heads as per the following: . $${n choose{k}} p^k(1-p)^{n-k}$$ . Let us write some code to see how this function varies. We fix $n$=10 and for varying $p$ and observe how the probability distirbution(pmf) varies. . import numpy as np from scipy import stats import matplotlib.pyplot as plt import seaborn as sns from ipywidgets import StaticInteract, RangeWidget %matplotlib inline . a=range(11) def plot_binomial(p=0.5): fig, ax = plt.subplots(figsize=(4,3)) y = [0]*11 for i in a: y[i-1] = stats.binom.pmf(i, 10, p) ax.bar(a,y,label=&quot;$p = %.1f$&quot; % p) ax.set_ylabel(&#39;PMF of $k$ heads&#39;) ax.set_xlabel(&#39;$k$&#39;) ax.set_ylim((0,0.5)) ax.set_xlim((0,10)) ax.legend() return fig . StaticInteract(plot_binomial, p=RangeWidget(0.0,1.0,0.1)) . p: As expected, as we increase $p$, there is a higher probability of getting more heads. Now, let us look at the Maximum likelihood problem. Say, we are given that we made $n$ coin tosses and obtained $x$ heads. Using this information and the nature of distribution, let us find what value of $p$ would have most likely resulted in this data. . If we choose the probability of heads to be $p$, the likelihood (probability) of obtaining $x$ heads in $n$ throws in our data $D $ is: $$L(D| theta) propto p^x(1-p)^{n-x}$$ If we differentiate this term wrt $p$ and equate to 0, we get: $$ xp^{x-1}(1-p)^{n-x} - p^x (n-x)(1-p)^{n-x-1} = 0$$ or $$ p^{x-1}(1-p)^{n-x-1}[x(1-p) - p(n-x)]= 0$$ or $$ x - xp - pn + xp = 0$$ or $$ p = frac{x}{n}$$ . This also generalizes over when multiple sets of throws are done. Thus, in the figure shown above, we find the probability of head for coins A and coins B, given the data, as : $$ P (A=head|D) = frac{ mathrm{heads~observed~when~A~was~thrown}}{ mathrm{times~A~was~thrown}}$$ $$ = frac{24}{24+6} = 0.8$$ Similarly, for coin B, we can find this ratio as 0.45. . Problem description . Now, we come to the main problem. What if we didn&#39;t note down which coin was thrown in which iteration. Can we still find out the probabilities of heads for the different coins. It turns out that we can use the EM algorithm for this task. I would recommend reading the theory in the nature paper before resuming this section. Now, we present the programming route to EM and its different components under this setting. . Creating the dataset . A head is labeled as a 1 and a tail as a 0. The five rows correspond to the five set of throws. . observations = np.array([[1,0,0,0,1,1,0,1,0,1], [1,1,1,1,0,1,1,1,1,1], [1,0,1,1,1,1,1,0,1,1], [1,0,1,0,0,0,1,1,0,0], [0,1,1,1,0,1,1,1,0,1]]) . The true coin choice- A =True, B= False . coins_id = np.array([False,True,True,False,True]) . Completely observed case . As discussed before, if we know which coin was used when, this task reduces to Maximum likelihood. . The sets of observations corresponding to coin A can be found as: . observations[coins_id] . array([[1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 0, 1, 1], [0, 1, 1, 1, 0, 1, 1, 1, 0, 1]]) . Number of heads recorded when A was thrown . np.sum(observations[coins_id]) . 24 . Thus, the probability of head for A given the data would be: . 1.0*np.sum(observations[coins_id])/observations[coins_id].size . 0.8 . The same quantity for coin B would be the following: . 1.0*np.sum(observations[~coins_id])/observations[~coins_id].size . 0.45 . Unseen data settings . Now, we follow the Step b in the first figure on this page (which is Figure 1 from the Nature article) . Step 0: Assuming initial set of parameters . $$ theta_A^0 = 0.6$$ $$ theta_B^0 = 0.5$$ . Iteration 1, Step 1 : E-step . Let us take the first group of observation. We have 5 Heads and 5 tails. PMF according to binomial distribution is given by : ${n choose{k}}p^k(1-p)^{n-k}$. For coin A we have $p= theta^0_A=0.6$ and $n=10$. We calculate the $pmf$ as follows: . coin_A_pmf_observation_1 = stats.binom.pmf(5,10,0.6) . coin_A_pmf_observation_1 . 0.20065812480000034 . Similarly, for coin B, we get . coin_B_pmf_observation_1 = stats.binom.pmf(5,10,0.5) . coin_B_pmf_observation_1 . 0.24609375000000025 . Since coin_B_pmf_observation_1 is greater than coin_A_pmf_observation_1, coin B is more likely to have produced the sequence of 5 Heads and 5 Tails. We now normalize these $pmfs$ to 1 and $weigh$ our observations. . normalized_coin_A_pmf_observation_1 = coin_A_pmf_observation_1/(coin_A_pmf_observation_1+coin_B_pmf_observation_1) print &quot;%0.2f&quot; %normalized_coin_A_pmf_observation_1 normalized_coin_B_pmf_observation_1 = coin_B_pmf_observation_1/(coin_A_pmf_observation_1+coin_B_pmf_observation_1) print &quot;%0.2f&quot; %normalized_coin_B_pmf_observation_1 . 0.45 0.55 . We now weigh in the observations according to this ratio. Thus, for observation set 1, we have: . weighted_heads_A_obervation_1 = 5*normalized_coin_A_pmf_observation_1 print &quot;Coin A Weighted count for heads in observation 1: %0.2f&quot; %weighted_heads_A_obervation_1 weighted_tails_A_obervation_1 = 5*normalized_coin_A_pmf_observation_1 print &quot;Coin A Weighted count for tails in observation 1: %0.2f&quot; %weighted_tails_A_obervation_1 weighted_heads_B_obervation_1 = 5*normalized_coin_B_pmf_observation_1 print &quot;Coin B Weighted count for heads in observation 1: %0.2f&quot; %weighted_heads_B_obervation_1 weighted_tails_B_obervation_1 = 5*normalized_coin_B_pmf_observation_1 print &quot;Coin B Weighted count for tails in observation 1: %0.2f&quot; %weighted_tails_B_obervation_1 . Coin A Weighted count for heads in observation 1: 2.25 Coin A Weighted count for tails in observation 1: 2.25 Coin B Weighted count for heads in observation 1: 2.75 Coin B Weighted count for tails in observation 1: 2.75 . We can similarly find out the weigted count in each observation set for both coin A and B. For now, we will pick up the numbers from the paper. . Iteration 1, Step 2: M-step . We now take the sum of weighted count of heads for both coin A and B. For coin A, we have 21.3 heads and 8.6 tails. Thus, we get the probability of getting a head at the end of this iteration as: . 21.3/(21.3+8.6) . 0.7123745819397994 . We can find the same quantity for coin B as well. Next, we repeat the same procedure using these latest calculated probabilities of obtaining heads for coins A and B. . EM single iteration . Let us now write a procedure to do a single iteration of the EM algorithm. The function takes in as argument the following: . priors $ theta_A$ and $ theta_B$ | observation matrix (5 X 10) in this case | . and outputs the new set of priors based on EM iteration. . def em_single(priors, observations): &quot;&quot;&quot; Performs a single EM step Arguments priors : [theta_A, theta_B] observations : [m X n matrix] Returns -- new_priors: [new_theta_A, new_theta_B] &quot;&quot;&quot; counts = {&#39;A&#39;:{&#39;H&#39;:0,&#39;T&#39;:0}, &#39;B&#39;:{&#39;H&#39;:0,&#39;T&#39;:0}} theta_A = priors[0] theta_B = priors[1] # E step for observation in observations: len_observation = len(observation) num_heads = observation.sum() num_tails = len_observation - num_heads contribution_A = stats.binom.pmf(num_heads,len_observation,theta_A) contribution_B = stats.binom.pmf(num_heads,len_observation,theta_B) weight_A = contribution_A/(contribution_A+contribution_B) weight_B = contribution_B/(contribution_A+contribution_B) # Incrementing counts counts[&#39;A&#39;][&#39;H&#39;]+= weight_A*num_heads counts[&#39;A&#39;][&#39;T&#39;]+= weight_A*num_tails counts[&#39;B&#39;][&#39;H&#39;]+= weight_B*num_heads counts[&#39;B&#39;][&#39;T&#39;]+= weight_B*num_tails # M step new_theta_A = counts[&#39;A&#39;][&#39;H&#39;]/(counts[&#39;A&#39;][&#39;H&#39;]+counts[&#39;A&#39;][&#39;T&#39;]) new_theta_B = counts[&#39;B&#39;][&#39;H&#39;]/(counts[&#39;B&#39;][&#39;H&#39;]+counts[&#39;B&#39;][&#39;T&#39;]) return [new_theta_A, new_theta_B] . EM procedure . This procedure calls the single EM iteration untill convergence or some stopping condition. We specificy two stopping conditions and the procedure stops when either condition is met. . 10000 iterations | the change in prior is less than 1e-6 | . def em(observations, prior, tol=1e-6, iterations=10000): import math iteration = 0 while iteration&lt;iterations: new_prior = em_single(prior, observations) delta_change = np.abs(prior[0]-new_prior[0]) if delta_change&lt;tol: break else: prior = new_prior iteration+=1 return [new_prior, iteration] . Results . Let us run the algorithm for the priors used in the paper . em(observations, [0.6,0.5]) . [[0.79678875938310978, 0.51958393567528027], 14] . Great! Our results match exactly! It took 14 iterations of the EM algorithm to reach this value. . What if we reverse the priors for A and B . em(observations, [0.5,0.6]) . [[0.51958345063012845, 0.79678895444393927], 15] . Ok. EM does not have a notion of A and B!! For it, there exists two coins and it agains finds the same results. . What if prior for both A and B were equal? . em(observations, [0.3,0.3]) . [[0.66000000000000003, 0.66000000000000003], 1] . So, this clearly is not a very good initialization strategy!! . What if one of the priors is very close to 1 . em(observations, [0.9999,0.00000001]) . [[0.79678850504581944, 0.51958235686544463], 13] . So EM is still smart enough! . References . Question on math stack exchange | Another question on math stack exhange | If you have any suggestions feel free let me know. .",
            "url": "https://nipunbatra.github.io/blog/ml/2014/06/01/em.html",
            "relUrl": "/ml/2014/06/01/em.html",
            "date": " • Jun 1, 2014"
        }
        
    
  
    
        ,"post35": {
            "title": "Gibbs sampling",
            "content": "Imports . import networkx as nx import itertools from matplotlib import rc rc(&quot;font&quot;, family=&quot;serif&quot;, size=12) rc(&quot;text&quot;, usetex=True) import daft import random import requests import numpy as np from IPython.display import HTML import matplotlib.pyplot as plt import matplotlib %matplotlib inline . Defining the network . network={ &quot;V&quot;: [&quot;Letter&quot;, &quot;Grade&quot;, &quot;Intelligence&quot;, &quot;SAT&quot;, &quot;Difficulty&quot;], &quot;E&quot;: [[&quot;Intelligence&quot;, &quot;Grade&quot;], [&quot;Difficulty&quot;, &quot;Grade&quot;], [&quot;Intelligence&quot;, &quot;SAT&quot;], [&quot;Grade&quot;, &quot;Letter&quot;]], &quot;Vdata&quot;: { &quot;Letter&quot;: { &quot;ord&quot;: 4, &quot;numoutcomes&quot;: 2, &quot;vals&quot;: [&quot;weak&quot;, &quot;strong&quot;], &quot;parents&quot;: [&quot;Grade&quot;], &quot;children&quot;: None, &quot;cprob&quot;: { &quot;[&#39;A&#39;]&quot;: [.1, .9], &quot;[&#39;B&#39;]&quot;: [.4, .6], &quot;[&#39;C&#39;]&quot;: [.99, .01] } }, &quot;SAT&quot;: { &quot;ord&quot;: 3, &quot;numoutcomes&quot;: 2, &quot;vals&quot;: [&quot;lowscore&quot;, &quot;highscore&quot;], &quot;parents&quot;: [&quot;Intelligence&quot;], &quot;children&quot;: None, &quot;cprob&quot;: { &quot;[&#39;low&#39;]&quot;: [.95, .05], &quot;[&#39;high&#39;]&quot;: [.2, .8] } }, &quot;Grade&quot;: { &quot;ord&quot;: 2, &quot;numoutcomes&quot;: 3, &quot;vals&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;], &quot;parents&quot;: [&quot;Difficulty&quot;, &quot;Intelligence&quot;], &quot;children&quot;: [&quot;Letter&quot;], &quot;cprob&quot;: { &quot;[&#39;easy&#39;, &#39;low&#39;]&quot;: [.3, .4, .3], &quot;[&#39;easy&#39;, &#39;high&#39;]&quot;: [.9, .08, .02], &quot;[&#39;hard&#39;, &#39;low&#39;]&quot;: [.05, .25, .7], &quot;[&#39;hard&#39;, &#39;high&#39;]&quot;: [.5, .3, .2] } }, &quot;Intelligence&quot;: { &quot;ord&quot;: 1, &quot;numoutcomes&quot;: 2, &quot;vals&quot;: [&quot;low&quot;, &quot;high&quot;], &quot;parents&quot;: None, &quot;children&quot;: [&quot;SAT&quot;, &quot;Grade&quot;], &quot;cprob&quot;: [.7, .3] }, &quot;Difficulty&quot;: { &quot;ord&quot;: 0, &quot;numoutcomes&quot;: 2, &quot;vals&quot;: [&quot;easy&quot;, &quot;hard&quot;], &quot;parents&quot;: None, &quot;children&quot;: [&quot;Grade&quot;], &quot;cprob&quot;: [.6, .4] } } } . Drawing the Bayesian Network . pgm = daft.PGM([8, 8], origin=[0, 0]) pgm.add_node(daft.Node(&#39;Difficulty&#39;,r&quot;Difficulty&quot;,2,6,aspect=3)) pgm.add_node(daft.Node(&#39;Intelligence&#39;,r&quot;Intelligence&quot;,5,6,aspect=3)) pgm.add_node(daft.Node(&#39;Grade&#39;,r&quot;Grade&quot;,3,4,aspect=3)) pgm.add_node(daft.Node(&#39;SAT&#39;,r&quot;SAT&quot;,6,4,aspect=3)) pgm.add_node(daft.Node(&#39;Letter&#39;,r&quot;Letter&quot;,4,2,aspect=3)) for node in network[&#39;Vdata&#39;]: parents=network[&#39;Vdata&#39;][node][&#39;parents&#39;] if parents is not None: for parent in parents: pgm.add_edge(parent, node) pgm.render() . &lt;matplotlib.axes._axes.Axes at 0x10e58b110&gt; . Finding the Markov blanket of a node . def find_markov_blanket(node,network): &#39;&#39;&#39; Find the Markov Blanket of the node in the given network Markov Blanket is given by: 1. The parents of the node 2. The children of the node 3. The parents of the children of the node &#39;&#39;&#39; mb=[] #Finding the parents of the node parents=network[&#39;Vdata&#39;][node][&#39;parents&#39;] if parents is not None: mb.append(parents) #Finding children of the node children=network[&#39;Vdata&#39;][node][&#39;children&#39;] if children is not None: mb.append(children) #Finding parent of each node for child in children: parents_child=network[&#39;Vdata&#39;][child][&#39;parents&#39;] if parents is not None: mb.append(parents) #Flattening out list of lists mb=list(itertools.chain(*mb)) #Removing repeated elements mb=list(set(mb)) return mb . find_markov_blanket(&#39;Grade&#39;,network) . [&#39;Difficulty&#39;, &#39;Letter&#39;, &#39;Intelligence&#39;] . Gibbs Sampling Procedures . Assigning a random state to a node in the network . def pick_random(node,network): &#39;&#39;&#39; Assigns a random state to a given node N &#39;&#39;&#39; num_outcomes=network[&#39;Vdata&#39;][node][&quot;numoutcomes&quot;] random_index=random.randint(0,num_outcomes-1) return network[&#39;Vdata&#39;][node][&quot;vals&quot;][random_index] . pick_random(&#39;SAT&#39;,network) . &#39;lowscore&#39; . Pick a random non evidence node to the update in the current iteration . def pick_random_non_evidence_node(non_evidence_nodes): return non_evidence_nodes[random.randint(0,len(non_evidence_nodes)-1)] . Update the value of a node given assignment in previous iteration . def get_next_value(node, network,simulation): parents_current_node_to_update=network[&#39;Vdata&#39;][node][&#39;parents&#39;] if parents_current_node_to_update is None: #The node has no parent and we can update it based on the prior cumsum=np.cumsum(network[&#39;Vdata&#39;][node][&quot;cprob&quot;]) else: #Find the row corresponding to the values of the parents in the previous iteration #NB We need to maintain the order, so we will do it values_parents=[simulation[-1][parent] for parent in parents_current_node_to_update] row=network[&#39;Vdata&#39;][node][&quot;cprob&quot;][str(values_parents)] cumsum=np.cumsum(row) choice=random.random() index=np.argmax(cumsum&gt;choice) return network[&#39;Vdata&#39;][node][&quot;vals&quot;][index] . Main procedure: Iteratively pick up a non evidence node to update . def gibbs_sampling(network, evidence, niter=2): simulation=[] nodes=network[&#39;V&#39;] non_evidence_nodes=[node for node in nodes if node not in evidence.keys()] #First iteration random value for all nodes d={} for node in nodes: d[node]=pick_random(node,network) #Put evidence for node in evidence: d[node]=evidence[node] simulation.append(d.copy()) #Now iterate for count in xrange(niter): #Pick up a random node to start current_node_to_update=pick_random_non_evidence_node(non_evidence_nodes) d[current_node_to_update]=get_next_value(current_node_to_update,network,simulation) simulation.append(d.copy()) return simulation . Illustration 1 . Distribution of Letter given that the student is Intelligent . iterations=int(1e4) sim=gibbs_sampling(network, {&quot;Intelligence&quot;:&quot;high&quot;},iterations) . Removing first 10% samples . after_removing_burnt_samples=sim[iterations/10:] count={val:0 for val in network[&#39;Vdata&#39;][&#39;Letter&#39;][&#39;vals&#39;]} . Finding the distribution of letter . for assignment in after_removing_burnt_samples: count[assignment[&#39;Letter&#39;]]+=1 . Counts . count . {&#39;strong&#39;: 7061, &#39;weak&#39;: 1940} . Counts to Probabilites . probabilites={} for l in count: probabilites[l]=count[l]*1.0/(.90*iterations) probabilites . {&#39;strong&#39;: 0.7845555555555556, &#39;weak&#39;: 0.21555555555555556} . Wait a min! What about the marginal distribution of Letter given NO evidence . iterations=int(1e4) sim=gibbs_sampling(network, {},iterations) after_removing_burnt_samples=sim[iterations/10:] count={val:0 for val in network[&#39;Vdata&#39;][&#39;Letter&#39;][&#39;vals&#39;]} for assignment in after_removing_burnt_samples: count[assignment[&#39;Letter&#39;]]+=1 probabilites_no_evidence={} for l in count: probabilites_no_evidence[l]=count[l]*1.0/(.90*iterations) probabilites_no_evidence . {&#39;strong&#39;: 0.4766666666666667, &#39;weak&#39;: 0.5234444444444445} . How does the evidence about &quot;Intelligent&quot; student affect the quality of letters? . plt.figure(figsize=(10, 8)) plt.subplot(2,2,1) plt.bar(range(2),[probabilites[&#39;strong&#39;],probabilites[&#39;weak&#39;]]) plt.xticks([0.5,1.5],[&#39;Strong&#39;,&#39;Weak&#39;]) plt.title(&#39;Letter Quality given Intelligent Student&#39;) plt.ylim((0,1.0)) plt.subplot(2,2,2) plt.bar(range(2),[probabilites_no_evidence[&#39;strong&#39;],probabilites_no_evidence[&#39;weak&#39;]]) plt.xticks([0.5,1.5],[&#39;Strong&#39;,&#39;Weak&#39;]) plt.title(&#39;Letter Quality given no prior information&#39;) plt.ylim((0,1.0)); .",
            "url": "https://nipunbatra.github.io/blog/ml/2014/05/01/gibbs-sampling.html",
            "relUrl": "/ml/2014/05/01/gibbs-sampling.html",
            "date": " • May 1, 2014"
        }
        
    
  
    
        ,"post36": {
            "title": "Programatically understanding dynamic time warping (DTW)",
            "content": "So you just recorded yourself saying a word and try to match it against another instance. The signals look similar, but have varying lengths and different activations for different features. So, how do you decide the similarity. Dynamic time warping (DTW) is probably something which can come to your rescue. Quoting wikipedia: . &quot;In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences which may vary in time or speed. For instance, similarities in walking patterns could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation.&quot; . In this post I will try and put forward a naive implementation of DTW and also explain the different pieces of the problem. . Customary imports . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . Creating two signals . x = np.array([1, 1, 2, 3, 2, 0]) y = np.array([0, 1, 1, 2, 3, 2, 1]) . Plotting the two signals . plt.plot(x,&#39;r&#39;, label=&#39;x&#39;) plt.plot(y, &#39;g&#39;, label=&#39;y&#39;) plt.legend(); . So, it appears that both the signals show similar behaviour: they both have a peak and around the peak they slop downwards. They vary in their speed and total duration. So, all set for DTW. . Aim . Our aim is to find a mapping between all points of x and y. For instance, x(3) may be mapped to y(4) and so on. . Making a 2d matrix to compute distances between all pairs of x and y . In this initial step, we will find out the distance between all pair of points in the two signals. Lesser distances implies that these points may be candidates to be matched together. . distances = np.zeros((len(y), len(x))) . distances . array([[ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.]]) . We will use euclidean distance between the pairs of points. . for i in range(len(y)): for j in range(len(x)): distances[i,j] = (x[j]-y[i])**2 . distances . array([[ 1., 1., 4., 9., 4., 0.], [ 0., 0., 1., 4., 1., 1.], [ 0., 0., 1., 4., 1., 1.], [ 1., 1., 0., 1., 0., 4.], [ 4., 4., 1., 0., 1., 9.], [ 1., 1., 0., 1., 0., 4.], [ 0., 0., 1., 4., 1., 1.]]) . Visualizing the distance matrix . We will write a small function to visualize the distance matrix we just created. . def distance_cost_plot(distances): im = plt.imshow(distances, interpolation=&#39;nearest&#39;, cmap=&#39;Reds&#39;) plt.gca().invert_yaxis() plt.xlabel(&quot;X&quot;) plt.ylabel(&quot;Y&quot;) plt.grid() plt.colorbar(); . distance_cost_plot(distances) . From the plot above, it seems like the diagonal entries have low distances, which means that the distance between similar index points in x and y is low. . Warping path . In order to create a mapping between the two signals, we need to create a path in the above plot. The path should start at (0,0) and want to reach (M,N) where (M, N) are the lengths of the two signals. Our aim is to find the path of minimum distance. To do this, we thus build a matrix similar to the distances matrix. This matrix would contain the minimum distances to reach a specific point when starting from (0,0). We impose some restrictions on the paths which we would explore: . The path must start at (0,0) and end at (M,N) | We cannot go back in time, so the path only flows forwards, which means that from a point (i, j), we can only right (i+1, j) or upwards (i, j+1) or diagonal (i+1, j+1). | These restrictions would prevent the combinatorial explosion and convert the problem to a Dynamic Programming problem which can be solved in O(MN) time. . accumulated_cost = np.zeros((len(y), len(x))) . Let us now build up the accumulated cost . Since we start from (0,0), the accumulated cost at this point is distance(0,0) | . accumulated_cost[0,0] = distances[0,0] . Lets see how accumulated cost looks at this point. . distance_cost_plot(accumulated_cost) . If we were to move along the first row, i.e. from (0,0) in the right direction only, one step at a time | . for i in range(1, len(x)): accumulated_cost[0,i] = distances[0,i] + accumulated_cost[0, i-1] . distance_cost_plot(accumulated_cost) . If we were to move along the first column, i.e. from (0,0) in the upwards direction only, one step at a time | . for i in range(1, len(y)): accumulated_cost[i,0] = distances[i, 0] + accumulated_cost[i-1, 0] . distance_cost_plot(accumulated_cost) . For all other elements, we have . . for i in range(1, len(y)): for j in range(1, len(x)): accumulated_cost[i, j] = min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]) + distances[i, j] . distance_cost_plot(accumulated_cost) . So, now we have obtained the matrix containing cost of all paths starting from (0,0). We now need to find the path minimizing the distance which we do by backtracking. . Backtracking and finding the optimal warp path . Backtracking procedure is fairly simple and involves trying to move back from the last point (M, N) and finding which place we would reached there from (by minimizing the cost) and do this in a repetitive fashion. . path = [[len(x)-1, len(y)-1]] i = len(y)-1 j = len(x)-1 while i&gt;0 and j&gt;0: if i==0: j = j - 1 elif j==0: i = i - 1 else: if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]): i = i - 1 elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]): j = j-1 else: i = i - 1 j= j- 1 path.append([j, i]) path.append([0,0]) . path . [[5, 6], [4, 5], [3, 4], [2, 3], [1, 2], [1, 1], [0, 1], [0, 0]] . path_x = [point[0] for point in path] path_y = [point[1] for point in path] . distance_cost_plot(accumulated_cost) plt.plot(path_x, path_y); . The above plot shows the optimum warping path which minimizes the sum of distance (DTW distance) along the path. Let us wrap up the function by also incorporating the DTW distance between the two signals as well. . def path_cost(x, y, accumulated_cost, distances): path = [[len(x)-1, len(y)-1]] cost = 0 i = len(y)-1 j = len(x)-1 while i&gt;0 and j&gt;0: if i==0: j = j - 1 elif j==0: i = i - 1 else: if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]): i = i - 1 elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]): j = j-1 else: i = i - 1 j= j- 1 path.append([j, i]) path.append([0,0]) for [y, x] in path: cost = cost +distances[x, y] return path, cost . path, cost = path_cost(x, y, accumulated_cost, distances) print path print cost . [[5, 6], [4, 5], [3, 4], [2, 3], [1, 2], [1, 1], [0, 1], [0, 0]] 2.0 . Let us compare our naive implementation with that of mlpy which also provides a DTW implementation. . import mlpy . dist, cost, path = mlpy.dtw_std(x, y, dist_only=False) . import matplotlib.cm as cm fig = plt.figure(1) ax = fig.add_subplot(111) plot1 = plt.imshow(cost.T, origin=&#39;lower&#39;, cmap=cm.gray, interpolation=&#39;nearest&#39;) plot2 = plt.plot(path[0], path[1], &#39;w&#39;) xlim = ax.set_xlim((-0.5, cost.shape[0]-0.5)) ylim = ax.set_ylim((-0.5, cost.shape[1]-0.5)) . The path looks almost identical to the one we got. The slight difference is due to the fact that the path chosen by our implementation and the one chosen by DTW have the same distance and thus we woulc choose either. . dist . 2.0 . Not bad! Our implementation gets the same distance between x and y. . Let us look at another interesting way to visualize the warp. We will place the two signals on the same axis and . plt.plot(x, &#39;bo-&#39; ,label=&#39;x&#39;) plt.plot(y, &#39;g^-&#39;, label = &#39;y&#39;) plt.legend(); paths = path_cost(x, y, accumulated_cost, distances)[0] for [map_x, map_y] in paths: print map_x, x[map_x], &quot;:&quot;, map_y, y[map_y] plt.plot([map_x, map_y], [x[map_x], y[map_y]], &#39;r&#39;) . 5 0 : 6 1 4 2 : 5 2 3 3 : 4 3 2 2 : 3 2 1 1 : 2 1 1 1 : 1 1 0 1 : 1 1 0 1 : 0 0 . The above plot shows the mapping between the two signal. The red lines connect the matched points which are given by the DTW algorithm Looks neat isn&#39;t it? Now, let us try this for some known signal. This example is inspired from the example used in R&#39;s dtw implementation. We will see the DTW path between a sine and cosine on the same angles. . idx = np.linspace(0, 6.28, 100) . x = np.sin(idx) . y = np.cos(idx) . distances = np.zeros((len(y), len(x))) . for i in range(len(y)): for j in range(len(x)): distances[i,j] = (x[j]-y[i])**2 . distance_cost_plot(distances) . accumulated_cost = np.zeros((len(y), len(x))) accumulated_cost[0,0] = distances[0,0] for i in range(1, len(y)): accumulated_cost[i,0] = distances[i, 0] + accumulated_cost[i-1, 0] for i in range(1, len(x)): accumulated_cost[0,i] = distances[0,i] + accumulated_cost[0, i-1] for i in range(1, len(y)): for j in range(1, len(x)): accumulated_cost[i, j] = min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]) + distances[i, j] . plt.plot(x, &#39;bo-&#39; ,label=&#39;x&#39;) plt.plot(y, &#39;g^-&#39;, label = &#39;y&#39;) plt.legend(); paths = path_cost(x, y, accumulated_cost, distances)[0] for [map_x, map_y] in paths: #print map_x, x[map_x], &quot;:&quot;, map_y, y[map_y] plt.plot([map_x, map_y], [x[map_x], y[map_y]], &#39;r&#39;) . Ok, this does look nice. I am impressed! . Conclusions . We worked out a naive DTW implementation pretty much from scratch. It seems to do reasonably well on artificial data. . References . Wikipedia page on dtw | R&#39;s dtw package | mlpy page on dtw | DTW review paper | Feel free to comment! .",
            "url": "https://nipunbatra.github.io/blog/ml/2014/05/01/dtw.html",
            "relUrl": "/ml/2014/05/01/dtw.html",
            "date": " • May 1, 2014"
        }
        
    
  
    
        ,"post37": {
            "title": "Denoising using Least Squares",
            "content": "I took the Linear and Integer Programming course on coursera last year. It was a very well paced and structured course. In one of the modules, the instructors discussed least squares and unlike the conventional line fitting, they demonstrated the concept via signal denoising. Matlab code for the same was also provided. This is an attempt to do the same thing in an IPython notebook and leverage static widgets for a better understanding. . import numpy as np import matplotlib.pyplot as plt import seaborn %matplotlib inline . True data . Let us create the true data, i.e. if there were no noise and the measuring system was perfect, how would our data look like. This is a modified sinusoid signal. . n=400 t=np.array(xrange(n)) ex=0.5*np.sin(np.dot(2*np.pi/n,t))*np.sin(0.01*t) . plt.plot(ex); . Noisy data . Now, let us add some Gaussian noise about the true data . corr=ex+0.05*np.random.normal(0,1,n) . This is how the corrupted signal looks like. . plt.plot(corr); . The optimization problem . Constraints . We want the cleaned data to be as close to the corrupted data | We want the successive entries in the cleaned data to be as close as possible | Mathematically, we can write this as follows: . $ min{ ||(x-x_{corr})||^2 + mu sum_{k=1}^{n-1} (x_{k+1}-x_{k})^{2}} $ . Here, $||(x-x_{corr})||^2$ corresponds to the first constraint and $ mu sum_{k=1}^{n-1} (x_{k+1}-x_{k})^{2} $ corresponds to the second constraint. . Formulation as least squares problem . For the least squares problem, we would like a form of $Ax=b$ . First constraint . Looking at the first constraint, $||(x-x_{corr})||^2$, this can be expanded as follows: . $ (x_1 -x_{corr}(1))^2 + (x_2 -x_{corr}(2))^2 + ..(x_n -x_{corr}(n))^2$ . which we can rewrite as : . $ ([1,0,0 ..0][x_1, x_2,..x_n]^T - x_{corr}(1))^2 + ([0,1,0 ..0][x_1, x_2,..x_n]^T - x_{corr}(2))^2 + ...([0,0,0 ..n][x_1, x_2,..x_n]^T - x_{corr}(n))^2$ . Comparing with the standard least squares, we get . $A_{first} = I^{nxn}$ and $b_{first} = x_{corr}$ . Second constraint . For the second constraint, $ mu sum_{k=1}^{n-1} (x_{k+1}-x_{k})^{2}$ . This can be written as: $( sqrt{ mu}(x_2-x_1))^2 + ( sqrt{ mu}(x_3-x_2))^2 + .. ( sqrt{ mu}(x_n-x_{n-1}))^2$ which we can write as: . $( sqrt{ mu}[-1,1,0,0,0,0..][x_1,x_2,..x_n]^T - 0)^2 + ...( sqrt{ mu}[0,0,0,..,-1,1][x_1,x_2,..x_n]^T - 0)^2$ . From this formulation, we get for this constraint, $A_{second}$ as follows (we ignore $ sqrt{ mu}$ for now) . [-1, 1, 0, ...., 0] [0, -1, 1, ......0] .. [0, 0, ..... -1, 1] . and $b_{second}$ as a zeros matrix of size (n-1, n) . Combining both constraints . So, we combine the two sets of As and bs in a bigger matrix to get $A_{combined}$ and $b_{combined}$ as follows: . $A_{combined}$ . [A_first A_second] . and $b_{combined}$ as : . [b_first b_second] . Constructing $A_{second}$ matrix with parameter $n$ and $ mu$ . root_mu=10 . d1=np.eye(n-1,n) . d1 . array([[ 1., 0., 0., ..., 0., 0., 0.], [ 0., 1., 0., ..., 0., 0., 0.], [ 0., 0., 1., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 1., 0., 0.], [ 0., 0., 0., ..., 0., 1., 0.]]) . d1=np.roll(d1,1) . d1 . array([[ 0., 1., 0., ..., 0., 0., 0.], [ 0., 0., 1., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 1., 0., 0.], [ 0., 0., 0., ..., 0., 1., 0.], [ 0., 0., 0., ..., 0., 0., 1.]]) . d2=-np.eye(n-1,n) . a_second= root_mu*(d1+d2) . a_second . array([[-10., 10., 0., ..., 0., 0., 0.], [ 0., -10., 10., ..., 0., 0., 0.], [ 0., 0., -10., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 10., 0., 0.], [ 0., 0., 0., ..., -10., 10., 0.], [ 0., 0., 0., ..., 0., -10., 10.]]) . Constructing $A_{first}$ matrix with parameter $n$ . a_first=np.eye(n) . a_first . array([[ 1., 0., 0., ..., 0., 0., 0.], [ 0., 1., 0., ..., 0., 0., 0.], [ 0., 0., 1., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 1., 0., 0.], [ 0., 0., 0., ..., 0., 1., 0.], [ 0., 0., 0., ..., 0., 0., 1.]]) . Constructing $A$ matrix . A=np.vstack((a_first,a_second)) . A . array([[ 1., 0., 0., ..., 0., 0., 0.], [ 0., 1., 0., ..., 0., 0., 0.], [ 0., 0., 1., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 10., 0., 0.], [ 0., 0., 0., ..., -10., 10., 0.], [ 0., 0., 0., ..., 0., -10., 10.]]) . Similarly, we construct $b$ matrix . corr=corr.reshape((n,1)) . b_2=np.zeros((n-1,1)) . b=np.vstack((corr,b_2)) . Let us use least squares for our denoising for $ mu=100$ . import numpy.linalg.linalg as LA . ans = LA.lstsq(A,b) . Comparing original, noisy and denoised signal . plt.plot(corr,label=&#39;noisy&#39;,alpha=0.4) plt.plot(ans[0],&#39;r&#39;,linewidth=2, label=&#39;denoised&#39;) plt.plot(ex,&#39;g&#39;,linewidth=2, label=&#39;original&#39;) plt.legend(); . Observing the variation with $ mu$ . Let us quickly create an interactive widget to see how denoising would be affected by different values of $ mu$. We will view this on a log scale. . def create_A(n, mu): &quot;&quot;&quot; Create the A matrix &quot;&quot;&quot; d1=np.eye(n-1,n) d1=np.roll(d1,1) a_second= np.sqrt(mu)*(d1+d2) a_first=np.eye(n) A=np.vstack((a_first,a_second)) return A def create_b(n): b_2=np.zeros((n-1,1)) b=np.vstack((corr,b_2)) return b def denoise(mu): &quot;&quot;&quot; Return denoised signal &quot;&quot;&quot; n = len(corr) A = create_A(n, mu) b = create_b(n) return LA.lstsq(A,b)[0] def comparison_plot(mu): fig, ax = plt.subplots(figsize=(4,3)) ax.plot(corr,label=&#39;noisy&#39;,alpha=0.4) ax.plot(denoise(np.power(10,mu)),&#39;r&#39;,linewidth=2, label=&#39;denoised&#39;) ax.plot(ex,&#39;g&#39;,linewidth=2, label=&#39;original&#39;) plt.title(r&quot;$ mu=%d$&quot; % np.power(10,mu)) plt.legend() return fig from ipywidgets import StaticInteract, RangeWidget StaticInteract(comparison_plot, mu = RangeWidget(0,5,1)) . mu: For smaller values of $ mu$, the first constraints is the dominant one and tries to keep the denoised signal close to the noisy one. For higher values of $ mu$, the second constraint dominates. .",
            "url": "https://nipunbatra.github.io/blog/ml/2013/09/01/denoising.html",
            "relUrl": "/ml/2013/09/01/denoising.html",
            "date": " • Sep 1, 2013"
        }
        
    
  
    
        ,"post38": {
            "title": "HMM Simulation for Continuous HMM",
            "content": "In this notebook we shall create a continuous Hidden Markov Model [1] for an electrical appliance. Problem description: . Our appliance is a compressor based appliance and has 2 states (compressor ON and compressor OFF). | When the compressor is ON, the power drawn by refrigerator is a normal distribution with mean 170 Watts and variance 4 | In off state, power drawn is 0 Watts with variance 1 W | Prior probability of starting in OFF states is 90% | If compressor is OFF, it remains OFF in the next cycle with probability 0.99 | If compressor is ON, it remains ON in the nxext cycle with probability 0.9 | . In all it matches the description of a continuous Hidden Markov Model. The different components of the Discrete HMM are as follows: . Observed States : Power draw | Hidden States : Compressor ON or OFF | Prior (pi) : Probability of starting in OFF or ON state | Transition Matrix (A): Probability of going from ON-&gt; OFF and vice versa | Emission Matrix (B) : Matrix encoding the mean and variance associated with a particular state. | . Next, we import the basic set of libraries used for matrix manipulation and for plotting. . import numpy as np import matplotlib.pyplot as plt import matplotlib #Setting Font Size as 20 matplotlib.rcParams.update({&#39;font.size&#39;: 20}) . Next, we define the different components of HMM which were described above. . pi=np.array([.9,.1]) A=np.array([[.99,.01],[.1,.9]]) B=np.array([{&#39;mean&#39;:0,&#39;variance&#39;:1},{&#39;mean&#39;:170,&#39;variance&#39;:4}]) . Now based on these probability we need to produce a sequence of observed and hidden states. We use the notion of weighted sampling, which basically means that terms/states with higher probabilies assigned to them are more likely to be selected/sampled. For example,let us consider the starting state. For this we need to use the pi matrix, since that encodes the likiliness of starting in a particular state. We observe that for starting in Fair state the probability is .667 and twice that of starting in Biased state. Thus, it is much more likely that we start in Fair state. We use Fitness Proportionate Selection [3] to sample states based on weights (probability). For selection of starting state we would proceed as follows: . We choose a random value between 0 and 1 | We iterate over the list of values (prior) and iteratively subtract the value at current position from the number which we chose at random and as soon as it becomes negative, we return the index. Let us demonstrate this with a function. | . &#39;&#39;&#39; Returns next state according to weigted probability array. Code based on Weighted random generation in Python [4] &#39;&#39;&#39; def next_state(weights): choice = random.random() * sum(weights) for i, w in enumerate(weights): choice -= w if choice &lt; 0: return i . We test the above function by making a call to it 1000 times and then we try to see how many times do we get a 0 (Fair) wrt 1 (Biased), given the pi vector. . count=0 for i in range(1000): count+=next_state(pi) print &quot;Expected number of Fair states:&quot;,1000-count print &quot;Expected number of Biased states:&quot;,count . Expected number of Fair states: 890 Expected number of Biased states: 110 . Thus, we can see that we get approximately twice the number of Fair states as Biased states which is as expected. . Next, we write the following functions: . create_hidden_sequence (pi,A,length): which creates a hidden sequence (Markov Chain) of desired length based on Pi and A. The algorithm followed is as follows: We choose the first state as described above. Next on the basis of current state, we see it&#39;s transition matrix and assign the next state by weighted sampling (by invoking next_state with argument as A[current_state]) | create_observed_sequence (hidden_sequence,B): which create an observed sequence based on hidden states and associated B. Based on current hidden state, we use it&#39;s emission parameters to sample the observation. | . def create_hidden_sequence(pi,A,length): out=[None]*length out[0]=next_state(pi) for i in range(1,length): out[i]=next_state(A[out[i-1]]) return out def create_observation_sequence_continuous(hidden_sequence,B): length=len(hidden_sequence) out=[None]*length for i in range(length): out[i]=np.random.normal(B[hidden_sequence[i]][&#39;mean&#39;],B[hidden_sequence[i]][&#39;variance&#39;],1) return out . Thus, using these functions and the HMM paramters we decided earlier, we create length 1000 sequence for hidden and observed states. . hidden=np.array(create_hidden_sequence(pi,A,1000)) observed=np.array(create_observation_sequence_continuous(hidden,B)) . Now, we create helper functions to plot the two sequence in a way we can intuitively understand the HMM. plt.figsize(16,10); plt.subplot(2,1,1) plt.title(&#39;Observed Power draw&#39;) plt.ylabel(&#39;Power (W)&#39;); plt.xlabel(&#39;Sample #&#39;); plt.plot(observed) plt.subplot(2,1,2); plt.fill_between(range(len(hidden)),hidden,0,alpha=0.5) plt.plot(hidden,linewidth=3); plt.ylabel(&#39;State&#39;); plt.xlabel(&#39;Sample #&#39;); plt.title(&#39;0- Compressor Off, 1- Compressor On&#39;); plt.tight_layout() . References . http://en.wikipedia.org/wiki/Hidden_Markov_model | http://www.stanford.edu/class/stats366/hmmR2.html | http://en.wikipedia.org/wiki/Fitness_proportionate_selection | http://eli.thegreenplace.net/2010/01/22/weighted-random-generation-in-python/ | http://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list |",
            "url": "https://nipunbatra.github.io/blog/ml/2013/07/01/hmm_continuous.html",
            "relUrl": "/ml/2013/07/01/hmm_continuous.html",
            "date": " • Jul 1, 2013"
        }
        
    
  
    
        ,"post39": {
            "title": "HMM Simulation for Unfair Casino Problem",
            "content": "In this notebook we shall create a Hidden Markov Model [1] for the Unfair Casino problem [2]. In short the problem is as follows: In a casino there may be two die- one fair and the other biased. The biased die is much more likely to produce a 6 than the other numbers. With the fair die all the outcomes (1 through 6) are equally likely. For the biased die, probability of observing a 6 is 0.5 and observing 1,2,3,4,5 is 0.1 each. Also, there are probabilies associated with the choice of a die to be thrown. The observer is only able to observe the values of die being thrown, without having a knowldge whether a fair or biased die were used. . In all it matches the description of a discrete Hidden Markov Model. The different components of the Discrete HMM are as follows: . Observed States : 1 through 6 on the die faces | Hidden States : Fair or Biased Die | Prior (pi) : Probability that the first throw is made from a fair or a biased die, which is : Pr (first throw is fair) and Pr (first throw is biased), which is represented as a 1 X 2 row matrix | Transition Matrix (A): Matrix encoding the probability of the 4 possible transition between fair and biased die, which are: Fair-&gt; Fair, Fair-&gt; Biased, Biased-&gt; Fair and Biased-&gt;Biased, which is represented as a 2 X 2 matrix | Emission Matrix (B) : Matrix encoding the probability of an observation given the hidden state. It is a 2 X 6 Matrix | . Next, we import the basic set of libraries used for matrix manipulation and for plotting. . import numpy as np import matplotlib.pyplot as plt import matplotlib #Setting Font Size as 20 matplotlib.rcParams.update({&#39;font.size&#39;: 20}) . Next, we define the different components of HMM which were described above. . &#39;&#39;&#39; Pi : Fair die is twice as likely as biased die A : The die thrower likes to keep in one state (fair/biased), and the tranisition from 1. Fair-&gt; Fair : .95 2. Fair-&gt;Biased: 1-.95=.05 3. Biased-&gt;Biased: .90 4. Biased-&gt;Biased=1-.90=.10 B : The fair die is equally likely to produce observations 1 through 6, for the biased die Pr(6)=0.5 and Pr(1)=Pr(2)=Pr(3)=Pr(4)=Pr(5)=0.1 &#39;&#39;&#39; pi=np.array([2.0/3,1.0/3]) A=np.array([[.95,.05],[.1,.9]]) B=np.array([[1.0/6 for i in range(6)],[.1,.1,.1,.1,.1,.5]]) . Now based on these probability sequences we need to produce a sequence of observed and hidden states. We use the notion of weighted sampling, which basically means that terms/states with higher probabilies assigned to them are more likely to be selected/sampled. For example,let us consider the starting state. For this we need to use the pi matrix, since that encodes the likiliness of starting in a particular state. We observe that for starting in Fair state the probability is .667 and twice that of starting in Biased state. Thus, it is much more likely that we start in Fair state. We use Fitness Proportionate Selection [3] to sample states based on weights (probability). For selection of starting state we would proceed as follows: . We choose a random value between 0 and 1 | We iterate over the list of values (prior) and iteratively subtract the value at current position from the number which we chose at random and as soon as it becomes negative, we return the index. Let us demonstrate this with a function. | . &#39;&#39;&#39; Returns next state according to weigted probability array. Code based on Weighted random generation in Python [4] &#39;&#39;&#39; def next_state(weights): choice = random.random() * sum(weights) for i, w in enumerate(weights): choice -= w if choice &lt; 0: return i . We test the above function by making a call to it 1000 times and then we try to see how many times do we get a 0 (Fair) wrt 1 (Biased), given the pi vector. . count=0 for i in range(1000): count+=next_state(pi) print &quot;Expected number of Fair states:&quot;,1000-count print &quot;Expected number of Biased states:&quot;,count . Expected number of Fair states: 649 Expected number of Biased states: 351 . Thus, we can see that we get approximately twice the number of Fair states as Biased states which is as expected. . Next, we write the following functions: . create_hidden_sequence (pi,A,length): which creates a hidden sequence (Markov Chain) of desired length based on Pi and A. The algorithm followed is as follows: We choose the first state as described above. Next on the basis of current state, we see it&#39;s transition matrix and assign the next state by weighted sampling (by invoking next_state with argument as A[current_state]) | create_observed_sequence (hidden_sequence,B): which create an observed sequence based on hidden states and associated B. Based on current hidden state, we use it&#39;s emission parameters to sample the observation. | . def create_hidden_sequence(pi,A,length): out=[None]*length out[0]=next_state(pi) for i in range(1,length): out[i]=next_state(A[out[i-1]]) return out def create_observation_sequence(hidden_sequence,B): length=len(hidden_sequence) out=[None]*length for i in range(length): out[i]=next_state(B[hidden_sequence[i]]) return out . Thus, using these functions and the HMM paramters we decided earlier, we create length 1000 sequence for hidden and observed states. . hidden=np.array(create_hidden_sequence(pi,A,1000)) observed=np.array(create_observation_sequence(hidden,B)) . Now, we create helper functions to plot the two sequence in a way we can intuitively understand the HMM. &#39;&#39;&#39;Group all contiguous values in tuple. Recipe picked from Stack Overflow [5]&#39;&#39;&#39; def group(L): first = last = L[0] for n in L[1:]: if n - 1 == last: # Part of the group, bump the end last = n else: # Not part of the group, yield current group and start a new yield first, last first = last = n yield first, last # Yield the last group &#39;&#39;&#39;Create tuples of the form (start, number_of_continuous values&#39;&#39;&#39; def create_tuple(x): return [(a,b-a+1) for (a,b) in x] . #Tuples of form index value, number of continuous values corresponding to Fair State indices_hidden_fair=np.where(hidden==0)[0] tuples_contiguous_values_fair=list(group(indices_hidden_fair)) tuples_start_break_fair=create_tuple(tuples_contiguous_values_fair) #Tuples of form index value, number of continuous values corresponding to Biased State indices_hidden_biased=np.where(hidden==1)[0] tuples_contiguous_values_biased=list(group(indices_hidden_biased)) tuples_start_break_biased=create_tuple(tuples_contiguous_values_biased) #Tuples for observations observation_tuples=[] for i in range(6): observation_tuples.append(create_tuple(group(list(np.where(observed==i)[0])))) . Now we plot the hidden and observation sequences . plt.figsize(20,10) plt.subplot(2,1,1) plt.xlim((0,1000)); plt.title(&#39;Observations&#39;); for i in range(6): plt.broken_barh(observation_tuples[i],(i+0.5,1),facecolor=&#39;k&#39;); plt.subplot(2,1,2); plt.xlim((0,1000)); plt.title(&#39;Hidden States Green:Fair, Red: Biased&#39;); plt.broken_barh(tuples_start_break_fair,(0,1),facecolor=&#39;g&#39;); plt.broken_barh(tuples_start_break_biased,(0,1),facecolor=&#39;r&#39;); . References . http://en.wikipedia.org/wiki/Hidden_Markov_model | http://www.stanford.edu/class/stats366/hmmR2.html | http://en.wikipedia.org/wiki/Fitness_proportionate_selection | http://eli.thegreenplace.net/2010/01/22/weighted-random-generation-in-python/ | http://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list |",
            "url": "https://nipunbatra.github.io/blog/ml/2013/06/01/hmm_simulate.html",
            "relUrl": "/ml/2013/06/01/hmm_simulate.html",
            "date": " • Jun 1, 2013"
        }
        
    
  
    
        ,"post40": {
            "title": "Aggregation in Timeseries using Pandas",
            "content": "We&#39;ve all grown up studying groupy by and aggregations in SQL. Pandas provides excellent functionality for group by and aggregations. However, for time series data, we need a bit of manipulation. In this post, I&#39;ll take a small example of weather time series data. . import pandas as pd import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) %matplotlib inline . df = pd.read_csv(&quot;weather.csv&quot;, index_col=0, parse_dates=True).tz_localize(&quot;UTC&quot;).tz_convert(&quot;US/Central&quot;) . df.head() . humidity temperature . 2015-01-01 00:00:00-06:00 0.73 | 38.74 | . 2015-01-01 01:00:00-06:00 0.74 | 38.56 | . 2015-01-01 02:00:00-06:00 0.75 | 38.56 | . 2015-01-01 03:00:00-06:00 0.79 | 37.97 | . 2015-01-01 04:00:00-06:00 0.80 | 37.78 | . Question 1: What is the mean temperature and humidity per hour of the day? . We&#39;ll create a new column in the df containing the hour information from the index. . df[&quot;hour&quot;] = df.index.hour . df.head() . humidity temperature hour . 2015-01-01 00:00:00-06:00 0.73 | 38.74 | 0 | . 2015-01-01 01:00:00-06:00 0.74 | 38.56 | 1 | . 2015-01-01 02:00:00-06:00 0.75 | 38.56 | 2 | . 2015-01-01 03:00:00-06:00 0.79 | 37.97 | 3 | . 2015-01-01 04:00:00-06:00 0.80 | 37.78 | 4 | . mean_temp_humidity = df.groupby(&quot;hour&quot;).mean() mean_temp_humidity.head() . humidity temperature . hour . 0 0.779322 | 45.976441 | . 1 0.803898 | 44.859492 | . 2 0.812203 | 44.244407 | . 3 0.819153 | 43.724068 | . 4 0.832712 | 43.105763 | . mean_temp_humidity.plot(subplots=True); . We can use pivoting to achieve the same dataframe. . mean_temp_humidity_pivoting = pd.pivot_table(df, index=[&quot;hour&quot;], values=[&quot;temperature&quot;, &quot;humidity&quot;]) . mean_temp_humidity_pivoting.head() . humidity temperature . hour . 0 0.779322 | 45.976441 | . 1 0.803898 | 44.859492 | . 2 0.812203 | 44.244407 | . 3 0.819153 | 43.724068 | . 4 0.832712 | 43.105763 | . By default the aggregation function used in pivoting is mean. . Question 2: Can we plot the daily variation in temperature per hour of the day? . For this, we want to have a dataframe with hour of day as the index and the different days as the different columns. . df[&quot;day&quot;] = df.index.dayofyear . df.head() . humidity temperature hour day . 2015-01-01 00:00:00-06:00 0.73 | 38.74 | 0 | 1 | . 2015-01-01 01:00:00-06:00 0.74 | 38.56 | 1 | 1 | . 2015-01-01 02:00:00-06:00 0.75 | 38.56 | 2 | 1 | . 2015-01-01 03:00:00-06:00 0.79 | 37.97 | 3 | 1 | . 2015-01-01 04:00:00-06:00 0.80 | 37.78 | 4 | 1 | . daily_temp = pd.pivot_table(df, index=[&quot;hour&quot;], columns=[&quot;day&quot;], values=[&quot;temperature&quot;]) . daily_temp.head() . temperature . day 1 2 3 4 5 6 7 8 9 10 ... 50 51 52 53 54 55 56 57 58 59 . hour . 0 38.74 | 39.94 | 39.57 | 41.83 | 33.95 | 36.98 | 46.93 | 29.95 | 36.57 | 36.19 | ... | 46.17 | 54.01 | 66.57 | 55.49 | 37.68 | 30.34 | 34.97 | 39.93 | 36.19 | 32.25 | . 1 38.56 | 39.76 | 39.75 | 40.85 | 32.29 | 35.89 | 45.33 | 28.55 | 37.31 | 36.40 | ... | 41.38 | 54.56 | 66.57 | 55.49 | 36.76 | 30.04 | 34.97 | 36.37 | 36.38 | 32.25 | . 2 38.56 | 39.58 | 39.94 | 39.73 | 31.59 | 36.44 | 44.51 | 27.44 | 37.78 | 36.59 | ... | 39.99 | 55.81 | 66.57 | 55.34 | 35.56 | 30.57 | 34.75 | 34.74 | 36.20 | 32.25 | . 3 37.97 | 38.83 | 40.16 | 38.78 | 30.48 | 36.85 | 43.92 | 25.97 | 37.97 | 36.38 | ... | 39.05 | 57.14 | 66.38 | 55.27 | 34.94 | 30.59 | 35.15 | 34.31 | 36.20 | 32.52 | . 4 37.78 | 39.02 | 40.65 | 39.74 | 29.89 | 35.72 | 44.37 | 24.74 | 37.82 | 35.49 | ... | 37.99 | 57.51 | 66.57 | 55.49 | 34.04 | 30.38 | 35.15 | 33.02 | 34.49 | 32.52 | . 5 rows × 59 columns . daily_temp.plot(style=&#39;k-&#39;, alpha=0.3, legend=False) plt.ylabel(&quot;Temp&quot;); . So, we can see some pattern up there! Around 15 hours, the temperature usually peaks. . There you go! Some recipes for aggregation and plotting of time series data. .",
            "url": "https://nipunbatra.github.io/blog/visualisation/2013/05/01/aggregation-timeseries.html",
            "relUrl": "/visualisation/2013/05/01/aggregation-timeseries.html",
            "date": " • May 1, 2013"
        }
        
    
  
    
        ,"post41": {
            "title": "Downloading weather data",
            "content": "In this notebook, I&#39;ll write a small illustration on downloading historical weather data using forceast.io. I&#39;ll also illustrate handling timezone issues when using such time series data. I am going to use python-forecastio, which is a Python wrapper around forecast.io service. I&#39;ll be downloading hourly weather data for Austin, Texas. . import datetime import pandas as pd import forecastio import getpass . # Enter your API here api_key = getpass.getpass() . ········ . len(api_key) . 32 . Austin&#39;s Latitude and longitude . lat = 30.25 lng = -97.25 . Let us see the forecast for 1 Jan 2015 . date = datetime.datetime(2015,1,1) . forecast = forecastio.load_forecast(api_key, lat, lng, time=date, units=&quot;us&quot;) . forecast . &lt;forecastio.models.Forecast at 0x10319ce50&gt; . hourly = forecast.hourly() . hourly.data . [&lt;forecastio.models.ForecastioDataPoint at 0x1068643d0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x106864bd0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x106864ad0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x106864cd0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x106864fd0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x106864d10&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x100734e10&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1061e3450&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1061e3350&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3250&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3110&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3150&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3190&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b31d0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3210&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3fd0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3dd0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3e10&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3e50&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068b3f50&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068c84d0&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068c8390&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068c8510&gt;, &lt;forecastio.models.ForecastioDataPoint at 0x1068c8550&gt;] . Extracting data for a single hour. . hourly.data[0].d . {u&#39;apparentTemperature&#39;: 32.57, u&#39;dewPoint&#39;: 33.39, u&#39;humidity&#39;: 0.79, u&#39;icon&#39;: u&#39;clear-night&#39;, u&#39;precipIntensity&#39;: 0, u&#39;precipProbability&#39;: 0, u&#39;pressure&#39;: 1032.61, u&#39;summary&#39;: u&#39;Clear&#39;, u&#39;temperature&#39;: 39.46, u&#39;time&#39;: 1420005600, u&#39;visibility&#39;: 10, u&#39;windBearing&#39;: 21, u&#39;windSpeed&#39;: 10.95} . Let us say that we want to use the temperature and humidity only. . attributes = [&quot;temperature&quot;, &quot;humidity&quot;] . times = [] data = {} for attr in attributes: data[attr] = [] . Now, let us download hourly data for 30 days staring January 1 this year. . start = datetime.datetime(2015, 1, 1) for offset in range(1, 60): forecast = forecastio.load_forecast(api_key, lat, lng, time=start+datetime.timedelta(offset), units=&quot;us&quot;) h = forecast.hourly() d = h.data for p in d: times.append(p.time) for attr in attributes: data[attr].append(p.d[attr]) . Now, let us create a Pandas data frame for this time series data. . df = pd.DataFrame(data, index=times) . df.head() . humidity temperature . 2015-01-01 11:30:00 0.73 | 38.74 | . 2015-01-01 12:30:00 0.74 | 38.56 | . 2015-01-01 13:30:00 0.75 | 38.56 | . 2015-01-01 14:30:00 0.79 | 37.97 | . 2015-01-01 15:30:00 0.80 | 37.78 | . Now, we need to fix the timezone. . df = df.tz_localize(&quot;Asia/Kolkata&quot;).tz_convert(&quot;US/Central&quot;) . df.head() . humidity temperature . 2015-01-01 00:00:00-06:00 0.73 | 38.74 | . 2015-01-01 01:00:00-06:00 0.74 | 38.56 | . 2015-01-01 02:00:00-06:00 0.75 | 38.56 | . 2015-01-01 03:00:00-06:00 0.79 | 37.97 | . 2015-01-01 04:00:00-06:00 0.80 | 37.78 | . I&#39;ll now export this file to a CSV to use it for following demonstrations on aggregations on time series. . df.to_csv(&quot;weather.csv&quot;) . A quick validation of our downloaded data. . %matplotlib inline import matplotlib.pyplot as plt plt.style.use(&#39;ggplot&#39;) . df.plot(subplots=True); .",
            "url": "https://nipunbatra.github.io/blog/visualisation/2013/04/01/download_weather.html",
            "relUrl": "/visualisation/2013/04/01/download_weather.html",
            "date": " • Apr 1, 2013"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Link to my website: https://nipunbatra.github.io .",
          "url": "https://nipunbatra.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nipunbatra.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}