{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating and Understanding Near-optimal sensor placements in Gaussian processes\n",
    "> Taking a programming approach to understanding\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Multiple\n",
    "- categories: [ML]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick revision of GPs\n",
    "\n",
    "#### Setup\n",
    "\n",
    "#### Definition of GP\n",
    "\n",
    "#### Posterior Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance independent of observations\n",
    "\n",
    "Highlight the formula for \n",
    "\n",
    "\\begin{aligned}\n",
    "\\mu_{y | \\mathcal{A}} &=\\mu_{y}+\\Sigma_{y \\mathcal{A}} \\Sigma_{\\mathcal{A} \\mathcal{A}}^{-1}\\left(x_{\\mathcal{A}}-\\mu_{\\mathcal{A}}\\right) \\\\\n",
    "\\sigma_{y | \\mathcal{A}}^{2} &=\\mathcal{X}(y, y)-\\Sigma_{y \\mathcal{A}} \\Sigma_{\\mathcal{A} \\mathcal{A}}^{-1} \\Sigma_{\\mathcal{A}_{y}}\n",
    "\\end{aligned}\n",
    "\n",
    "and mention that Variance is independent of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment #1: Showing variance independent of observations in 1D GP\n",
    "\n",
    "Plot a 1D GP and show the variance at subset of points, then change the original observations and then show that the variance remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment #2: Sampling locations with highest variance\n",
    "\n",
    "In this experiment, you show iteratively sampling locations with highest variance and how the posterior variance changes over iterations\n",
    "\n",
    "\n",
    "Show how at each iteration the highest variance points are farthest away from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment #3: Sampling locations with highest variance (in 2d)\n",
    "\n",
    "Repeat experiment #2, but instead of 1d, use a 2d grid of size 5 X 5 and maybe have initial observations from 3 points in this 5 X 5 grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain sensor selection problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revise entropy, conditional and differential entropy for continuous variables\n",
    "\n",
    "in the context of multivariate Gaussians (refer to my ML class slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing sensor placement using entropy criterion\n",
    "\n",
    "Write the formula as shown in Krause paper, and then write it in terms of H(V\\A|A) = H(V\\A, A) - H(A) = H(V) - H(A)\n",
    "\n",
    "Now write we want to select A to argmax H(V\\A|A) = argmin H(A)\n",
    "\n",
    "Now, H(A) can be calculated using formula given above for differential entropy of multivariate Gaussian. Similarly, H(V) can also be calculated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment #4: Finding conditional entropy of unseen locations given observations\n",
    "\n",
    "Choose number of observations points as 3, and use the same 5 X 5 grid. You have to pick three (x, y) where 0<=x, y<=4\n",
    "\n",
    "Make a plot showing the entropies of a subset of the possible 3 placements. Also, show the total number of such placements. Which are the 3 points with highest conditional entropy of unseen and which are the ones with the lowest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mention the NP-hardness of the above\n",
    "\n",
    "mention how you were dealing with a combinatorial problem and the solution is NP-hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy procedure \n",
    "\n",
    "Mention the greedy procedure.\n",
    "\n",
    "From the ML lecture slides on differential entropy, write the formula for differential entropy of 1d Gaussian and now just modify it to include the conditioning on observations.\n",
    "\n",
    "\n",
    "$$H\\left(x_{y} | x_{\\mathcal{A}}\\right)=\\frac{1}{2} \\log \\left(2 \\pi e \\sigma_{x_{y} | x_{\\boldsymbol{q}}}^{2}\\right)=\\frac{1}{2} \\log \\sigma_{x_{y} | x_{\\boldsymbol{q}}}^{2}+\\frac{1}{2}(\\log (2 \\pi)+1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment #5: Run the greedy algorithm on 2d dataset\n",
    "\n",
    "\n",
    "Comment on the set of observations we get via the greedy procedure..provide some quality visualisations and graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mutual information \n",
    "\n",
    "Draw the VENN diagram in ML lectures on mutual information and then add "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
