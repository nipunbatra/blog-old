{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Model Change\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Nipun Batra\n",
    "- categories: [ML]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y= load_digits(n_class=2, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f90a3c01340>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAALDklEQVR4nO3dfaiedR3H8c+n4+bcXFr5gGyzDdKlFjk5TGQpuGHNB9QgaCOFNDsQKNMCUemP+qu/NCNEGnMmOB05HYmYNnxAzVpuc5Xb2WQNc2fppoRsLtqT3/4492DaWee6r3M93Ofb+wUHzzn3zfl976Nvr/tc5z7XzxEhAHl8qu0BAFSLqIFkiBpIhqiBZIgaSOa4Or7oRB8fkzSlji/drhNPaHS5qWfua2ytPYcmNbbWwQ8nNLbWxH809z1s0r+1Twdiv0e6rZaoJ2mKLvSCOr50qz7qn9Poepf+4tXG1lqz64uNrfXu76c1ttaZP27ue9iktfHcMW/j6TeQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kEyhqG0vtL3V9jbbd9Q9FIDyRo3adp+k+yRdLulcSYttn1v3YADKKXKknitpW0Rsj4gDklZKuqbesQCUVSTqaZJ2HPXxUOdzH2N7wPY62+sOan9V8wHoUmUnyiJiaUT0R0T/BB1f1ZcF0KUiUe+UNOOoj6d3PgegBxWJ+jVJZ9meZXuipEWSnqx3LABljXqRhIg4ZPtmSc9K6pO0PCI21T4ZgFIKXfkkIp6W9HTNswCoAK8oA5IhaiAZogaSIWogGaIGkiFqIBmiBpKpZYeOrA796J+NrnfXKVtTrqXzmlvqise+1dxikg5vavD7eAwcqYFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSKbIDh3Lbe+2/UYTAwEYmyJH6l9JWljzHAAqMmrUEfGSpGb/kgFAaZX9lZbtAUkDkjRJk6v6sgC6xLY7QDKc/QaSIWogmSK/0npU0h8kzbY9ZPu79Y8FoKwie2ktbmIQANXg6TeQDFEDyRA1kAxRA8kQNZAMUQPJEDWQzLjfdudf37iwsbVePu+Xja0lSbOeuamxtc658+3G1hp45dXG1vp/xJEaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkilyjbIbtF2xvtr3J9pImBgNQTpHXfh+S9MOI2GB7qqT1ttdExOaaZwNQQpFtd96JiA2d9/dKGpQ0re7BAJTT1V9p2Z4paY6ktSPcxrY7QA8ofKLM9omSHpd0a0Ts+eTtbLsD9IZCUdueoOGgV0TEE/WOBGAsipz9tqQHJA1GxD31jwRgLIocqedJul7SfNsbO29X1DwXgJKKbLvziiQ3MAuACvCKMiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSGfd7aWV29o3rGlvrcGMrSddO+bCxtZY2tlLv4EgNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRT5MKDk2z/yfafO9vu/KSJwQCUU+RlovslzY+IDzuXCn7F9m8j4o81zwaghCIXHgxJR16sO6HzFnUOBaC8ohfz77O9UdJuSWsiYsRtd2yvs73uoPZXPCaAogpFHRGHI+J8SdMlzbX9pRHuw7Y7QA/o6ux3RHwg6QVJC2uZBsCYFTn7fartkzvvnyDpMklbap4LQElFzn6fIekh230a/p/AryPiqXrHAlBWkbPff9HwntQAxgFeUQYkQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMuN+253Jq//rD8bqc19zS0lS3+mnNbbW4V27G1vrhrcvbmytrTd9prG1JOkLtzW63Ig4UgPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kEzhqDsX9H/dNhcdBHpYN0fqJZIG6xoEQDWKbrszXdKVkpbVOw6AsSp6pL5X0u2SPjrWHdhLC+gNRXbouErS7ohY/7/ux15aQG8ocqSeJ+lq229JWilpvu2Ha50KQGmjRh0Rd0bE9IiYKWmRpOcj4rraJwNQCr+nBpLp6nJGEfGipBdrmQRAJThSA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8mM+213mvTmwX2Nrjf40zMbW+vsG5vbduesyc2tteHtLze2Vq/gSA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDKFXibauZLoXkmHJR2KiP46hwJQXjev/b40It6vbRIAleDpN5BM0ahD0u9sr7c9MNId2HYH6A1Fn35/NSJ22j5N0hrbWyLipaPvEBFLJS2VpE/7s1HxnAAKKnSkjoidnX/ulrRa0tw6hwJQXpEN8qbYnnrkfUlfk/RG3YMBKKfI0+/TJa22feT+j0TEM7VOBaC0UaOOiO2SvtLALAAqwK+0gGSIGkiGqIFkiBpIhqiBZIgaSIaogWTYdqcLX39uSaPr/ezilY2tddvyRY2t9f0Tmntc0x7Z1tha0vAFB9rGkRpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWQKRW37ZNurbG+xPWj7oroHA1BO0dd+/1zSMxHxTdsTJU2ucSYAYzBq1LZPknSJpO9IUkQckHSg3rEAlFXk6fcsSe9JetD267aXda7//TFsuwP0hiJRHyfpAkn3R8QcSfsk3fHJO0XE0ojoj4j+CTq+4jEBFFUk6iFJQxGxtvPxKg1HDqAHjRp1RLwraYft2Z1PLZC0udapAJRW9Oz3LZJWdM58b5d0Q30jARiLQlFHxEZJ/fWOAqAKvKIMSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWTYS6sL59y9t9H1fnPunMbW+l7/y42tdfft325srcm71o5+p2Q4UgPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyYwate3Ztjce9bbH9q0NzAaghFFfJhoRWyWdL0m2+yTtlLS63rEAlNXt0+8Fkv4WEX+vYxgAY9ftH3QskvToSDfYHpA0IEmT2D8PaE3hI3Xnmt9XS3pspNvZdgfoDd08/b5c0oaI2FXXMADGrpuoF+sYT70B9I5CUXe2rr1M0hP1jgNgrIpuu7NP0udqngVABXhFGZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJOCKq/6L2e5K6/fPMUyS9X/kwvSHrY+NxtefzEXHqSDfUEnUZttdFRH/bc9Qh62PjcfUmnn4DyRA1kEwvRb207QFqlPWx8bh6UM/8TA2gGr10pAZQAaIGkumJqG0vtL3V9jbbd7Q9TxVsz7D9gu3NtjfZXtL2TFWy3Wf7ddtPtT1LlWyfbHuV7S22B21f1PZM3Wr9Z+rOBgFvavhySUOSXpO0OCI2tzrYGNk+Q9IZEbHB9lRJ6yVdO94f1xG2fyCpX9KnI+Kqtuepiu2HJL0cEcs6V9CdHBEftDxWV3rhSD1X0raI2B4RByStlHRNyzONWUS8ExEbOu/vlTQoaVq7U1XD9nRJV0pa1vYsVbJ9kqRLJD0gSRFxYLwFLfVG1NMk7Tjq4yEl+Y//CNszJc2RtLblUapyr6TbJX3U8hxVmyXpPUkPdn60WNa56Oa40gtRp2b7REmPS7o1Iva0Pc9Y2b5K0u6IWN/2LDU4TtIFku6PiDmS9kkad+d4eiHqnZJmHPXx9M7nxj3bEzQc9IqIyHJ55XmSrrb9loZ/VJpv++F2R6rMkKShiDjyjGqVhiMfV3oh6tcknWV7VufExCJJT7Y805jZtoZ/NhuMiHvanqcqEXFnREyPiJka/nf1fERc1/JYlYiIdyXtsD2786kFksbdic1uN8irXEQcsn2zpGcl9UlaHhGbWh6rCvMkXS/pr7Y3dj53V0Q83d5IKOAWSSs6B5jtkm5oeZ6utf4rLQDV6oWn3wAqRNRAMkQNJEPUQDJEDSRD1EAyRA0k8x+y4p2tN490nwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[4].reshape(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 2)  # 5*5 from image dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        #x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.707\n",
      "[1,     6] loss: 0.610\n",
      "[1,    11] loss: 0.693\n",
      "[1,    16] loss: 0.693\n",
      "[1,    21] loss: 0.693\n",
      "[1,    26] loss: 0.579\n",
      "[1,    31] loss: 0.318\n",
      "[1,    36] loss: 0.297\n",
      "[1,    41] loss: 0.281\n",
      "[1,    46] loss: 0.683\n",
      "[1,    51] loss: 0.589\n",
      "[1,    56] loss: 0.462\n",
      "[1,    61] loss: 0.382\n",
      "[1,    66] loss: 0.283\n",
      "[1,    71] loss: 0.693\n",
      "[1,    76] loss: 0.956\n",
      "[1,    81] loss: 0.689\n",
      "[1,    86] loss: 0.311\n",
      "[1,    91] loss: 0.329\n",
      "[1,    96] loss: 0.693\n",
      "[2,     1] loss: 0.542\n",
      "[2,     6] loss: 0.416\n",
      "[2,    11] loss: 0.310\n",
      "[2,    16] loss: 0.278\n",
      "[2,    21] loss: 0.558\n",
      "[2,    26] loss: 0.278\n",
      "[2,    31] loss: 0.279\n",
      "[2,    36] loss: 0.285\n",
      "[2,    41] loss: 0.281\n",
      "[2,    46] loss: 0.555\n",
      "[2,    51] loss: 0.278\n",
      "[2,    56] loss: 0.279\n",
      "[2,    61] loss: 0.279\n",
      "[2,    66] loss: 0.278\n",
      "[2,    71] loss: 0.693\n",
      "[2,    76] loss: 0.283\n",
      "[2,    81] loss: 0.280\n",
      "[2,    86] loss: 0.285\n",
      "[2,    91] loss: 0.279\n",
      "[2,    96] loss: 0.571\n",
      "[3,     1] loss: 0.303\n",
      "[3,     6] loss: 0.416\n",
      "[3,    11] loss: 0.297\n",
      "[3,    16] loss: 0.278\n",
      "[3,    21] loss: 0.558\n",
      "[3,    26] loss: 0.278\n",
      "[3,    31] loss: 0.279\n",
      "[3,    36] loss: 0.284\n",
      "[3,    41] loss: 0.281\n",
      "[3,    46] loss: 0.555\n",
      "[3,    51] loss: 0.278\n",
      "[3,    56] loss: 0.279\n",
      "[3,    61] loss: 0.279\n",
      "[3,    66] loss: 0.278\n",
      "[3,    71] loss: 0.693\n",
      "[3,    76] loss: 0.278\n",
      "[3,    81] loss: 0.277\n",
      "[3,    86] loss: 0.278\n",
      "[3,    91] loss: 0.277\n",
      "[3,    96] loss: 0.555\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, 100, 5):\n",
    "        inputs = torch.Tensor(X[i:i+5])\n",
    "        labels = torch.Tensor(y[i:i+5]).type(torch.LongTensor)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss))\n",
    "        running_loss = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = torch.argmax(net(torch.Tensor(X[105:])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y[105:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49019607843137253"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor(X[106:107]).type(torch.FloatTensor)\n",
    "out = net(inp)\n",
    "c = criterion(out, torch.Tensor(y[106:107]).type(torch.LongTensor))\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-93-68a08c2d82be>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:417.)\n",
      "  c._grad\n"
     ]
    }
   ],
   "source": [
    "c._grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
